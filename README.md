# Awesome-Motion-Datasets

## üîç Related Papers

‚ö†Ô∏è Automated analysis may be inaccurate.

<table style="width: 100%;">
<tr><td><strong>Date</strong></td><td><strong>Paper</strong></td><td><strong>Contribution</strong></td><td><strong>Links</strong></td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.24997"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yuyangyin.github.io/PanoWorld-X/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing Jiaotong University<br>
‚Ä¢ Dataset: PanoExplorer, Samples: 116759, Modality: 360¬∞ panoramic videos with 3D exploration routes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.24968"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Electrical Engineering, Hongik University, Seoul 04066, South Korea<br>
‚Ä¢ Dataset: E-SIE, Samples: 720, Modality: RGB videos + Event camera streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Biomechanical-phase based Temporal Segmentation in Sports Videos: a Demonstration on Javelin-Throw</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.24606"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Bikudebug/Javelin_Throw_Dataset"><img src="https://img.shields.io/github/stars/Bikudebug/Javelin_Throw_Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Physics, Indian Institute of Technology Gandhinagar<br>
‚Ä¢ Dataset: Javelin Throw Dataset, Samples: 211, Modality: RGB videos with frame-level annotations (4 phases) and 2D pose sequences (16-joint skeletons in MPII format)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Understanding Cognitive States from Head & Hand Motion Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.24255"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Illinois Institute of Technology<br>
‚Ä¢ Dataset: VR Head and Hand Motion Dataset with Cognitive State Annotations, Samples: None, Modality: VR head and hand motion trajectories (18DOF), First-person video, Frame-level cognitive state annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>AssemblyHands-X: Modeling 3D Hand-Body Coordination for Understanding Bimanual Human Activities</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.23888"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo, Tokyo, Japan<br>
‚Ä¢ Dataset: AssemblyHands-X, Samples: None, Modality: Multi-view RGB videos + 3D keypoint poses + SMPL-X parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>InteractMove: Text-Controlled Human-Object Interaction Generation in 3D Scenes with Movable Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.23612"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Cxhcmhhh/InteractMove"><img src="https://img.shields.io/github/stars/Cxhcmhhh/InteractMove.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Wangxuan Institute of Computer Technology, Peking University<br>
‚Ä¢ Dataset: InteractMove, Samples: 30500, Modality: 3D motion sequences + object trajectories + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Developing Vision-Language-Action Model from Egocentric Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.21986"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kyoto University<br>
‚Ä¢ Dataset: Not explicitly named, referred to as 'our dataset', Samples: 45157, Modality: RGB videos + 6DoF object pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.21919"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://reinliu.github.io/text2move/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Sydney<br>
‚Ä¢ Dataset: Synthetic Moving Sound Dataset, Samples: 76850, Modality: Binaural audio + 3D spatial trajectories + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.21086"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yu-shaonian.github.io/UniTransfer-Web/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Lab of CAD&CG, Zhejiang University<br>
‚Ä¢ Dataset: OpenAnimal, Samples: 10000, Modality: RGB videos (single-animal sequences with diverse motion patterns)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>AI-Enabled Crater-Based Navigation for Lunar Mapping</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.20748"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Adelaide, Australian Institute for Machine Learning<br>
‚Ä¢ Dataset: CRESENT-365, Samples: 15283, Modality: Synthetic lunar surface images with ground truth poses for crater-based navigation<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.20358"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cwchenwang.github.io/physctrl"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Pennsylvania<br>
‚Ä¢ Dataset: Physics Simulation Dataset, Samples: 550000, Modality: 3D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.18566"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Optical Science and Engineering, Zhejiang University, China<br>
‚Ä¢ Dataset: ZJU-MoCap-Blur, Samples: 6, Modality: RGB videos with simulated motion blur<br>
‚Ä¢ Dataset: MMHPSD-Blur, Samples: 6, Modality: RGB videos with simulated motion blur<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.18455"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="geodex2p.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Thomas Lord Department of Computer Science at the University of Southern California, USA<br>
‚Ä¢ Dataset: GD2P Dataset, Samples: 1300000, Modality: Hand poses, object point clouds, basis point set representations, physics simulation validation data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.18387"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cogsys-tuebingen.github.io/blurball/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Tuebingen<br>
‚Ä¢ Dataset: Table tennis ball detection dataset, Samples: 64119, Modality: RGB videos with ball position, orientation, and blur length annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.17513"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mediax-sjtu.github.io/4DGCPro"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Cooperative Medianet Innovation Center, Shanghai Jiaotong University<br>
‚Ä¢ Dataset: 4DGCPro dataset, Samples: None, Modality: RGB videos (81 synchronized Z-CAM cinema cameras, 3840√ó2160)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.17168"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beihang University<br>
‚Ä¢ Dataset: HAGE, Samples: ~2.5 hours of curated recordings from 8 subjects, Modality: synchronized 16kHz audio, binocular gaze, 3D head rotations, 1080 √ó1080 video, 3D facial parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.14636"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WeiYuFei0217/ZJH-VO-Dataset/"><img src="https://img.shields.io/github/stars/ZJH-VO-Dataset/.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: ZJH-VO Multi-Scale Dataset, Samples: 12 trajectories with 12,666 frames, Modality: Monocular RGB videos, 2D LiDAR ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.14063"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Georgia Institute of Technology, School of Aerospace Engineering<br>
‚Ä¢ Dataset: 7daysJune subset (of TartanAviation dataset), Samples: None, Modality: Aircraft trajectory (ADS-B data) + audio radio calls (CTAF)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Towards Context-Aware Human-like Pointing Gestures with RL Motion Imitation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.12880"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Division of Speech, Music and Hearing, KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: Human Pointing Gestures Dataset, Samples: 83, Modality: Optical MoCap joints + hand motion capture gloves + head-mounted iPhone face/voice capture<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.12430"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Mechanical Engineering, Purdue University, USA<br>
‚Ä¢ Dataset: MechBench, Samples: 693, Modality: CAD point clouds, SE(3) motion trajectories, twist vectors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.11323"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SongJgit/filternet, https://github.com/SongJgit/TBDTracker"><img src="https://img.shields.io/github/stars/SongJgit/TBDTracker.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Army Engineering University, Shijiazhuang, 050003, Heibei, China<br>
‚Ä¢ Dataset: Semi-simulated motion estimation dataset, Samples: Large-scale, constructed from multiple sequences of MOT17, MOT20, SoccerNet, and DanceTrack, Modality: 2D Bounding-box trajectories (position, aspect ratio, height) with simulated noise<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.10961"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/fsa125/HR-pQCT-Motion-Correction-ESWGAN-GP"><img src="https://img.shields.io/github/stars/fsa125/HR-pQCT-Motion-Correction-ESWGAN-GP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Weldon School of Biomedical Engineering, Purdue University, West Lafayette, IN, USA<br>
‚Ä¢ Dataset: HR-pQCT Motion-Corrupted and Ground-Truth Paired Dataset, Samples: 483, Modality: HR-pQCT 3D bone images, 2D slices<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.10687"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stability AI<br>
‚Ä¢ Dataset: KinematicParts20K, Samples: over 20,000, Modality: Multi-view RGB videos + Kinematic part segmentation maps (derived from rigging annotations and skinning weights)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.10096"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/hhi-assist/home"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: VITA laboratory, EPFL, Lausanne, Switzerland<br>
‚Ä¢ Dataset: HHI-Assist, Samples: 908, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.09971"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dolby Laboratories, Inc.<br>
‚Ä¢ Dataset: EventAID, Samples: 1600+ seconds of capture, Modality: Events + RGB images/videos<br>
‚Ä¢ Dataset: LED, Samples: 3000 sequences, Modality: Events + RGB images/videos<br>
‚Ä¢ Dataset: RLED, Samples: 64200 aligned image and event pairs, Modality: Events + RGB images<br>
‚Ä¢ Dataset: SEE-600K, Samples: 610126 image-event pairs, Modality: Events + RGB images<br>
‚Ä¢ Dataset: EDS, Samples: None, Modality: Events + RGB images + IMU measurements<br>
‚Ä¢ Dataset: TUM-VIE, Samples: None, Modality: Stereo event camera data<br>
‚Ä¢ Dataset: BlinkFlow, Samples: None, Modality: Event-based optical flow<br>
‚Ä¢ Dataset: Real-World-Blur, Samples: 5 scenes, Modality: Color events + RGB images<br>
‚Ä¢ Dataset: Real-World-Challenge, Samples: 5 scenes, Modality: Color events + RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.09720"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lachlanchumbley.github.io/ColesObjectSet/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Deakin University, Australia<br>
‚Ä¢ Dataset: Australian Supermarket Object Set (ASOS), Samples: 50, Modality: 3D textured meshes, RGB images, physical objects<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>SpatialVID: A Large-Scale Video Dataset with Spatial Annotations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.09676"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nju-3dv.github.io/projects/SpatialVID"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: SpatialVID, Samples: 2.7 million clips, Modality: RGB videos, camera poses, depth maps, dynamic masks, structured captions, motion instructions<br>
‚Ä¢ Dataset: SpatialVID-HQ, Samples: 0.37 million clips, Modality: RGB videos, camera poses, depth maps, dynamic masks, structured captions, motion instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.09555"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wzyabcas/InterAct"><img src="https://img.shields.io/github/stars/wzyabcas/InterAct.svg?style=social&label=Star"></a><br><a href="https://sirui-xu.github.io/InterAct/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Illinois Urbana-Champaign<br>
‚Ä¢ Dataset: InterAct, Samples: 11350, Modality: 3D MoCap (SMPL-H/SMPL-X human models, rigid/dynamic objects, marker coordinates, velocities, signed distance vectors, foot-ground contact labels, object rotations/translations, BPS geometry)<br>
‚Ä¢ Dataset: InterAct-X, Samples: 16201, Modality: 3D MoCap (SMPL-H/SMPL-X human models, rigid/dynamic objects, marker coordinates, velocities, signed distance vectors, foot-ground contact labels, object rotations/translations, BPS geometry)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Motion-Based User Identification across XR and Metaverse Applications by Deep Classification and Similarity Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.08539"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of W√ºrzburg<br>
‚Ä¢ Dataset: Cross-application XR motion dataset, Samples: 49 users with over 60 hours of motion data, Modality: Head and hand controller tracking data (positions, rotations) in 5 VR applications<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>EHWGesture -- A dataset for multimodal understanding of clinical gestures</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.07525"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/smilies-polito/EHWGesture"><img src="https://img.shields.io/github/stars/smilies-polito/EHWGesture.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy<br>
‚Ä¢ Dataset: EHWGesture, Samples: 1100, Modality: RGB videos, Depth maps, Event camera data, Motion capture tracking<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>MIORe & VAR-MIORe: Benchmarks to Push the Boundaries of Restoration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.06803"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/george200150/MIORe"><img src="https://img.shields.io/github/stars/george200150/MIORe.svg?style=social&label=Star"></a><br><a href="https://github.com/george200150/MIORe"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Vision Lab, CAIDAS & IFI, University of W√ºrzburg<br>
‚Ä¢ Dataset: MIORe, Samples: 333, Modality: RGB videos + optical flow<br>
‚Ä¢ Dataset: VAR-MIORe, Samples: 333, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.06607"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://skel.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems<br>
‚Ä¢ Dataset: BioAMASS, Samples: 2198, Modality: Synthetic MoCap markers + Optimized biomechanical skeleton parameters (scale and pose) + Paired SMPL meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.05747"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hku-cg.github.io/interact/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: InterAct, Samples: 241, Modality: Motion-capture (body markers, finger markers), facial mesh animations, speech audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.05330"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SeyedMuhammadHosseinMousavi/Multi-Modal-Fusion"><img src="https://img.shields.io/github/stars/SeyedMuhammadHosseinMousavi/Multi-Modal-Fusion.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Cyrus Intelligence Research Ltd<br>
‚Ä¢ Dataset: MVRS (Multimodal Virtual Reality Stimuli-based emotion recognition dataset), Samples: 13, Modality: ['MoCap joints (Microsoft Kinect v2)', 'EMG signals', 'GSR signals', 'Eye-tracking (FHD webcam)', 'VR stimuli videos']<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Evaluating Idle Animation Believability: a User Perspective</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.05023"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computational Science and Artificial Intelligence, University of the Basque Country, Gipuzkoa, Spain<br>
‚Ä¢ Dataset: ReActIdle, Samples: 27,273 frames (15.15 mins) genuine idle motion + 55,039 frames (30.57 mins) acted idle motion, Modality: MoCap joints (3D BVH format)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.04117"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MustafaSakhai/videos-specifications-rgb_to_dvs-v2e"><img src="https://img.shields.io/github/stars/MustafaSakhai/videos-specifications-rgb_to_dvs-v2e.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.17030898"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Computer Science, Electronics and Telecommunications, AGH University of Science and Technology, 30-059 Krakow, Poland<br>
‚Ä¢ Dataset: DVS-PedX, Samples: 198 synthetic sequences + 346 real-converted sequences, Modality: DVS event streams (AEDAT), RGB frames, accumulated DVS 'event frames'<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.02807"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MSiam/PixFoundation-2.0.git"><img src="https://img.shields.io/github/stars/MSiam/PixFoundation-2.0.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: nan<br>
‚Ä¢ Dataset: MoCentric-Bench, Samples: 793, Modality: RGB videos + referring expressions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Articulated Object Estimation in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.01708"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://artipoint.cs.uni-freiburg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Freiburg<br>
‚Ä¢ Dataset: Arti4D, Samples: 45 RGB-D sequences containing 414 human-object interactions, Modality: RGB-D videos, articulated object interactions, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.00767"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mael-zys.github.io/InterPose/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)<br>
‚Ä¢ Dataset: InterPose, Samples: 73814, Modality: SMPL-X body/hand pose parameters + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.21732"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Systems and Robotics, University of Lisbon<br>
‚Ä¢ Dataset: DMDBench, Samples: 1000, Modality: RGB images of Digital Measurement Devices (DMDs) with motion blur, clutter, and occlusion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.21635"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cifasis/rosariov2/"><img src="https://img.shields.io/github/stars/rosariov2/.svg?style=social&label=Star"></a><br><a href="https://cifasis.github.io/rosariov2/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CIFASIS (CONICET-UNR), Rosario, Santa Fe, Argentina<br>
‚Ä¢ Dataset: The Rosario Dataset v2, Samples: 6, Modality: Stereo IR camera, RGB camera, IMU (6-DoF and 9-DoF), GNSS (SPP, RTK, PPK), wheel odometry<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>EmoCAST: Emotional Talking Portrait via Emotive Text Description</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.20615"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/GVCLab/EmoCAST"><img src="https://img.shields.io/github/stars/GVCLab/EmoCAST.svg?style=social&label=Star"></a><br><a href="https://github.com/GVCLab/EmoCAST"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Macau<br>
‚Ä¢ Dataset: Emotive Text-to-Talking Head (ETTH), Samples: 158 hours (across 15k+ identities), Modality: RGB videos + emotive text descriptions + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.19926"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Colin-Jing/FARM"><img src="https://img.shields.io/github/stars/Colin-Jing/FARM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology (Guangzhou), China<br>
‚Ä¢ Dataset: High-Dynamic Humanoid Motion (HDHM), Samples: 3593, Modality: MoCap joints (SMPL format)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.19895"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Intelligent Robotics and Advanced Manufacturing, Fudan University<br>
‚Ä¢ Dataset: PersonaVid, Samples: 18867, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.19002"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering, The Hong Kong Polytechnic University (PolyU), Kowloon, Hong Kong<br>
‚Ä¢ Dataset: HPose, Samples: 31925, Modality: 6D joint poses, contextual semantics annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.07095"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VankouF/MotionMillion-Codes"><img src="https://img.shields.io/github/stars/VankouF/MotionMillion-Codes.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: MotionMillion, Samples: 2000000, Modality: SMPL parameters<br>
‚Ä¢ Dataset: MotionMillion-Eval, Samples: 126, Modality: text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Democratizing High-Fidelity Co-Speech Gesture Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.06812"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mpi-lab.github.io/Democratizing-CSG/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: South China University of Technology<br>
‚Ä¢ Dataset: CSG-405, Samples: 147550, Modality: RGB videos + 2D skeletons<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.06530"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, BRAC University, Dhaka, Bangladesh<br>
‚Ä¢ Dataset: Sign3D-WLASL, Samples: 1983, Modality: 3D skeletal keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.06405"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DFKI, RPTU, Kaiserslautern, Germany<br>
‚Ä¢ Dataset: ImpAct, Samples: None, Modality: bio-impedance, IMU, video, 3D pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.06400"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VranLee/SU-T"><img src="https://img.shields.io/github/stars/VranLee/SU-T.svg?style=social&label=Star"></a><br><a href="https://vranlee.github.io/SU-T/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: China Agricultural University<br>
‚Ä¢ Dataset: Multiple Fish Tracking Dataset 2025 (MFT25), Samples: 15, Modality: RGB videos with annotated bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Learning to Track Any Points from Human Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.06233"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST AI<br>
‚Ä¢ Dataset: Anthro-LD, Samples: 1400, Modality: RGB videos + 2D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.05698"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mohsij/space-event-rgb-fusion"><img src="https://img.shields.io/github/stars/mohsij/space-event-rgb-fusion.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AI for Space Group, The University of Adelaide, Australia<br>
‚Ä¢ Dataset: FRESH, Samples: 24, Modality: RGB frames + event data + 6DoF poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Neural-Driven Image Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.05397"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://loongx1.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NUS<br>
‚Ä¢ Dataset: L-Mind, Samples: 23928, Modality: EEG, fNIRS, PPG, head motion (6-axis IMU), speech, image pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Beyond Features: How Dataset Design Influences Multi-Agent Trajectory Prediction Performance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.05098"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robert Bosch GmbH, Stuttgart, Germany<br>
‚Ä¢ Dataset: L4 Motion Forecasting dataset, Samples: 90k, Modality: LiDAR, cameras, radars<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.04955"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xinyueli2896/Expotion.git"><img src="https://img.shields.io/github/stars/xinyueli2896/Expotion.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates<br>
‚Ä¢ Dataset: Expotion Dataset, Samples: 7 hours of video, Modality: RGB videos of facial expressions and upper-body gestures<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for Particle Image Velocimetry</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.04750"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: International School, Beijing University of Posts and Telecommunications<br>
‚Ä¢ Dataset: Comprehensive PIV Benchmark Dataset, Samples: 19500, Modality: Synthetic particle image pairs + ground-truth velocity fields<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Grounded Gesture Generation: Language, Motion, and Space</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.04522"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://groundedgestures.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: Grounded Gestures, Samples: 6250, Modality: MoCap (HumanML3D format), Speech, 3D Scene Info<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.02948"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Westlake University, Zhejiang University<br>
‚Ä¢ Dataset: DriveMRP-10K, Samples: 10000, Modality: multimodal dataset comprising scene images (front-view, BEV), motion trajectories (waypoints), and textual annotations (VQA pairs, risk labels)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.02565"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.buzhenhuang.com/works/CloseApp.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southeast University, National University of Singapore<br>
‚Ä¢ Dataset: WildCHI, Samples: 100, Modality: RGB videos + pseudo ground-truth SMPL<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.02479"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/loseevaya/CrowdTrack"><img src="https://img.shields.io/github/stars/loseevaya/CrowdTrack.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: CrowdTrack, Samples: 33, Modality: RGB videos + bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.02200"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/ESTR-CoT"><img src="https://img.shields.io/github/stars/Event-AHU/ESTR-CoT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei 230601, China<br>
‚Ä¢ Dataset: CoT_ESTR, Samples: 16222, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.00660"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/crs524/MTCNet"><img src="https://img.shields.io/github/stars/crs524/MTCNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Medical Ultrasound Image Computing (MUSIC) Lab, School of Biomedical Engineering, Medical School, Shenzhen University, Shenzhen, China<br>
‚Ä¢ Dataset: 4D MV dataset, Samples: 160, Modality: 4D Ultrasound<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.00525"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://djamahl99.github.io/qaymo-pages/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Queensland, Brisbane, Australia<br>
‚Ä¢ Dataset: Box-QAymo, Samples: 13714, Modality: Camera images with box-referenced Q&A pairs derived from 3D object trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.00339"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/Amar-S/MOVi-MC-AC"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Lawrence Livermore National Laboratory<br>
‚Ä¢ Dataset: MOVi-MC-AC, Samples: 2041, Modality: Multi-camera RGB videos, Depth masks, Modal/Amodal segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.24074"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DurrLab/C3VD"><img src="https://img.shields.io/github/stars/DurrLab/C3VD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Biomedical Engineering, Johns Hopkins University<br>
‚Ä¢ Dataset: C3VDv2, Samples: 192, Modality: RGB videos + depth + surface normals + optical flow + 6-DoF camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23957"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sinoyou.github.io/gavs"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich, Switzerland<br>
‚Ä¢ Dataset: Repurposed DeepFused dataset, Samples: 15, Modality: RGB videos + 3D camera poses + dynamic object masks + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23759"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, NUS, Singapore<br>
‚Ä¢ Dataset: Hyst-YT, Samples: 1980, Modality: RGB surgical videos with part-level segmentation masks<br>
‚Ä¢ Dataset: Lob-YT, Samples: 203, Modality: RGB surgical videos with part-level segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23690"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lucaria-academy.github.io/SynMotion/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ant Group<br>
‚Ä¢ Dataset: MotionBench, Samples: 96-160, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Event-based Tiny Object Detection: A Benchmark Dataset and Baseline</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23575"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ChenYichen9527/Ev-UAV"><img src="https://img.shields.io/github/stars/ChenYichen9527/Ev-UAV.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not found in the provided document<br>
‚Ä¢ Dataset: EV-UAV, Samples: 147, Modality: Event camera stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>DexH2R: A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23152"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dexh2r.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: DexH2R, Samples: 4282, Modality: multi-view RGB-D streams, 3D annotations, robot kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.22718"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of Minnesota<br>
‚Ä¢ Dataset: Partial-RoboArt, Samples: None, Modality: 4D point clouds<br>
‚Ä¢ Dataset: Occluded-RoboArt, Samples: None, Modality: 4D point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.22554"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/seamless_interaction"><img src="https://img.shields.io/github/stars/facebookresearch/seamless_interaction.svg?style=social&label=Star"></a><br><a href="https://huggingface.co/datasets/facebook/seamless-interaction"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta<br>
‚Ä¢ Dataset: Seamless Interaction Dataset, Samples: 64739, Modality: RGB videos, Audio, SMPL-H poses, facial expression codes, text transcripts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Generating Attribute-Aware Human Motions from Textual Prompt</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.21912"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: HumanAttr, Samples: 18199, Modality: SMPL parameters from MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.21765"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/QiLi111/tus-rec-challenge_baseline"><img src="https://img.shields.io/github/stars/QiLi111/tus-rec-challenge_baseline.svg?style=social&label=Star"></a><br><a href="https://github-pages.ucl.ac.uk/tus-rec-challenge/TUS-REC2024/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UCL Hawkes Institute, Department of Medical Physics and Biomedical Engineering, University College London, London, WC1E 6BT, U.K.<br>
‚Ä¢ Dataset: TUS-REC2024, Samples: 2040, Modality: 2D ultrasound videos + 6-DoF pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.21680"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vinayak-vg/PhotonSplat"><img src="https://img.shields.io/github/stars/vinayak-vg/PhotonSplat.svg?style=social&label=Star"></a><br><a href="https://vinayak-vg.github.io/PhotonSplat/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Indian Institute of Technology, Madras, India<br>
‚Ä¢ Dataset: PhotonScenes, Samples: 9, Modality: multi-view SPAD binary images + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.20936"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://physrig.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Illinois Urbana Champaign<br>
‚Ä¢ Dataset: PhysRig Synthetic Dataset, Samples: 120, Modality: Simulated 3D mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.20550"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Tuebingen<br>
‚Ä¢ Dataset: BOAT360, Samples: None, Modality: Fisheye RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.20103"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: BrokenVideos, Samples: 3254, Modality: RGB videos + pixel-level masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.19851"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anima-x.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beihang University, China<br>
‚Ä¢ Dataset: None, Samples: 161023, Modality: Rigged 3D animation sequences (processed into multi-view videos and corresponding 2D pose maps)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>The Starlink Robot: A Platform and Dataset for Mobile Satellite Communication</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.19781"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/StarlinkRobot"><img src="https://img.shields.io/github/stars/github.com/StarlinkRobot.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University College London<br>
‚Ä¢ Dataset: Starlink Robot Dataset, Samples: None, Modality: robot kinematics, pose trajectories, LiDAR point clouds, fisheye camera images, IMU, GPS, communication metrics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Deblurring in the Wild: A Real-World Dataset from Smartphone High-Speed Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.19445"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Dhaka<br>
‚Ä¢ Dataset: SloMoDeblur, Samples: 42045, Modality: blur-sharp image pairs from high-speed videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EvDetMAV: Generalized MAV Detection from Moving Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.19416"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WindyLab/EvDetMAV"><img src="https://img.shields.io/github/stars/WindyLab/EvDetMAV.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology, Zhejiang University<br>
‚Ä¢ Dataset: EventMAV, Samples: 25335, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Matrix-Game: Interactive World Foundation Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.18701"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SkyworkAI/Matrix-Game"><img src="https://img.shields.io/github/stars/SkyworkAI/Matrix-Game.svg?style=social&label=Star"></a><br><a href="https://matrix-game-homepage.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Skywork AI<br>
‚Ä¢ Dataset: Matrix-Game-MC, Samples: 2,700 hours of unlabeled video clips, >1,000 hours of labeled video clips, Modality: RGB videos + action labels (keyboard, mouse) + agent kinematics (position, velocity, orientation)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.18443"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZzhYgwh/TwistEstimator"><img src="https://img.shields.io/github/stars/ZzhYgwh/TwistEstimator.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Automation, Northwestern Polytechnical University, Xi‚Äôan, Shaanxi, 710129 P.R. China.<br>
‚Ä¢ Dataset: None, Samples: 10, Modality: Event camera data, 4D mmWave radar data, IMU data, RTK-GPS ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.17590"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/taco-group/DRAMA-X"><img src="https://img.shields.io/github/stars/taco-group/DRAMA-X.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Texas A&M University<br>
‚Ä¢ Dataset: DRAMA-X, Samples: 5686, Modality: RGB video clips + object trajectories + intent labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.17332"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="P2MFDS Github Repository"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Northwestern Polytechnical University<br>
‚Ä¢ Dataset: None, Samples: 18000, Modality: mmWave 3D point cloud + 3D vibration data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.16856"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/little-snail-f/ParkFormer"><img src="https://img.shields.io/github/stars/little-snail-f/ParkFormer.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Automation, Chinese Academy of Sciences,Beijing 100190, China<br>
‚Ä¢ Dataset: ParkFormer Dataset, Samples: 272, Modality: RGB images, depth maps, ego-motion states (velocity, acceleration), pedestrian trajectories, control commands<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EchoShot: Multi-Shot Portrait Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.15838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://johnneywang.github.io/EchoShot-webpage"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xi‚Äôan Jiaotong University<br>
‚Ä¢ Dataset: PortraitGala, Samples: 650000, Modality: RGB videos + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Toward Rich Video Human-Motion2D Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.14428"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/FooAuto/Toward-Rich-Video-Human-Motion2D-Generation"><img src="https://img.shields.io/github/stars/FooAuto/Toward-Rich-Video-Human-Motion2D-Generation.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Technology, Tongji University, China<br>
‚Ä¢ Dataset: Motion2D-Video-150K, Samples: 150000, Modality: 2D skeleton sequences with textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Action Dubber: Timing Audible Actions via Inflectional Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.13320"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WenlongWan/Audible623"><img src="https://img.shields.io/github/stars/WenlongWan/Audible623.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Engineering, South China University of Technology<br>
‚Ä¢ Dataset: Audible 623, Samples: 623, Modality: RGB videos + frame-level annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>MAMMA: Markerless & Automatic Multi-Person Motion Action Capture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.13040"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, Germany<br>
‚Ä¢ Dataset: MAMMASyn, Samples: 2.8k sequences / 2.5M samples, Modality: Synthetic multi-view RGB video + SMPL-X annotations + dense 2D landmarks + segmentation masks<br>
‚Ä¢ Dataset: Latin-Dance, Samples: 10 sequences, Modality: Vicon MoCap data<br>
‚Ä¢ Dataset: Interacting Couples, Samples: 48 sequences, Modality: Vicon MoCap data<br>
‚Ä¢ Dataset: MAMMAEval-Singles, Samples: 22 sequences, Modality: Multi-view RGB video + Vicon MoCap data + SMPL-X annotations<br>
‚Ä¢ Dataset: MAMMAEval-Dance, Samples: 17 sequences, Modality: Multi-view RGB video + Vicon MoCap data + SMPL-X annotations<br>
‚Ä¢ Dataset: MAMMAEval-Extra, Samples: 16 sequences, Modality: Multi-view RGB video + Vicon MoCap data (standard and extra markers) + SMPL-X annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Multiple Object Tracking in Video SAR: A Benchmark and Tracking Baseline</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.12105"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/softwarePupil/VSMB"><img src="https://img.shields.io/github/stars/softwarePupil/VSMB.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Information Engineering, Beihang University<br>
‚Ä¢ Dataset: Video SAR MOT Benchmark (VSMB), Samples: 45, Modality: Video SAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.11774"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Indian Institute of Technology Kanpur<br>
‚Ä¢ Dataset: Isometric-Multiclass Dataset (IMCD), Samples: 4339, Modality: 2D pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>RationalVLA: A Rational Vision-Language-Action Model with Dual System</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.10826"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://irpn-eai.github.io/RationalVLA/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: RAtional MAnipulation (RAMA), Samples: 14412, Modality: Language instructions + RGB images + robot arm kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial Motion Capture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.10580"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZuoCX1996/TIC"><img src="https://img.shields.io/github/stars/ZuoCX1996/TIC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xiamen University, China<br>
‚Ä¢ Dataset: DSTIC, Samples: 1.04M sequences, Modality: Optical motion capture (body pose, absolute IMU orientation and acceleration), raw IMU readings<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.10353"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://Motion-R1.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: GigaAI<br>
‚Ä¢ Dataset: MotionCoT Data Engine, Samples: None, Modality: Chain-of-Thought (CoT) annotations paired with existing motion descriptions and motion sequences (MoCap)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.09997"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta, UC Merced<br>
‚Ä¢ Dataset: Customized Kubric dataset, Samples: 40000, Modality: Synthetic multi-view videos + 3D scene flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.09663"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Anhui University<br>
‚Ä¢ Dataset: RS-Art, Samples: 18 objects (6 categories, 3 instances each), each with 7 distinct articulation states and over 400 observations per instance., Modality: RGB-D captures, camera poses (intrinsics/extrinsics), reverse-engineered 3D models (USD, URDF, PLY) with part meshes, textures, joint definitions, and physics properties.<br>
‚Ä¢ Dataset: PartNet-Mobility (extended), Samples: Extended with 2-3 new instances per category across 8 categories, plus 4 objects with three movable parts., Modality: Synthetic 3D models with articulation data.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Synthetic Human Action Video Data Generation with Pose Transfer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.09411"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="synthetic-human-action.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SSPS<br>
‚Ä¢ Dataset: RANDOM People, Samples: 3600, Modality: Synthetic RGB videos + 3D Gaussian avatars + source identity videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Audio-Sync Video Generation with Multi-Stream Temporal Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.08003"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hjzheng.net/projects/MTV/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing Academy of Artificial Intelligence<br>
‚Ä¢ Dataset: DEMIX, Samples: 392000, Modality: RGB videos + demixed audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07865"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vLAR-group/FreeGave"><img src="https://img.shields.io/github/stars/vLAR-group/FreeGave.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: vLAR Group, The Hong Kong Polytechnic University<br>
‚Ä¢ Dataset: FreeGave-GoPro Dataset, Samples: 6, Modality: multi-view RGB videos with camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07860"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/uzh-rpg/event_based_ping_pong_ball_trajectory_prediction"><img src="https://img.shields.io/github/stars/uzh-rpg/event_based_ping_pong_ball_trajectory_prediction.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics and Perception Group, University of Zurich, Switzerland<br>
‚Ä¢ Dataset: Egocentric Event-Based Ping Pong Trajectories, Samples: 30, Modality: 3D ground-truth ball trajectories, event streams, RGB videos, eye-tracking, IMU, audio, SLAM poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07603"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: SurgBench, Samples: 225250, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>OpenDance: Multimodal Controllable 3D Dance Generation Using Large-scale Internet Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07565"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://open-dance.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: OpenDance5D, Samples: 41000, Modality: RGB video, audio waveform, 2D skeletal keypoints, 3D SMPL motion, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Improving Traffic Signal Data Quality for the Waymo Open Motion Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07150"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/michigan-traffic-lab/WOMD-Traffic-Signal-Data-Improvement"><img src="https://img.shields.io/github/stars/michigan-traffic-lab/WOMD-Traffic-Signal-Data-Improvement.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Civil and Environmental Engineering, University of Michigan, Ann Arbor, MI, USA<br>
‚Ä¢ Dataset: Waymo Open Motion Dataset - Improved Traffic Signal Data, Samples: over 360,000 scenarios, Modality: Vehicle trajectories with imputed traffic signal states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.06596"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://fulmen67.github.io/EV-LayerSegNet"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Advanced Research and Innovation Center, Khalifa University<br>
‚Ä¢ Dataset: Not explicitly named in the paper, Samples: 1025, Modality: simulated event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>(LiFT) Lightweight Fitness Transformer: A language-vision model for Remote Monitoring of Physical Training</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.06480"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Electrical and Computer Engineering University of California, San Diego<br>
‚Ä¢ Dataset: Olympia, Samples: 7618, Modality: RGB videos + 3D skeletal keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.06318"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southeast University<br>
‚Ä¢ Dataset: GyroPeak-100, Samples: None, Modality: IMU signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>ChronoTailor: Harnessing Attention Guidance for Fine-Grained Video Virtual Try-On</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.05858"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Communication University of China<br>
‚Ä¢ Dataset: StyleDress, Samples: 12500, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Gen4D: Synthesizing Humans and Scenes in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.05397"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Vision and Image Processing Lab, University of Waterloo, Canada<br>
‚Ä¢ Dataset: SportPAL, Samples: 2012, Modality: Synthetic images with 2D/3D human pose annotations and SMPLX parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>FreeTimeGS: Free Gaussian Primitives at Anytime and Anywhere for Dynamic Scene Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.05348"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zju3dv.github.io/freetimegs/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: SelfCap, Samples: 8, Modality: multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>FRED: The Florence RGB-Event Drone Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.05163"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://miccunifi.github.io/FRED/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Florence<br>
‚Ä¢ Dataset: FRED (Florence RGB-Event Drone dataset), Samples: 4620, Modality: RGB videos and event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.04224"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://oxdan.active.vision/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Oxford<br>
‚Ä¢ Dataset: Oxford Day-and-Night, Samples: 491750, Modality: Egocentric RGB/grayscale video + IMU + 6DoF camera poses + 3D point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EV-Flying: an Event-based Dataset for In-The-Wild Recognition of Flying Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.04048"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Florence<br>
‚Ä¢ Dataset: EV-Flying, Samples: 3206, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Controllable Human-centric Keyframe Interpolation with Generative Prior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.03119"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gseancdat.github.io/projects/PoseFuse3D_KI"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: CHKI-Video, Samples: 2614, Modality: RGB videos + 2D poses + 3D SMPL-X parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.03107"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ByteDance-Seed/BM-code"><img src="https://img.shields.io/github/stars/ByteDance-Seed/BM-code.svg?style=social&label=Star"></a><br><a href="https://boese0601.github.io/bytemorph"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ByteDance Seed<br>
‚Ä¢ Dataset: ByteMorph-6M, Samples: 6450000, Modality: image pairs + text instructions<br>
‚Ä¢ Dataset: ByteMorph-Bench, Samples: 613, Modality: image pairs + text instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.02845"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LEI-QI-233/HAR-in-Space"><img src="https://img.shields.io/github/stars/LEI-QI-233/HAR-in-Space.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Karlsruhe Institute of Technology<br>
‚Ä¢ Dataset: MicroG-4M, Samples: 4759, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.02733"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/LecterF/LinkTo-Anime"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Macau University of Science and Technology, Macau, China<br>
‚Ä¢ Dataset: LinkTo-Anime, Samples: 395, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>InterRVOS: Interaction-aware Referring Video Object Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.02356"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cvlab-kaist.github.io/InterRVOS"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST AI<br>
‚Ä¢ Dataset: InterRVOS-8K, Samples: 8738, Modality: RGB videos + language expressions + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.01674"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nju-pcalab.github.io/projects/MotionSight"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: MotionVid-QA, Samples: 40000, Modality: RGB videos + QA pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.01608"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/amathislab/EPFL-Smart-Kitchen"><img src="https://img.shields.io/github/stars/amathislab/EPFL-Smart-Kitchen.svg?style=social&label=Star"></a><br><a href="https://github.com/amathislab/EPFL-Smart-Kitchen"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: √âcole Polytechnique F√©d√©rale de Lausanne (EPFL), Lausanne<br>
‚Ä¢ Dataset: EPFL-Smart-Kitchen-30, Samples: 60189, Modality: RGB-D videos, IMU, eye gaze, 3D hand poses, 3D body poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.00411"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: LoHoSet, Samples: 23000, Modality: RGB-D images, language instructions, robot actions (end-effector Cartesian positions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Sequence-Based Identification of First-Person Camera Wearers in Third-Person Views</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.00394"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Indiana University Bloomington<br>
‚Ä¢ Dataset: TF2025, Samples: None, Modality: Synchronized first- and third-person videos with segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>MotionPersona: Characteristics-aware Locomotion Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.00173"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motionpersona25.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: MotionPersona, Samples: 3150, Modality: MoCap joints (SMPL-X)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>From Motion to Behavior: Hierarchical Modeling of Humanoid Generative Behavior Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.00043"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sun Yat-sen University<br>
‚Ä¢ Dataset: GBC-100K, Samples: 123700, Modality: RGB videos + SMPL parameters + hierarchical textual annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Spatiotemporal Analysis of Forest Machine Operations Using 3D Video Classification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.24375"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.nibio.no/maciekwielgosz/forest_video_classification"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Norwegian Institute of Bioeconomy Research (NIBIO)<br>
‚Ä¢ Dataset: Forest Machine Operations Video Dataset, Samples: 349, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.24249"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China<br>
‚Ä¢ Dataset: bronchoscopy localization dataset, Samples: 66, Modality: Bronchoscopy videos + annotated 6-DOF camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Autoregressive Meta-Actions for Unified Controllable Trajectory Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.23612"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://arma-traj.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: Waymo Motion Dataset with frame-level meta-action annotations, Samples: None, Modality: Agent trajectories and map data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.23525"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xyz123xyz456/hallo4"><img src="https://img.shields.io/github/stars/xyz123xyz456/hallo4.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: Audio-Driven Portrait DPO Dataset, Samples: 20000, Modality: RGB video pairs with human preference labels (motion-video alignment, portrait fidelity)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Semantics-Aware Human Motion Generation from Audio Instructions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.23465"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Chinese Academy of Sciences, China<br>
‚Ä¢ Dataset: Oral Dataset (from HumanML3D), Samples: 87384, Modality: audio instructions + MoCap-based pose sequences<br>
‚Ä¢ Dataset: Oral Dataset (from KIT-ML), Samples: 12696, Modality: audio instructions + MoCap-based pose sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.22977"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vivocameraresearch.github.io/hypermotion/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Centre for Computer Animation, Bournemouth University, UK<br>
‚Ä¢ Dataset: Open-HyperMotionX Dataset, Samples: 19597, Modality: RGB videos + 2D pose annotations<br>
‚Ä¢ Dataset: HyperMotionX Bench, Samples: 100, Modality: RGB videos + 2D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.22859"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://muskie82.github.io/4dtam/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dyson Robotics Laboratory, Imperial College London<br>
‚Ä¢ Dataset: Sim4D, Samples: 50, Modality: RGB images, depth, surface normals, foreground masks, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.22769"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of St Andrews<br>
‚Ä¢ Dataset: MotionGaze, Samples: 803K+ IMU readings, Modality: IMU (accelerometer, gyroscope), RGB images, Gaze coordinates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.22335"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://aczheng-cai.github.io/up_slam.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Chinese Academy of Sciences<br>
‚Ä¢ Dataset: Bonn RGB-D Dynamic Dataset masks, Samples: None, Modality: Dynamic object masks for RGB-D sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.21653"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bwgzk-keke.github.io/DiffPhy/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Johns Hopkins University<br>
‚Ä¢ Dataset: PhyHQ, Samples: 8000, Modality: RGB videos + text captions + physical phenomena labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Frame In-N-Out: Unbounded Controllable Image-to-Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.21491"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://uva-computer-vision-lab.github.io/Frame-In-N-Out/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Virginia<br>
‚Ä¢ Dataset: Frame In-N-Out Training Dataset, Samples: 72300, Modality: RGB videos with processed metadata including motion trajectories, text prompts, bounding boxes, and identity reference images<br>
‚Ä¢ Dataset: Frame In-N-Out Evaluation Benchmark, Samples: 372, Modality: RGB videos with processed metadata including motion trajectories, text prompts, bounding boxes, and identity reference images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>RefAV: Towards Planning-Centric Scenario Mining</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.20981"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="cainand.github.io/RefAV/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: RefAV, Samples: 1000, Modality: LiDAR, 360¬∞ ring cameras, HD maps, 3D track annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.20460"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: VCIP, CS, Nankai University, Horizon Robotics<br>
‚Ä¢ Dataset: PM-X, Samples: 600, Modality: Dual-state rendered images, URDF annotations, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>HAODiff: Human-Aware One-Step Diffusion via Dual-Prompt Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.19742"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gobunu/HAODiff"><img src="https://img.shields.io/github/stars/gobunu/HAODiff.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: MPII-Test, Samples: 5427, Modality: RGB images with human motion blur<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.19386"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nate-gillman/force-prompting"><img src="https://img.shields.io/github/stars/nate-gillman/force-prompting.svg?style=social&label=Star"></a><br><a href="https://force-prompting.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: Global wind force dataset, Samples: 15000, Modality: Synthetic RGB videos + Force vectors<br>
‚Ä¢ Dataset: Local point force dataset, Samples: 23000, Modality: Synthetic RGB videos + Force vectors + Application points<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>PosePilot: An Edge-AI Solution for Posture Correction in Physical Exercises</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.19186"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gadhvirushiraj/PosePilot"><img src="https://img.shields.io/github/stars/gadhvirushiraj/PosePilot.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: HTI Lab, Plaksha University, Mohali, India<br>
‚Ä¢ Dataset: In-house Dataset, Samples: 336, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>EventEgoHands: Event-based Egocentric 3D Hand Mesh Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.19169"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Keio University<br>
‚Ä¢ Dataset: N-HOT3D, Samples: 447704, Modality: synthetic event streams + 3D hand mesh annotations (MANO parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>BiomechGPT: Towards a Biomechanically Fluent Multimodal Foundation Model for Clinically Relevant Motion Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.18465"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Neuroscience, Northwestern University; Shirley Ryan AbilityLab<br>
‚Ä¢ Dataset: Not explicitly named, Samples: 27000 motion-question-answer pairs, Modality: Biomechanical trajectories (joint angles and velocities)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.17677"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ophnet-3d.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Monash University<br>
‚Ä¢ Dataset: OphNet-3D, Samples: 41, Modality: Multi-view RGB-D videos with annotated MANO hand meshes and 6-DoF instrument poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.17201"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Informatics Institute, University of Amsterdam<br>
‚Ä¢ Dataset: Fish Data, Samples: 3796, Modality: RGB videos + MOT annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.17114"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/BASHLab/RAVEN"><img src="https://img.shields.io/github/stars/BASHLab/RAVEN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical & Computer Engineering
Worcester Polytechnic Institute<br>
‚Ä¢ Dataset: AVS-QA, Samples: 300000, Modality: RGB video + audio + IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.16602"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Peking University<br>
‚Ä¢ Dataset: MEgoHand Unified Dataset, Samples: 24000, Modality: RGB-D frames + MANO hand parameters + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.16249"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China.<br>
‚Ä¢ Dataset: DOFS (Deformable Object with Full Spatial information), Samples: 120 episodes (70 simulation, 50 real-world), Modality: multi-view RGB-D images, robot gripper poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.15287"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://intothemild.github.io/GS2E.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Computer Engineering, Peking University<br>
‚Ä¢ Dataset: GS2E, Samples: 1150, Modality: RGB frames, event streams, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Intentional Gesture: Deliver Your Intentions with Gestures for Speech</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.15197"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://andypinxinliu.github.io/Intentional-Gesture"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Rochester<br>
‚Ä¢ Dataset: InG (Intention-Grounded Gestures), Samples: 47913, Modality: SMPL-based gesture data + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.14866"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nisarganc.github.io/UPTor-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Bosch Corporate Research, Robert Bosch GmbH, Stuttgart, Germany, University of Technology Nuremberg (UTN), Germany<br>
‚Ä¢ Dataset: DARKO, Samples: 508, Modality: 3D Poses from RGB<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Learning collision risk proactively from naturalistic driving data at scale</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.13556"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yiru-Jiao/GSSM"><img src="https://img.shields.io/github/stars/Yiru-Jiao/GSSM.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.15787/VTT1/EFYEJR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Delft University of Technology<br>
‚Ä¢ Dataset: Bird‚Äôs eye view trajectory reconstruction of naturalistic crashes and near-crashes in the SHRP2 NDS, Samples: 6664, Modality: Bird's eye view trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>eStonefish-scenes: A synthetically generated dataset for underwater event-based optical flow prediction tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.13309"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/CIRS-Girona/estonefish-scenes"><img src="https://img.shields.io/github/stars/CIRS-Girona/estonefish-scenes.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/15130453"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Vision and Robotics Research Institute (ViCOROB), Universitat de Girona (UdG), Spain<br>
‚Ä¢ Dataset: eStonefish-scenes, Samples: 8, Modality: event streams, grayscale images, ground truth optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Event-Driven Dynamic Scene Depth Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.13279"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NUS<br>
‚Ä¢ Dataset: EventDC-Real, Samples: 15845, Modality: Color images, LiDAR, Event streams<br>
‚Ä¢ Dataset: EventDC-SemiSyn, Samples: 9307, Modality: Color images, LiDAR, Event streams<br>
‚Ä¢ Dataset: EventDC-FullSyn, Samples: 21500, Modality: Color images, Depth data, Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>FlowCut: Unsupervised Video Instance Segmentation via Temporal Mask Matching</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.13174"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Vision Group, University of Bern, Switzerland<br>
‚Ä¢ Dataset: FlowCut pseudo-labeled video dataset, Samples: 167365, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.12774"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Western Australia<br>
‚Ä¢ Dataset: UniHM Dataset, Samples: 44962, Modality: SMPL motion + text descriptions + scene voxels + object point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Event-based Star Tracking under Spacecraft Jitter: the e-STURT Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.12588"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zenodo.org/records/14031911"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Australian Institute for Machine Learning<br>
‚Ä¢ Dataset: e-STURT (Event-based Star Tracking Under Jitter), Samples: 200, Modality: event streams + piezoelectric actuator telemetry<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.11920"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University<br>
‚Ä¢ Dataset: H2R-UR5-SSv2-1M-Gripper, Samples: 62500, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-SSv2-1M-Leaphand, Samples: 62500, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-SSv2-1M-Mix, Samples: 62500, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-Franka-SSv2-1M, Samples: 62500, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-Ego4D-1M-Gripper, Samples: 117624, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-Ego4D-1M-Leaphand, Samples: 117624, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-Ego4D-1M-Mix, Samples: 117624, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-Franka-Ego4D-1M, Samples: 117624, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.11868"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National University of Defense Technology<br>
‚Ä¢ Dataset: MonoMobility Dataset, Samples: 26, Modality: monocular videos, 3D point clouds, motion part segmentation, motion attribute annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ElderFallGuard: Real-Time IoT and Computer Vision-Based Fall Detection System for Elderly Safety</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.11845"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information and Communicaton Technology, Bangladesh University of Engineering Technology, Dhaka, bangladesh<br>
‚Ä¢ Dataset: ElderFallGuard dataset, Samples: 7200, Modality: RGB images + 2D pose landmarks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.11282"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="github.com/shrutarv/MTevent_toolkit"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Dortmund<br>
‚Ä¢ Dataset: MTevent, Samples: 75, Modality: Stereo event camera streams, RGB videos, MoCap poses, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Robust 2D lidar-based SLAM in arboreal environments without IMU/GNSS</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.10847"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Pontificia Universidad Cat ¬¥olica de Chile<br>
‚Ä¢ Dataset: Pullally, Samples: None, Modality: LiDAR, RTK-GNSS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.10696"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tartanair.org/tartanground"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotic Systems Lab, ETH Zurich, Zurich, Switzerland<br>
‚Ä¢ Dataset: TartanGround, Samples: 910 trajectories / 1.5 million samples, Modality: RGB stereo video, depth, optical flow, stereo disparity, LiDAR point clouds, ground truth poses, semantic segmented images, occupancy maps, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.10205"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universitat de Barcelona, Spain<br>
‚Ä¢ Dataset: Foodkit, Samples: 21, Modality: RGB videos, image sequences, camera pose trajectories, segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.09694"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AgibotTech/EWMBench"><img src="https://img.shields.io/github/stars/AgibotTech/EWMBench.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AgiBot, HIT<br>
‚Ä¢ Dataset: EWMBENCH Dataset, Samples: 100, Modality: RGB videos, robot end-effector trajectories, textual instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.08986"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Arkansas, Fayetteville, AR, USA<br>
‚Ä¢ Dataset: ChicGrasp, Samples: 50, Modality: RGB video, robot proprioception (joint positions, joint velocities), binary gripper states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>RDD: Robust Feature Detector and Descriptor using Deformable Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.08013"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://xtcpete.github.io/rdd/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Creative Technologies, University of Southern California<br>
‚Ä¢ Dataset: MegaDepth-View, Samples: 1487, Modality: RGB images + camera poses + depth maps<br>
‚Ä¢ Dataset: Air-to-Ground, Samples: 1500, Modality: RGB images + camera poses + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>BETTY Dataset: A Multi-modal Dataset for Full-Stack Autonomy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.07266"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pitt-mit-iac.github.io/betty-dataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics Institute, Carnegie Mellon University<br>
‚Ä¢ Dataset: BETTY Dataset, Samples: None, Modality: Camera, LiDAR, Radar, IMU, GNSS, Odometry, Planned Trajectories, Actuation Commands (Throttle, Brake, Steering), Tire State (temperature, pressure, speed, torque, slip angle), Suspension<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.07007"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zyzhangUstc/MELLM"><img src="https://img.shields.io/github/stars/zyzhangUstc/MELLM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, University of Science and Technology of China<br>
‚Ä¢ Dataset: Instruction-following MEU Dataset, Samples: 4793, Modality: Instruction-description pairs and motion-enhanced color maps derived from optical flow of video frames (based on the DFME dataset).<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.06537"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence, Beihang University; The Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: MRFashion-7K, Samples: 7335, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.05001"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nie-lang/StabStitch2"><img src="https://img.shields.io/github/stars/nie-lang/StabStitch2.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Science, Beijing Jiaotong University<br>
‚Ä¢ Dataset: StabStitch-D, Samples: over 100 video pairs, Modality: RGB video pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.04974"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wengwanjiang.github.io/ReAlign-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Engineering, Southeast University, Nanjing, China<br>
‚Ä¢ Dataset: BiHumanML3D, Samples: 13312, Modality: 3D motion sequences + bilingual (English/Chinese) text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.04203"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Qzping/ELGAR"><img src="https://img.shields.io/github/stars/Qzping/ELGAR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Central Conservatory of Music, China and Tsinghua University, China<br>
‚Ä¢ Dataset: SPD-GEN, Samples: 81, Modality: MoCap joint rotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>A Dataset and Toolkit for Multiparameter Cardiovascular Physiology Sensing on Rings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.04172"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/thuhci/RingTool"><img src="https://img.shields.io/github/stars/thuhci/RingTool.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: ùúè-Ring, Samples: 28.21 hours of raw data from 34 subjects, Modality: PPG (infrared and red channels), 3-axis accelerometer data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.04002"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mshoe/PARC"><img src="https://img.shields.io/github/stars/mshoe/PARC.svg?style=social&label=Star"></a><br><a href="https://github.com/mshoe/PARC"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Simon Fraser University, Canada<br>
‚Ä¢ Dataset: Initial Parkour Dataset, Samples: None, Modality: MoCap joints with contact labels<br>
‚Ä¢ Dataset: PARC Generated Dataset, Samples: approximately 3000 sequences, Modality: Physics-corrected kinematic motions with contact labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03738"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://amo-humanoid.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC San Diego<br>
‚Ä¢ Dataset: AMO dataset, Samples: None, Modality: Dynamically-feasible whole-body reference joint angles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>A Unit Enhancement and Guidance Framework for Audio-Driven Avatar Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03603"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: CNAS (Chinese News Anchor Speech Dataset), Samples: 1473, Modality: RGB videos + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>AquaticVision: Benchmarking Visual SLAM in Underwater Environment with Events and Frames</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03448"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/aquaticvision-lias"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Data Science, The Chinese University of Hong Kong, Shenzhen, P. R. China<br>
‚Ä¢ Dataset: AquaticVision, Samples: 9, Modality: Stereo event streams, stereo grayscale frames, IMU, motion capture trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03154"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Simon Fraser University, Canada<br>
‚Ä¢ Dataset: BrokenAMASS, Samples: over 40 hours, Modality: MoCap skeleton<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03116"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Lab of Multispectral Information Intelligent Processing Technology School of Artificial Intelligence and Automation, Huazhong University of Science and Technology<br>
‚Ä¢ Dataset: Complex, High-speed Motion (CHMD), Samples: None, Modality: RGB videos + Events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.01838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://kevinlee09.github.io/research/MVHumanNet++/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen<br>
‚Ä¢ Dataset: MVHumanNet++, Samples: 60000, Modality: Multi-view RGB videos, human masks, camera parameters, 2D/3D keypoints, SMPL/SMPLX parameters, normal maps, depth maps, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.01746"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mattie-e.github.io/Co3/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: GES-Inter, Samples: 27390, Modality: 3D human postures (SMPL-X) + separated audio + text transcripts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.00866"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kocurvik/rdnet"><img src="https://img.shields.io/github/stars/kocurvik/rdnet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Mathematics, Physics and Informatics, Comenius University Bratislava, Bratislava, Slovakia.<br>
‚Ä¢ Dataset: ROTUNDA and CATHEDRAL, Samples: 2891, Modality: RGB images + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>SOTA: Spike-Navigated Optimal TrAnsport Saliency Region Detection in Composite-bias Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.00394"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lwxfight/sota"><img src="https://img.shields.io/github/stars/lwxfight/sota.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory for Multimedia Information Processing, Peking University; Hubei Key Laboratory of Transportation Internet of Things, Wuhan University of Technology<br>
‚Ä¢ Dataset: SPIKE-DAVIS, Samples: None, Modality: Synthetic spike streams from interpolated videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Fine-grained spatial-temporal perception for gas leak segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.00295"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of British Columbia - Okanagan<br>
‚Ä¢ Dataset: GasVid, Samples: 187, Modality: infrared videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.21718"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing University of Posts and Telecommunications, Beijing, China<br>
‚Ä¢ Dataset: ListenerX, Samples: 6683, Modality: 3D parametric head model (FLAME) sequences, speaker audio, text descriptions, emotion intensity tags<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.20808"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bit-bots.github.io/SoccerDiffusion"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Hamburg<br>
‚Ä¢ Dataset: SoccerDiffusion Dataset, Samples: 88, Modality: RGB images, IMU rotations, joint states, joint commands, game state<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.20234"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UP-COUNT/tracking"><img src="https://img.shields.io/github/stars/UP-COUNT/tracking.svg?style=social&label=Star"></a><br><a href="https://up-count.github.io/tracking"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Robotics and Machine Intelligence, Pozna ¬¥n University of Technology<br>
‚Ä¢ Dataset: UP-COUNT-TRACK, Samples: 31, Modality: RGB videos + trajectory annotations + sensor metadata<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.19654"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Loughborough University, Epinal Way, Loughborough, LE11 3TU, Leicestershire, United Kingdom<br>
‚Ä¢ Dataset: DRL-generated 2D SLAM error dataset (unnamed in paper), Samples: 75000, Modality: 2D Occupancy Grid Maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.19514"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Lappeenranta-Lahti University of Technology LUT, Finland<br>
‚Ä¢ Dataset: FSAnno, Samples: 783, Modality: Motion data, skeleton data, RGB video, audio, text annotations<br>
‚Ä¢ Dataset: FSBench, Samples: 783, Modality: Motion data, skeleton data, RGB video, audio, text annotations (QA pairs, comments, scores)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.18864"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: Particle Scenes with Spike and Displacement (PSSD), Samples: 30000 sample sequences (10,000 for each of 3 sub-datasets), Modality: Spike streams + ground truth displacement fields + images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>E-VLC: A Real-World Dataset for Event-based Visible Light Communication And Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.18521"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://woven-visionai.github.io/evlc-dataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Woven by Toyota<br>
‚Ä¢ Dataset: E-VLC, Samples: 110, Modality: Event camera streams, Frame-based videos, Ground-truth camera poses, LED marker positions, Bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Dynamic Camera Poses and Where to Find Them</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17788"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://research.nvidia.com/labs/dir/dynpose-100k"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA, University of Michigan<br>
‚Ä¢ Dataset: DynPose-100K, Samples: 100131, Modality: RGB videos + camera poses<br>
‚Ä¢ Dataset: Lightspeed, Samples: 36, Modality: Synthetic videos + ground truth camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17414"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://2y7c3.github.io/3DV-TON/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DAMO Academy, Alibaba Group; Hupan Lab<br>
‚Ä¢ Dataset: HR-VVT, Samples: 130, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17371"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://deepscenario.github.io/DSC3D/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DeepScenario, TU Munich, Munich Center for Machine Learning<br>
‚Ä¢ Dataset: DeepScenario Open 3D Dataset (DSC3D), Samples: 175000, Modality: 6DoF bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Contrastive Learning for Continuous Touch-Based Authentication</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17271"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: North China University of Technology<br>
‚Ä¢ Dataset: Ffinger, Samples: interaction data from 29 participants, Modality: Multi-channel time series of finger trajectories (coordinates, timestamp, pressure, area, velocity, direction)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Advancing Radar Hand Gesture Recognition: A Hybrid Spectrum Synthetic Framework Merging Simulation with Neural Networks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.16423"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Institute of Health Data Science, Peking University<br>
‚Ä¢ Dataset: unnamed radar-vision HGR dataset, Samples: None, Modality: mmWave radar signals, 3D hand joint coordinates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>SILM: A Subjective Intent Based Low-Latency Framework for Multiple Traffic Participants Joint Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.16377"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory of General Artificial Intelligence, Key Laboratory of Machine Perception (MoE), School of Intelligence Science and Technology, Peking University, Beijing 100871, China.<br>
‚Ä¢ Dataset: CAIC-TP, Samples: more than 25000 sequences, Modality: Trajectory sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.15786"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gdaosu.github.io/sat2groundscape"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Ohio State University<br>
‚Ä¢ Dataset: Sat2GroundScape, Samples: 45000 sequences (estimated from 90 scenes * ~500 sequences/scene); containing 25,000+ panoramic pairs and 100,000+ perspective pairs, Modality: Satellite images, ground-view panoramic/perspective images, depth maps, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Towards Understanding Camera Motions in Any Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.15376"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/linzhiqiu/camerabench"><img src="https://img.shields.io/github/stars/linzhiqiu/camerabench.svg?style=social&label=Star"></a><br><a href="https://linzhiqiu.github.io/papers/camerabench"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CMU<br>
‚Ä¢ Dataset: CameraBench, Samples: 3381, Modality: RGB videos + camera motion labels + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.14602"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://k2muse.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; University of Chinese Academy of Sciences, Beijing, China<br>
‚Ä¢ Dataset: K2MUSE, Samples: None, Modality: kinematics (MoCap markers), kinetics (force plates), amplitude-mode ultrasound (AUS), surface electromyography (sEMG)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>MILUV: A Multi-UAV Indoor Localization dataset with UWB and Vision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.14376"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/decargroup/miluv"><img src="https://img.shields.io/github/stars/decargroup/miluv.svg?style=social&label=Star"></a><br><a href="https://decargroup.github.io/miluv/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: McGill University<br>
‚Ä¢ Dataset: MILUV, Samples: 36, Modality: UWB, stereo vision, monocular vision, IMU, laser rangefinder, magnetometer, MoCap poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Adversarial Locomotion and Motion Imitation for Humanoid Policy Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.14305"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://almi-humanoid.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Artificial Intelligence (TeleAI), China Telecom<br>
‚Ä¢ Dataset: ALMI-X, Samples: 81,549 trajectories, Modality: Robot kinematic trajectories (states, actions, joint angles, etc.) with language descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.12284"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bit.ly/LatentAct"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Illinois Urbana-Champaign<br>
‚Ä¢ Dataset: HoloAssist with 3D hand motion and contact annotations, Samples: 15000, Modality: 3D hand pose trajectories + contact maps (from egocentric RGB videos)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Coding-Prior Guided Diffusion Network for Video Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.12222"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="Not provided in the paper."><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the paper.<br>
‚Ä¢ Dataset: GoPro dataset augmented with coding priors, Samples: 3214 image pairs, Modality: RGB videos + Motion Vectors + Coding Residuals<br>
‚Ä¢ Dataset: DVD dataset augmented with coding priors, Samples: 71 videos, Modality: RGB videos + Motion Vectors + Coding Residuals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>CodingHomo: Bootstrapping Deep Homography With Video Coding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.12165"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/liuyike422/CodingHomo"><img src="https://img.shields.io/github/stars/liuyike422/CodingHomo.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information and Communication Engineering, University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: CA-unsup, Samples: 446200, Modality: Image pairs + motion vectors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Action Anticipation from SoccerNet Football Video Broadcasts</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.12021"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MohamadDalal/FAANTRA"><img src="https://img.shields.io/github/stars/MohamadDalal/FAANTRA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Aalborg University<br>
‚Ä¢ Dataset: SoccerNet Ball Action Anticipation dataset, Samples: 12433, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>RESPLE: Recursive Spline Estimation for LiDAR-Based Odometry</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.11580"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ASIG-X/RESPLE"><img src="https://img.shields.io/github/stars/ASIG-X/RESPLE.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Link√∂ping University, Sweden<br>
‚Ä¢ Dataset: HelmDyn, Samples: 10, Modality: LiDAR, IMU, MoCap<br>
‚Ä¢ Dataset: R-Campus, Samples: 1, Modality: LiDAR, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.11521"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: INTER DRIVE, Samples: 150000, Modality: Agent trajectories + natural language annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.11326"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pvuw.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Big Data, Fudan University<br>
‚Ä¢ Dataset: MOSE (extended test set), Samples: None, Modality: RGB videos + segmentation masks<br>
‚Ä¢ Dataset: MeViS (extended test set), Samples: None, Modality: RGB videos + language expressions + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.10905"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: InterHF, Samples: 90000, Modality: annotated video clips<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>SeeTree -- A modular, open-source system for tree detection and orchard localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.10764"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Jostan86/pf_orchard_localization"><img src="https://img.shields.io/github/stars/Jostan86/pf_orchard_localization.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Collaborative Robotics and Intelligent Systems (CoRIS) Institute, Oregon State University<br>
‚Ä¢ Dataset: SeeTree Dataset, Samples: 55, Modality: IMU, GNSS, RGB-D, wheel odometry<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>HUMOTO: A 4D Dataset of Mocap Human Object Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.10414"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jiaxin-lu.github.io/humoto/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Texas at Austin, Adobe Research<br>
‚Ä¢ Dataset: HUMOTO, Samples: 736, Modality: MoCap (full-body and hands), RGB-D videos, 3D object models, text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>RGB-Event based Pedestrian Attribute Recognition: A Benchmark Dataset and An Asymmetric RWKV Fusion Framework</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.10018"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/OpenPAR"><img src="https://img.shields.io/github/stars/Event-AHU/OpenPAR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: EventPAR, Samples: 100000, Modality: RGB frames and event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-wave Point Cloud Sequence</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.09862"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University, Bytedance Research<br>
‚Ä¢ Dataset: virtual radar-text dataset, Samples: 13308, Modality: mmWave radar point cloud sequence + text<br>
‚Ä¢ Dataset: real test dataset, Samples: 375, Modality: mmWave radar point cloud sequence + text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>CamMimic: Zero-Shot Image To Camera Motion Personalized Video Generation Using Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.09472"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cammimic.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland College Park<br>
‚Ä¢ Dataset: None, Samples: 680, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Low-Light Image Enhancement using Event-Based Illumination Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.09379"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: INSAIT, Sofia University<br>
‚Ä¢ Dataset: EvLowLight, Samples: 60, Modality: RGB images + temporal-mapping events + motion events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.09129"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: dConstruct Robotics<br>
‚Ä¢ Dataset: Multi-camera SLAM Dataset, Samples: 4, Modality: RGB images, IMU, Lidar, Pose Trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.08222"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/F3Set/F3Set"><img src="https://img.shields.io/github/stars/F3Set/F3Set.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ningbo University<br>
‚Ä¢ Dataset: F3Set (tennis), Samples: 11584, Modality: RGB videos + event annotations<br>
‚Ä¢ Dataset: F3Set (badminton), Samples: 112, Modality: RGB videos + event annotations<br>
‚Ä¢ Dataset: F3Set (table tennis), Samples: 42, Modality: RGB videos + event annotations<br>
‚Ä¢ Dataset: F3Set (tennis doubles), Samples: 78, Modality: RGB videos + event annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>RealCam-Vid: High-resolution Video Dataset with Dynamic Scenes and Metric-scale Camera Movements</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.08212"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZGCTroy/RealCam-Vid"><img src="https://img.shields.io/github/stars/ZGCTroy/RealCam-Vid.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science Zhejiang University<br>
‚Ä¢ Dataset: RealCam-Vid, Samples: , Modality: High-resolution videos + metric-scale camera parameters + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Multi-person Physics-based Pose Estimation for Combat Sports</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.08175"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ¬¥Ecole de technologie sup ¬¥erieure, Montreal, Canada<br>
‚Ä¢ Dataset: Elite Boxing Footage, Samples: Over 20 minutes of video footage, Modality: Multi-view RGB videos<br>
‚Ä¢ Dataset: Supplementary Dataset, Samples: None, Modality: Multi-view RGB videos + MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Towards Unconstrained 2D Pose Estimation of the Human Spine</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.08110"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://saifkhichi96.github.io/research/spinepose/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: German Research Center for Artificial Intelligence (DFKI)<br>
‚Ä¢ Dataset: SpineTrack, Samples: 58766, Modality: RGB images + 2D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>FMNV: A Dataset of Media-Published News Videos for Fake News Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.07687"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DennisIW/FMNV"><img src="https://img.shields.io/github/stars/DennisIW/FMNV.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the paper<br>
‚Ä¢ Dataset: FMNV, Samples: 2393, Modality: RGB videos, audio, text titles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>S2R-HDR: A Large-Scale Rendered Dataset for HDR Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.07667"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://openimaginglab.github.io/S2R-HDR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory<br>
‚Ä¢ Dataset: S2R-HDR, Samples: 1000, Modality: HDR image sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.07375"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/IRMVLab/MMTwin"><img src="https://img.shields.io/github/stars/IRMVLab/MMTwin.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IRMV Lab, the Department of Automation, Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: Self-recorded HTP benchmark (unnamed in paper), Samples: 1200, Modality: egocentric RGB videos + point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.07083"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://kszpxxzmc.github.io/GenDoP/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: DataDoP, Samples: 29000, Modality: camera trajectories, RGBD images, textual captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Uncertainty-Aware Hybrid Machine Learning in Virtual Sensors for Vehicle Sideslip Angle Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.06105"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MB-Team-THI/UAHL-RevStED"><img src="https://img.shields.io/github/stars/MB-Team-THI/UAHL-RevStED.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CARISSMA Institute of Automated Driving, Technische Hochschule Ingolstadt, Germany<br>
‚Ä¢ Dataset: ReV-StED (Real-world Vehicle State Estimation Dataset), Samples: 900000, Modality: Vehicle dynamics sensor data from onboard sensors (CAN), GNSS/Inertial System, and optical speed sensor.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>UVG-VPC: Voxelized Point Cloud Dataset for Visual Volumetric Video-based Coding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.05888"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ultravideo.fi/UVG-VPC/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ultra Video Group, Tampere University, Tampere, Finland<br>
‚Ä¢ Dataset: UVG-VPC, Samples: 12, Modality: Voxelized point cloud sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Event-based Civil Infrastructure Visual Defect Detection: ev-CIVIL Dataset and Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.05679"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gwgknudayanga/evCIVIL"><img src="https://img.shields.io/github/stars/gwgknudayanga/evCIVIL.svg?style=social&label=Star"></a><br><a href="https://figshare.com/s/825aec2714266fa40d29"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Photonics Engineering, Technical University of Denmark, Denmark<br>
‚Ä¢ Dataset: ev-CIVIL, Samples: 680, Modality: DVS event streams + Grayscale frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>From Sparse Signal to Smooth Motion: Real-Time Motion Generation with Rolling Prediction Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.05265"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/barquerogerman/RPM"><img src="https://img.shields.io/github/stars/barquerogerman/RPM.svg?style=social&label=Star"></a><br><a href="https://barquerogerman.github.io/RPM/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta Reality Labs, Universitat de Barcelona, Computer Vision Center<br>
‚Ä¢ Dataset: GORP, Samples: >14 hours from 28 participants, Modality: VR tracking signals (head, hands/controllers) + MoCap ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.05046"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nju-cite-mocaphumanoid.github.io/MotionPRO/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Science and Engineering, Nanjing University, Nanjing, China<br>
‚Ä¢ Dataset: MotionPRO, Samples: 729, Modality: Pressure data, RGB videos, Optical MoCap data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Inter-event Interval Microscopy for Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.04924"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory for Multimedia Information Processing, Peking University, Beijing 100871, China<br>
‚Ä¢ Dataset: IEIMat, Samples: None, Modality: Event streams and fluorescence microscopy images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>OmniCam: Unified Multimodal Video Generation via Camera Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.02312"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: OmniTr, Samples: 1000, Modality: RGB videos, camera trajectories, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Beyond Static Scenes: Camera-controllable Background Generation for Human Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.02004"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yaomingshuai.github.io/Beyond-Static-Scenes.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Harbin Institute of Technology, Harbin, China<br>
‚Ä¢ Dataset: DynaScene, Samples: 200000, Modality: RGB videos + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Dynamic Initialization for LiDAR-inertial SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.01451"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lian-yue0515/D-LI-Init"><img src="https://img.shields.io/github/stars/lian-yue0515/D-LI-Init.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: school of Mechanical Engineering, Shandong University, Jinan 250061, China and Key Laboratory of High Efficiency and Clean Mechanical Manufacture, Ministry of Education, Jinan 250061, China<br>
‚Ä¢ Dataset: D-LI-Init test datasets, Samples: 49, Modality: LiDAR, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.00438"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LannnSun/a-real-life-flexiwear-bodynet-dataset"><img src="https://img.shields.io/github/stars/LannnSun/a-real-life-flexiwear-bodynet-dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Key Laboratory of Navigation and Location-based Services, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, 200240<br>
‚Ä¢ Dataset: a real-life flexiwear-bodynet-dataset, Samples: 429, Modality: IMU data from smartphone, smartwatch, and headphones; Ground truth trajectories from VIO<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.24379"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sqwu.top/Any2Cap/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kuaishou Technology, National University of Singapore<br>
‚Ä¢ Dataset: Any2CapIns, Samples: 44644, Modality: RGB videos + human pose sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR) Challenge</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.24306"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/athaddius/STIRMetrics"><img src="https://img.shields.io/github/stars/athaddius/STIRMetrics.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/14803158"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Intuitive Surgical Inc.<br>
‚Ä¢ Dataset: STIR Challenge 2024 dataset (STIRC2024), Samples: 60, Modality: Stereo RGB videos + point trajectories from IR tattoos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.24026"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://humandreamer.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: GigaAI<br>
‚Ä¢ Dataset: MotionVid, Samples: 1270321, Modality: text + 2D pose sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Can Visuo-motor Policies Benefit from Random Exploration Data? A Case Study on Stacking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.23571"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cloudgripper.org"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: CloudGripper-Stack-750, Samples: 12400, Modality: RGB videos + proprioception states + actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>FRAME: Floor-aligned Representation for Avatar Motion from Egocentric Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.23094"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="vcai.mpi-inf.mpg.de/projects/FRAME"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: FRAME, Samples: 1600000, Modality: Stereo fisheye video, 6D head pose, Ground truth 3D joint positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>SocialGen: Modeling Multi-Human Social Interaction with Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.22906"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://socialgenx.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: SocialX, Samples: >40K, Modality: XH3D (SMPL-compatible pose data) + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.22397"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vadeli.github.io/GAITGen/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto, Computer Science Department, Vector Institute, KITE Research Institute, UHN<br>
‚Ä¢ Dataset: PD-GaM, Samples: 1701, Modality: 3D mesh dataset (SMPL parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.22394"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/Endo-TTAP-36E5"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Mechatronics and Engineering, Shenzhen University, Shenzhen, China<br>
‚Ä¢ Dataset: Endo-TAPC5, Samples: 40, Modality: RGB videos + point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.21860"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://maniptrans.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of General Artificial Intelligence, BIGAI<br>
‚Ä¢ Dataset: DEXMANIP NET, Samples: 3.3K episodes, Modality: simulated robotic manipulation trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.21268"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.lidarhumanmotion.net/climbingcap/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University; National Institute for Data Science in Health and Medicine, Xiamen University; Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University<br>
‚Ä¢ Dataset: AscendMotion, Samples: 412000, Modality: RGB, LiDAR, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>SpikeDerain: Unveiling Clear Videos from Rainy Sequences Using Color Spike Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.20315"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Wuhan University of Technology<br>
‚Ä¢ Dataset: RAIN100C, Samples: 100, Modality: RGB videos + color spike streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EGVD: Event-Guided Video Diffusion Model for Physically Realistic Large-Motion Frame Interpolation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.20268"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OpenImagingLab/EGVD"><img src="https://img.shields.io/github/stars/OpenImagingLab/EGVD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University, Shanghai AI Laboratory<br>
‚Ä¢ Dataset: DJI 30fps, Samples: 191 sequences, Modality: RGB videos + simulated event data<br>
‚Ä¢ Dataset: Comprehensive Training Dataset (Prophesee, BS-ERGB, DJI 30fps, GOPRO 240fps), Samples: 400 sequences, Modality: RGB videos + real/simulated event data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EBS-EKF: Accurate and High Frequency Event-based Star Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.20101"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.kitware.com/nest-public/kwebsstartracking"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kitware<br>
‚Ä¢ Dataset: EBS-EKF Star Tracking Dataset, Samples: 14, Modality: Event streams + 3D rotational trajectories (quaternions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.19913"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/c7w/PartRM"><img src="https://img.shields.io/github/stars/c7w/PartRM.svg?style=social&label=Star"></a><br><a href="https://PartRM.c7w.tech/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: PartDrag-4D, Samples: 20548, Modality: Multi-view images, animated meshes, point clouds, and 2D drag vectors for articulated objects<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.19355"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ikodoh.github.io/ST-VLM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea University<br>
‚Ä¢ Dataset: STKit, Samples: 116000, Modality: RGB videos with kinematic instruction tuning data (QA pairs)<br>
‚Ä¢ Dataset: STKit-Bench, Samples: 1400, Modality: RGB videos with kinematic instruction tuning data (QA pairs)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>TrackRAD2025 challenge dataset: Real-time tumor tracking for MRI-guided radiotherapy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.19119"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LMUK-RADONC-PHYS-RES/trackrad2025/"><img src="https://img.shields.io/github/stars/trackrad2025/.svg?style=social&label=Star"></a><br><a href="https://trackrad2025.grand-challenge.org/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Radiation Oncology, Radiation Oncology Key Laboratory of Sichuan Province, Sichuan Clinical Research Center for Cancer, Sichuan Cancer Hospital & Institute, Sichuan Cancer Center, University of Electronic Science and Technology of China, Chengdu, China<br>
‚Ä¢ Dataset: TrackRAD2025, Samples: 585, Modality: 2D cine MRIs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Target-Aware Video Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18950"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://taeksuu.github.io/tavid/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Seoul National University<br>
‚Ä¢ Dataset: unnamed, Samples: 1290, Modality: RGB videos + segmentation masks + text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Predicting the Road Ahead: A Knowledge Graph based Foundation Model for Scene Understanding in Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18730"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/boschresearch/fm4su"><img src="https://img.shields.io/github/stars/boschresearch/fm4su.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Bosch Corporate Research, Renningen, Germany, University of Stuttgart, Stuttgart, Germany<br>
‚Ä¢ Dataset: BEV symbolic scene representation dataset from nuScenesKG, Samples: 30000, Modality: Symbolic Bird's-Eye-View (BEV) representation from Knowledge Graph<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18552"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://potentialming.github.io/projects/EvAnimate"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Sydney<br>
‚Ä¢ Dataset: EvTikTok, Samples: 350, Modality: simulated event streams<br>
‚Ä¢ Dataset: EvHumanMotion, Samples: 113, Modality: RGB videos + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Human-Object Interaction via Automatically Designed VLM-Guided Motion Policy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18349"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vlm-rmd.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Interplay, Samples: 1210, Modality: Interaction plans, 3D scene layouts, text instructions, and top-view images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>AGIR: Assessing 3D Gait Impairment with Reasoning based on LLMs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18141"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/w/AGIR-7BF7/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ICube laboratory, University of Strasbourg, CNRS, France<br>
‚Ä¢ Dataset: Enhanced Parkinson's Disease (PD) gait dataset, Samples: 883, Modality: 3D joint data + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>TransAnimate: Taming Layer Diffusion to Generate RGBA Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17934"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Clemson University<br>
‚Ä¢ Dataset: Animate Dataset, Samples: 3000, Modality: RGBA videos<br>
‚Ä¢ Dataset: Foreground Object Videos Dataset, Samples: 7000, Modality: RGBA videos<br>
‚Ä¢ Dataset: Synthesized Transparent Motion Videos, Samples: 20000, Modality: RGBA videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Event-Based Crossing Dataset (EBCD)</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17499"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/joeduman/Thresholded event-based-crossing-dataset"><img src="https://img.shields.io/github/stars/joeduman/Thresholded event-based-crossing-dataset.svg?style=social&label=Star"></a><br><a href="https://ieee-dataport.org/documents/event-based-crossing-dataset-ebcd"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UMBC<br>
‚Ä¢ Dataset: Event-Based Crossing Dataset (EBCD), Samples: 33, Modality: Event-based images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17358"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Oxford, Department of Computer Science<br>
‚Ä¢ Dataset: Synthetic Motion-Blurred Image Dataset (from ScanNet++v2), Samples: 121200, Modality: Synthesized motion-blurred RGB images, depth maps, optical flow fields, camera poses<br>
‚Ä¢ Dataset: Real-world Motion-Blurred Image Dataset, Samples: 10000, Modality: Real-world motion-blurred RGB images, ARKit poses, IMU measurements<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17132"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: FallingDetection-CeleX, Samples: 875, Modality: Event-camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>ColabSfM: Collaborative Structure-from-Motion by Point Cloud Registration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17093"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/EricssonResearch/ColabSfM"><img src="https://img.shields.io/github/stars/EricssonResearch/ColabSfM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Link√∂ping University<br>
‚Ä¢ Dataset: ColabSfM SfM Registration Dataset, Samples: 22000, Modality: Pairs of 3D SfM point clouds with normals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Pedestrians and Robots: A Novel Dataset for Learning Distinct Social Navigation Forces</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.16481"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Humanoid Robots Lab, University of Bonn, Germany<br>
‚Ä¢ Dataset: robot-pedestrian influence (RPI) dataset, Samples: 18669, Modality: 2D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.16421"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://quanhaol.github.io/magicmotion-site/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: MagicData, Samples: 51000, Modality: RGB videos with text and trajectory (mask, bounding box) annotations<br>
‚Ä¢ Dataset: MagicBench, Samples: 600, Modality: RGB videos with trajectory (mask, bounding box) annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>PoseTraj: Pose-Aware Trajectory Control in Video Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.16068"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://robingg1.github.io/Pose-Traj/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Edinburgh<br>
‚Ä¢ Dataset: PoseTraj-10K, Samples: 10000, Modality: RGB videos + trajectories + 3D bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.15835"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vulab-ai.github.io/BARD-GS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Case Western Reserve University<br>
‚Ä¢ Dataset: Real-world Captured Blurry Dataset, Samples: 12, Modality: Paired blurry (24 FPS) and sharp (240 FPS) RGB videos of dynamic scenes from synchronized cameras<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Controlling Avatar Diffusion with Learnable Gaussian Embedding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.15809"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ustc3dv.github.io/Learn2Control/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: Synthetic Head Dataset, Samples: 10000, Modality: Synthetic images + FLAME parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14935"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://favor-bench.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: FAVOR-Bench, Samples: 1776, Modality: RGB videos<br>
‚Ä¢ Dataset: FAVOR-Train, Samples: 17152, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14919"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: Unified and Extended Motion Dataset, Samples: 48251, Modality: MoCap joints + Text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Matching Skeleton-based Activity Representations with Heterogeneous Signals for HAR</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14547"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Shuheng-Li/SKELAR"><img src="https://img.shields.io/github/stars/Shuheng-Li/SKELAR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, San Diego<br>
‚Ä¢ Dataset: MASD (Multimodal Activity Sensing Dataset), Samples: 540, Modality: IMU, WiFi, skeleton<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14247"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/NSN-Hello/GeoFlow-SLAM"><img src="https://img.shields.io/github/stars/NSN-Hello/GeoFlow-SLAM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Horizon Robotics<br>
‚Ä¢ Dataset: Go2 D435i dataset, Samples: 4, Modality: RGB-D, IMU, LiDAR, Legged Odometry<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14229"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ha-vln-project.vercel.app/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Washington<br>
‚Ä¢ Dataset: HAPS 2.0, Samples: 486, Modality: SMPL mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>8-Calves Image dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.13777"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/tonyFang04/8-calves"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Bristol<br>
‚Ä¢ Dataset: 8-Calves dataset, Samples: 1, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Multi-Platform Teach-and-Repeat Navigation by Visual Place Recognition Based on Deep-Learned Local Features</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.13090"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://imr.ciirc.cvut.cz/Datasets/TaR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Czech Institute of Informatics, Robotics and Cybernetics, Czech technical University in Prague; Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague<br>
‚Ä¢ Dataset: TaR, Samples: 3, Modality: RGB images + pose transformations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Beyond Role-Based Surgical Domain Modeling: Generalizable Re-Identification in the Operating Room</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.13028"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wngTn/orreid"><img src="https://img.shields.io/github/stars/wngTn/orreid.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Chair for Computer Aided Medical Procedures, Technical University of Munich, Boltzmannstra√üe 3, 85748, Garching, Germany<br>
‚Ä¢ Dataset: ORReID13, Samples: 6358, Modality: 3D point clouds + multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>GIFT: Generated Indoor video frames for Texture-less point tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12944"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/GIFT-6D02/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southern University of Science and Technology<br>
‚Ä¢ Dataset: GIFT, Samples: 1800, Modality: RGB videos + point trajectories + optical flow + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>AUTV: Creating Underwater Video Datasets with Pixel-wise Annotations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12828"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: UTV, Samples: 2000, Modality: RGB videos + fine-grained text annotations (including motion/behavior)<br>
‚Ä¢ Dataset: SUTV, Samples: 10000, Modality: RGB videos + pixel-wise segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Stereo Event-based, 6-DOF Pose Tracking for Uncooperative Spacecraft</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12732"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Zibin6/SE6PT"><img src="https://img.shields.io/github/stars/Zibin6/SE6PT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Aerospace Science and Engineering, National University of Defense Technology<br>
‚Ä¢ Dataset: Stereo Event-based Uncooperative Spacecraft Motion Dataset, Samples: 17, Modality: Stereo event streams + 6-DOF pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12419"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/3190105222/EgoEv_Gesture"><img src="https://img.shields.io/github/stars/3190105222/EgoEv_Gesture.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang University, China<br>
‚Ä¢ Dataset: EgoEvGesture, Samples: 5419, Modality: event camera streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>M2UD: A Multi-model, Multi-scenario, Uneven-terrain Dataset for Ground Robot with Localization and Mapping Evaluation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12387"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yaepiii.github.io/M2UD/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Robotics at Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China<br>
‚Ä¢ Dataset: M2UD, Samples: 58, Modality: LiDAR, RGB-D Camera, IMU, GNSS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Bring Your Rear Cameras for Egocentric 3D Human Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.11652"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/EgoRear/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, SIC<br>
‚Ä¢ Dataset: Ego4View-Syn, Samples: 8372, Modality: synthetic RGB videos (fisheye) + SMPL parameters<br>
‚Ä¢ Dataset: Ego4View-RW, Samples: 478, Modality: real-world RGB videos (fisheye) + MoCap joints + SMPL parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EMoTive: Event-guided Trajectory Modeling for 3D Motion Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.11371"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: USTC<br>
‚Ä¢ Dataset: CarlaEvent3D, Samples: 22125, Modality: Events + Images + 3D Motion Labels (Optical Flow, Motion in Depth, Scene Flow)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Enhancing Hand Palm Motion Gesture Recognition by Eliminating Reference Frame Bias via Frame-Invariant Similarity Measures</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.11352"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.15020057"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering and Flanders Make at KU Leuven<br>
‚Ä¢ Dataset: Hand Palm Motion (HPM) dataset, Samples: 420, Modality: Motion capture trajectories (3D position + quaternion)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>6D Object Pose Tracking in Internet Videos for Robotic Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.10307"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague<br>
‚Ä¢ Dataset: New dataset of instructional videos, Samples: 32, Modality: RGB videos + 6D object pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>RMG: Real-Time Expressive Motion Generation with Self-collision Avoidance for 6-DOF Companion Robotic Arms</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.09959"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: robotic arm expressive motion dataset, Samples: over 10,000, Modality: Robotic arm joint trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Unified Dense Prediction of Video Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.09344"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Virginia<br>
‚Ä¢ Dataset: Panda-Dense, Samples: 300000, Modality: RGB videos + entity segmentation + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>SwapAnyone: Consistent and Realistic Video Synthesis for Swapping Any Person into Any Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.09154"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/PKU-YuanGroup/SwapAnyone"><img src="https://img.shields.io/github/stars/PKU-YuanGroup/SwapAnyone.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: HumanAction-32K, Samples: 32000, Modality: RGB videos + pose keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Ev-Layout: A Large-scale Event-based Multi-modal Dataset for Indoor Layout Estimation and Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.08370"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software, Shandong University, China<br>
‚Ä¢ Dataset: Ev-Layout, Samples: 2500, Modality: RGB images, event streams, IMU data, illuminance sensor data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HERO: Human Reaction Generation from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.08270"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JackYu6/HERO"><img src="https://img.shields.io/github/stars/JackYu6/HERO.svg?style=social&label=Star"></a><br><a href="https://jackyu6.github.io/HERO"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: ViMo, Samples: 3500, Modality: RGB videos + 3D human motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Depth-Assisted Network for Indiscernible Marine Object Counting with Adaptive Motion-Differentiated Feature Encoding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.08152"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OUCVisionGroup/VIMOC-Net"><img src="https://img.shields.io/github/stars/OUCVisionGroup/VIMOC-Net.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Engineering, Ocean University of China<br>
‚Ä¢ Dataset: VIMOC Dataset, Samples: 50, Modality: RGB videos + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>AG-VPReID: A Challenging Large-Scale Benchmark for Aerial-Ground Video-based Person Re-Identification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.08121"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Vi-Sion/AG-VPReID-Net"><img src="https://img.shields.io/github/stars/Vi-Sion/AG-VPReID-Net.svg?style=social&label=Star"></a><br><a href="https://github.com/Vi-Sion/AG-VPReID-Net"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical Engineering and Robotics, Queensland University of Technology<br>
‚Ä¢ Dataset: AG-VPReID, Samples: 32321, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HumanMM: Global Human Motion Recovery from Multi-shot Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07597"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zhangyuhong01.github.io/HumanMM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University, IDEA Research<br>
‚Ä¢ Dataset: ms-Motion, Samples: 600, Modality: RGB videos + 3D human motion (SMPL) + camera parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>AthletePose3D: A Benchmark Dataset for 3D Human Pose Estimation and Kinematic Validation in Athletic Movements</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07499"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/calvinyeungck/AthletePose3D"><img src="https://img.shields.io/github/stars/calvinyeungck/AthletePose3D.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Graduate School of Informatics, Nagoya University, Nagoya, Japan<br>
‚Ä¢ Dataset: AthletePose3D, Samples: 165000, Modality: RGB videos + 3D poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>PersonaBooth: Personalized Text-to-Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07390"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://boeun-kim.github.io/page-PersonaBooth"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Birmingham, Korea Electronics Technology Institute, Dankook University<br>
‚Ä¢ Dataset: PerMo, Samples: 6,610 clips, Modality: MoCap (optical markers, skeleton, SMPL-H mesh), text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs and Chain-of-Thought Prompting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07234"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau<br>
‚Ä¢ Dataset: Highway-Text, Samples: 6606, Modality: Text descriptions of traffic scenarios<br>
‚Ä¢ Dataset: Urban-Text, Samples: 5431, Modality: Text descriptions of traffic scenarios<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>YOLOMG: Vision-based Drone-to-Drone Detection with Appearance and Pixel-Level Motion Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07115"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Irisky123/YOLOMG"><img src="https://img.shields.io/github/stars/Irisky123/YOLOMG.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Artificial Intelligence, Westlake University, Hangzhou, China<br>
‚Ä¢ Dataset: ARD100, Samples: 100, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07020"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University at Buffalo, SUNY<br>
‚Ä¢ Dataset: DriveLM-Deficit, Samples: 53895, Modality: RGB video clips<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HybridReg: Robust 3D Point Cloud Registration with Hybrid Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07019"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hxwork/HybridReg"><img src="https://img.shields.io/github/stars/hxwork/HybridReg.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: HybridMatch, Samples: 50600, Modality: Point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Motion Anything: Any to Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.06955"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://steve-zeyu-zhang.github.io/MotionAnything"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ANU<br>
‚Ä¢ Dataset: Text-Music-Dance (TMD), Samples: 2153, Modality: text, music, and dance motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Sign Language Translation using Frame and Event Stream: Benchmark Dataset and Algorithms</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.06484"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/OpenESL"><img src="https://img.shields.io/github/stars/Event-AHU/OpenESL.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: VECSL, Samples: 15676, Modality: RGB frames + Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.06053"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dropletx.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IEIT System Co., Ltd.<br>
‚Ä¢ Dataset: DropletVideo-10M, Samples: 10000000, Modality: RGB videos + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>A Helping (Human) Hand in Kinematic Structure Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.05301"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics and Biology Laboratory, Technische Universit√§t Berlin; Science of Intelligence, Research Cluster of Excellence, Berlin<br>
‚Ä¢ Dataset: unnamed, Samples: 30, Modality: RGB-D videos + MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.05231"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="Search for 'Kaiwu' on ScienceDB"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the text<br>
‚Ä¢ Dataset: Kaiwu, Samples: 11664, Modality: Motion Capture (3D skeleton ground truth), Multi-view RGB-D videos, Audio, IMU, EMG, Eye Gaze (first-person video), Hand Pose (data glove), Tactile/Force (data glove)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Combined Physics and Event Camera Simulator for Slip Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tub-rip/event-slip"><img src="https://img.shields.io/github/stars/tub-rip/event-slip.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit ¬®at Berlin, and Robotics Institute Germany<br>
‚Ä¢ Dataset: Simple Set, Samples: 192, Modality: Event camera data, RGB frames, Object/Gripper orientations (quaternions)<br>
‚Ä¢ Dataset: Complex Set, Samples: 1200, Modality: Event camera data, RGB frames, Object/Gripper orientations (quaternions)<br>
‚Ä¢ Dataset: Real Set, Samples: 5, Modality: Event camera data, RGB frames, Robot kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>What Are You Doing? A Closer Look at Controllable Human Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04666"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/google-deepmind/wyd-benchmark"><img src="https://img.shields.io/github/stars/google-deepmind/wyd-benchmark.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google DeepMind<br>
‚Ä¢ Dataset: What Are You Doing? (WYD), Samples: 1544, Modality: RGB videos + captions + video segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>3HANDS Dataset: Learning from Humans for Generating Naturalistic Handovers with Supernumerary Robotic Limbs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04635"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hci.cs.uni-saarland.de/projects/3hands/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Saarland University<br>
‚Ä¢ Dataset: 3HANDS, Samples: 946, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Omnidirectional Multi-Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04565"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xifen523/OmniTrack"><img src="https://img.shields.io/github/stars/xifen523/OmniTrack.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hunan University<br>
‚Ä¢ Dataset: QuadTrack, Samples: 32, Modality: panoramic image sequences + 2D bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04257"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Seoul National University<br>
‚Ä¢ Dataset: Truebones Zoo dataset (annotated), Samples: 1000+, Modality: skeletal motion + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Supervised Visual Docking Network for Unmanned Surface Vehicles Using Auto-labeling in Real-world Water Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.03282"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/usv-docking/home"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information Science and Engineering, Hebei University of Science and Technology, 26 Yuxiang Street, Yuhua District, Shijiazhuang, Heibei Province, 050018, P.R. China<br>
‚Ä¢ Dataset: USV Visual Docking Dataset, Samples: 2000, Modality: Fisheye images + relative pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Monocular Person Localization under Camera Ego-motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.02916"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MEDLAR-T-Rex/rpf_quadruped"><img src="https://img.shields.io/github/stars/MEDLAR-T-Rex/rpf_quadruped.svg?style=social&label=Star"></a><br><a href="https://medlartea.github.io/rpf-quadruped/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Key Laboratory of Robotics and Computer Vision, Southern University of Science and Technology (SUSTech), and the Department of Electronic and Electrical Engineering, SUSTech.<br>
‚Ä¢ Dataset: Our Dataset, Samples: None, Modality: RGB videos (pin-hole, fisheye, equirectangular), UWB distance measurements, motion capture poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.02572"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://racevla.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Intelligent Space Robotics Laboratory, Skolkovo Institute of Science and Technology<br>
‚Ä¢ Dataset: RaceVLA dataset, Samples: 200, Modality: Vicon MoCap (position, velocity, yaw), RGB images, natural language instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>BdSLW401: Transformer-Based Word-Level Bangla Sign Language Recognition Using Relative Quantization Encoding (RQE)</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.02360"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the paper<br>
‚Ä¢ Dataset: BdSLW401, Samples: 102176, Modality: Pose landmarks (MediaPipe)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>One-Step Event-Driven High-Speed Autofocus</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.01214"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang University, China<br>
‚Ä¢ Dataset: PSF-based Focus Event Synthetic Dataset, Samples: 84, Modality: Synthetic focus event stacks with simulated motion<br>
‚Ä¢ Dataset: DAVIS346 Autofocus Dataset, Samples: 28, Modality: Time-synchronized event streams and grayscale images<br>
‚Ä¢ Dataset: Prophesee EVK4 Autofocus Dataset, Samples: 28, Modality: Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HiMo: High-Speed Objects Motion Compensation in Point Clouds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.00803"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/KTH-RPL/HiMo"><img src="https://img.shields.io/github/stars/KTH-RPL/HiMo.svg?style=social&label=Star"></a><br><a href="https://kin-zhang.github.io/HiMo"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Division of Robotics, Perception, and Learning (RPL), KTH Royal Institute of Technology; Autonomous Transport Solutions Lab, Scania Group<br>
‚Ä¢ Dataset: Scania, Samples: 500 sequences, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.00495"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://xuanchenli.github.io/TexTalk/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: TexTalk4D, Samples: 100, Modality: audio-synced 3D mesh sequences with 8K dynamic textures<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>High Dynamic Range Video Compression: A Large-Scale Benchmark Dataset and A Learned Bit-depth Scalable Compression Algorithm</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.00410"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sdkinda/HDR-Learned-Video-Coding"><img src="https://img.shields.io/github/stars/sdkinda/HDR-Learned-Video-Coding.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shangha University<br>
‚Ä¢ Dataset: HDRVD2K, Samples: 2200, Modality: HDR videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.00389"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Keio University<br>
‚Ä¢ Dataset: AMPL (Acoustic Music-based PoseLearning dataset), Samples: None, Modality: Mocap joints + Acoustic signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Towards long-term player tracking with graph hierarchies and domain-specific features</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.21242"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mkoshkina/sports-SUSHI"><img src="https://img.shields.io/github/stars/mkoshkina/sports-SUSHI.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: York University<br>
‚Ä¢ Dataset: Hockey Tracking Dataset, Samples: 20, Modality: RGB videos with MOT annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>egoPPG: Heart Rate Estimation from Eye-Tracking Cameras in Egocentric Systems to Benefit Downstream Vision Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.20879"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, ETH Zurich<br>
‚Ä¢ Dataset: egoPPG-DB, Samples: 150, Modality: Eye-tracking videos + POV RGB videos + IMU + PPG + ECG<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>EyEar: Learning Audio Synchronized Human Gaze Trajectory Based on Physics-Informed Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.20858"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XiaochuanLiu-ruc/EyEar"><img src="https://img.shields.io/github/stars/XiaochuanLiu-ruc/EyEar.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Gaoling School of Artifcial Intelligence, Renmin University of China, Beijing, China<br>
‚Ä¢ Dataset: EyEar-20k, Samples: 20000, Modality: Gaze trajectories + images + synchronized audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.20824"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Texas at Austin<br>
‚Ä¢ Dataset: Handheld Burst Motion Dataset, Samples: 102, Modality: Homography matrices from multi-frame captures<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>FuseGrasp: Radar-Camera Fusion for Robotic Grasping of Transparent Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.20037"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Information Engineering, The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: self-built RGB-D-Radar transparent object dataset, Samples: 600, Modality: RGB images + depth images + mmWave radar images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>C-Drag: Chain-of-Thought Driven Motion Controller for Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.19868"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WesLee88524/C-Drag-Official-Repo"><img src="https://img.shields.io/github/stars/WesLee88524/C-Drag-Official-Repo.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northwestern Polytechnical University<br>
‚Ä¢ Dataset: VOI, Samples: 72, Modality: RGB videos with bounding box and trajectory annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>TransVDM: Motion-Constrained Video Diffusion Model for Transparent Video Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.19454"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Alibaba Group<br>
‚Ä¢ Dataset: None, Samples: 10000, Modality: RGB-alpha video clips + bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.18373"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://siplab.org/projects/EgoSim"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich<br>
‚Ä¢ Dataset: MultiEgoView, Samples: 119.4 hours (synthetic) + 5 hours (real), Modality: RGB videos (from 6 body-worn cameras), ground-truth 3D body poses, activity annotations, simulated IMU data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Impact of Object Weight in Handovers: Inspiring Robotic Grip Release and Motion from Human Handovers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.17834"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/paragkhanna1/RPL Khanna Human Handover Datasets"><img src="https://img.shields.io/github/stars/paragkhanna1/RPL Khanna Human Handover Datasets.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Division of Robotics, Perception and Learning (RPL), EECS, KTH Royal Institute of Technology, Sweden<br>
‚Ä¢ Dataset: Handovers@RPL-2.0, Samples: 3235, Modality: MoCap joints, Forces<br>
‚Ä¢ Dataset: YCB-Handovers, Samples: 2771, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>V-HOP: Visuo-Haptic 6D Object Pose Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.17434"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lhy.xyz/projects/v-hop/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: V-HOP Multi-embodied Dataset, Samples: 1550000, Modality: RGB-D videos, robot kinematics (joint positions), tactile sensor data, 6D object pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>A dataset of high-resolution plantar pressures for gait analysis across varying footwear and walking speeds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.17244"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UNB-StepUP/StepUP-P150"><img src="https://img.shields.io/github/stars/UNB-StepUP/StepUP-P150.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.20383/103.01285"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of New Brunswick, Institute of Biomedical Engineering<br>
‚Ä¢ Dataset: UNB StepUP-P150, Samples: over 200,000 footsteps, Modality: Plantar pressure<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>DeProPose: Deficiency-Proof 3D Human Pose Estimation via Adaptive Multi-View Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.16419"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WUJINHUAN/DeProPose"><img src="https://img.shields.io/github/stars/WUJINHUAN/DeProPose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence, Xidian University, China<br>
‚Ä¢ Dataset: Deficiency-Aware 3D Pose Estimation (DA-3DPE) dataset, Samples: 575689, Modality: multi-view RGB videos + 3D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.14917"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not specified in text<br>
‚Ä¢ Dataset: VQA driving instruction dataset, Samples: None, Modality: multi-view scene videos, BEV map images, QA annotations, vehicle motion trajectories, and control signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.14795"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Milab, Westlake University<br>
‚Ä¢ Dataset: Humanoid-VLA motion-language interleaved dataset, Samples: 929000 clips, Modality: motion sequences + text annotations<br>
‚Ä¢ Dataset: Humanoid-S, Samples: 4646 video clips, Modality: human pose + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Exploiting Deblurring Networks for Radiance Fields</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.14454"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KT<br>
‚Ä¢ Dataset: BlurRF-Synth, Samples: 150, Modality: Synthesized RGB images (motion & defocus blurred)<br>
‚Ä¢ Dataset: BlurRF-Real, Samples: 5, Modality: Real-world RGB images (motion blurred)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.14004"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen University<br>
‚Ä¢ Dataset: Inter3D, Samples: 4, Modality: RGB images + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.13859"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: MSVCOD, Samples: 162, Modality: RGB videos with pixel-level annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Event-Based Video Frame Interpolation With Cross-Modal Asymmetric Bidirectional Motion Fields</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.13716"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/intelpro/CBMNet"><img src="https://img.shields.io/github/stars/intelpro/CBMNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea Advanced Institute of Science and Technology<br>
‚Ä¢ Dataset: ERF-X170FPS, Samples: 140, Modality: RGB videos + Event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>MoVer: Motion Verification for Motion Graphics Animations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.13372"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mover-dsl.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University, USA<br>
‚Ä¢ Dataset: MoVer Test Dataset, Samples: 5600, Modality: Text prompts paired with ground truth MoVer verification programs for 2D SVG motion graphics animations.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>iMOVE: Instance-Motion-Aware Video Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.11594"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kuaishou Technology, Zhejiang University<br>
‚Ä¢ Dataset: iMOVE-IT, Samples: 114000, Modality: RGB videos with bounding box motion trajectories and dynamic captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>AdaManip: Adaptive Articulated Object Manipulation Environments and Policy Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.11124"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://adamanip.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center on Frontiers of Computing Studies, School of Computer Science, Peking University<br>
‚Ä¢ Dataset: AdaManip Adaptive Demonstrations, Samples: 5540 simulated sequences, Modality: Robot end-effector pose trajectories, 3D point clouds, robot proprioception states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>E-3DGS: Event-Based Novel View Rendering of Large-Scale Scenes Using 3D Gaussian Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.10827"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/E3DGS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Saarland University, MPI for Informatics, SIC<br>
‚Ä¢ Dataset: E-3DGS-Real, Samples: None, Modality: color event stream, RGB images, camera poses<br>
‚Ä¢ Dataset: E-3DGS-Synthetic, Samples: 3, Modality: simulated color event stream, rendered RGB frames, camera poses<br>
‚Ä¢ Dataset: E-3DGS-Synthetic-Hard, Samples: 3, Modality: simulated color event stream, rendered RGB frames, noisy camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.09533"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University of Science and Technology<br>
‚Ä¢ Dataset: TalkingFace-Wild, Samples: 31300, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>A Deep Inverse-Mapping Model for a Flapping Robotic Wing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.09378"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Hadar933/AdaptiveSpectrumLayer"><img src="https://img.shields.io/github/stars/Hadar933/AdaptiveSpectrumLayer.svg?style=social&label=Star"></a><br><a href="https://www.beatus-lab.org"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Engineering, The Institute of Life Sciences, Center for Bioengineering, The Hebrew University of Jerusalem<br>
‚Ä¢ Dataset: None, Samples: 153, Modality: 3D wing kinematics (Euler angles) from stereo high-speed cameras, synchronized with aerodynamic force sensor data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>EventSTR: A Benchmark Dataset and Baselines for Event Stream based Scene Text Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.09020"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/EventSTR"><img src="https://img.shields.io/github/stars/Event-AHU/EventSTR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University<br>
‚Ä¢ Dataset: EventSTR, Samples: 9928, Modality: Event stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Measuring Anxiety Levels with Head Motion Patterns in Severe Depression Population</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.08813"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Univ. Lille, CNRS, Centrale Lille, Institut Mines-T ¬¥el¬¥ecom, UMR 9189 CRIStAL, F-59000 Lille, France; Univ. Lille, Inserm, CHU Lille, U1172 - LilNCog - Lille Neuroscience & Cognition, F-59000 Lille, France<br>
‚Ä¢ Dataset: CALYPSO Depression Dataset, Samples: 32, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.08639"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cinemaster-dev.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dalian University of Technology<br>
‚Ä¢ Dataset: Unnamed 3D box and camera pose video dataset, Samples: 156000, Modality: RGB videos + 3D bounding boxes + 3D camera trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.07869"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://eventego3d.mpi-inf.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Computing and Artificial Intelligence, Max Planck Institute for Informatics, SIC, Saarbr√ºcken, Germany; Augmented Vision, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany.<br>
‚Ä¢ Dataset: EE3D-R, Samples: 12 sequences, ~4.64e5 poses, Modality: egocentric event streams, 3D body joints, allocentric RGB streams, SMPL body parameters<br>
‚Ä¢ Dataset: EE3D-W, Samples: 9 sequences, ~4.18e5 poses, Modality: egocentric event streams, 3D body joints, allocentric RGB streams, SMPL body parameters<br>
‚Ä¢ Dataset: EE3D-S, Samples: 946 motion sequences, ~6.34e6 3D human poses, Modality: synthetic egocentric event streams, 3D body joints, SMPL body models<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.07531"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: Camera Motion Control Dataset, Samples: 62000, Modality: RGB videos + camera trajectories<br>
‚Ä¢ Dataset: Object Motion Control Dataset, Samples: 60000, Modality: RGB videos + dense/sparse object trajectories + optical flow<br>
‚Ä¢ Dataset: VideoLightingDirection (VLD) Dataset, Samples: 57600, Modality: Synthetic RGB videos + camera trajectories + lighting direction<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>CT-UIO: Continuous-Time UWB-Inertial-Odometer Localization Using Non-Uniform B-spline with Fewer Anchors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.06287"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JasonSun623/CT-UIO"><img src="https://img.shields.io/github/stars/JasonSun623/CT-UIO.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Engineering Research Center of Robot Visual Perception and Control Technology, Hunan University, Changsha 410012, China<br>
‚Ä¢ Dataset: Corridor Dataset, Samples: 6, Modality: UWB, IMU, Odometer, LIDAR<br>
‚Ä¢ Dataset: Exhibition Hall Dataset, Samples: 6, Modality: UWB, IMU, Odometer, LIDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.05979"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology, China<br>
‚Ä¢ Dataset: Open-VFX, Samples: 675, Modality: RGB videos + text prompts + instance segmentation masks + start/end timestamps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.04847"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://agnjason.github.io/HumanDiT-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University, ByteDance<br>
‚Ä¢ Dataset: Unnamed HumanDiT dataset, Samples: 4500000, Modality: RGB videos + body pose sequences + background keypoint sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>High-Speed Dynamic 3D Imaging with Sensor Fusion Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.04630"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of North Carolina, Chapel Hill, USA<br>
‚Ä¢ Dataset: Synthetic High-Speed Dynamic Scenes, Samples: 3, Modality: RGB videos + Depth maps + Events<br>
‚Ä¢ Dataset: Real-World High-Speed Dynamic Scenes, Samples: 3, Modality: RGB videos + Depth maps + Events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>HD-EPIC: A Highly-Detailed Egocentric Video Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.04144"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://hd-epic.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Uni. of Bristol<br>
‚Ä¢ Dataset: HD-EPIC, Samples: 19900, Modality: Egocentric RGB videos, 6DoF camera trajectories, 3D digital twins, 3D object/hand masks, gaze, audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.03639"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://snap-research.github.io/PointVidGen/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Los Angeles<br>
‚Ä¢ Dataset: PointVid, Samples: 70000, Modality: RGB videos + 3D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Swarm Characteristic Classification using Robust Neural Networks with Optimized Controllable Inputs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.03619"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DWPeltier3/Swarm-NN-TSC"><img src="https://img.shields.io/github/stars/DWPeltier3/Swarm-NN-TSC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Naval Postgraduate School<br>
‚Ä¢ Dataset: Combined ND, Samples: 72000, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
‚Ä¢ Dataset: Combined DM, Samples: 24000, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
‚Ä¢ Dataset: Combined Noise, Samples: 244800, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
‚Ä¢ Dataset: Combined DM+, Samples: 200000, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
‚Ä¢ Dataset: Combined ND & DM, Samples: 240000, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>SKI Models: Skeleton Induced Vision-Language Embeddings for Understanding Activities of Daily Living</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.03459"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/thearkaprava/SKI-Models"><img src="https://img.shields.io/github/stars/thearkaprava/SKI-Models.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of North Carolina at Charlotte<br>
‚Ä¢ Dataset: NTU120 video-instruction pairs, Samples: 100K question-answer pairs, Modality: RGB videos + text instruction pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal Structures from Multi-view Joint Cloud</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.02936"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China<br>
‚Ä¢ Dataset: BUMocap-X, Samples: 1 sequence (120 seconds), Modality: MoCap joints from multi-view video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Event-aided Semantic Scene Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.02334"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Pandapan01/EvSSC"><img src="https://img.shields.io/github/stars/Pandapan01/EvSSC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang University<br>
‚Ä¢ Dataset: DSEC-SSC, Samples: 12 sequences (3,488 frames), Modality: Event camera data + RGB images + LiDAR<br>
‚Ä¢ Dataset: SemanticKITTI-E, Samples: 4649 frames (3834 train, 815 val), Modality: RGB images + simulated event data + LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Every Image Listens, Every Image Dances: Music-Driven Image Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.18801"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stony Brook University<br>
‚Ä¢ Dataset: MuseDance, Samples: 2904, Modality: RGB videos + audio + text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>UDC-VIT: A Real-World Video Dataset for Under-Display Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.18545"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mcrl/UDC-VIT"><img src="https://img.shields.io/github/stars/mcrl/UDC-VIT.svg?style=social&label=Star"></a><br><a href="https://kyusuahn.github.io/UDC-VIT.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Data Science, Seoul National University, Seoul, Republic of Korea; Research Center, Samsung Display Co., Ltd., Yongin, Republic of Korea<br>
‚Ä¢ Dataset: UDC-VIT, Samples: 647, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via Multimodal Visual Feature Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.18124"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="remote-bmxs.netlify.app"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Academy for Engineering & Technology, Fudan University<br>
‚Ä¢ Dataset: NEPose, Samples: 50 videos, Modality: Binocular 4K endoscopic videos + pose trajectories from optical tracking system<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>VidSole: A Multimodal Dataset for Joint Kinetics Quantification and Disease Detection with Deep Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.17890"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: VidSole, Samples: 2632, Modality: instrumented insole forces and moments, 2-viewpoint RGB video, 3D motion capture, force plate data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Extending Information Bottleneck Attribution to Video Sequences</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.16889"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/anonrep/IBA-for-Video-Sequences"><img src="https://img.shields.io/github/stars/anonrep/IBA-for-Video-Sequences.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit ¬®at Berlin<br>
‚Ä¢ Dataset: deepfake detection dataset, Samples: 378, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.14319"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Xiaohao-Xu/SLAM-under-Perturbation"><img src="https://img.shields.io/github/stars/Xiaohao-Xu/SLAM-under-Perturbation.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Michigan, Ann Arbor<br>
‚Ä¢ Dataset: Robust-Ego3D, Samples: 1000, Modality: RGB-D videos + pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Improving Video Generation with Human Feedback</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.13918"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gongyeliu.github.io/videoalign"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: Human-labeled video generation preference dataset, Samples: 108000, Modality: RGB videos + human preference annotations<br>
‚Ä¢ Dataset: VideoGen-RewardBench, Samples: 26500, Modality: RGB videos + human preference annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Multimodal Sensor Dataset for Monitoring Older Adults Post Lower-Limb Fractures in Community Settings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.13888"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/abedidev/maison-llf"><img src="https://img.shields.io/github/stars/abedidev/maison-llf.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/14597613"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KITE Research Institute, Toronto Rehabilitation Institute, University Health Network, Toronto, Canada<br>
‚Ä¢ Dataset: MAISON-LLF, Samples: 560, Modality: Accelerometer, GPS, step count, binary motion events, heart rate, sleep data, clinical questionnaires<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.13335"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan 430074, China; S-Lab for Advanced Intelligence, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: ZJU-MoCap-Blur, Samples: 6 sequences, Modality: Synthesized motion-blurred RGB videos + SMPL poses + foreground masks<br>
‚Ä¢ Dataset: Real-Human-Blur, Samples: None, Modality: Monocular motion-blurred RGB videos + SMPL poses + foreground masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Machine Learning Modeling for Multi-order Human Visual Motion Processing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.12810"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/anoymized/multi-order-motion-model"><img src="https://img.shields.io/github/stars/anoymized/multi-order-motion-model.svg?style=social&label=Star"></a><br><a href="https://anoymized.github.io/motion-model-website/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Graduate School of Informatics, Kyoto University, Kyoto, 606-8501, Japan.<br>
‚Ä¢ Dataset: Material-Controlled Motion Dataset, Samples: None, Modality: RGB videos + optical flow<br>
‚Ä¢ Dataset: Second-order Motion Benchmark, Samples: 40 scenes, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Int2Planner: An Intention-based Multi-modal Motion Planner for Integrated Prediction and Planning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.12799"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cxlz/Int2Planner"><img src="https://img.shields.io/github/stars/cxlz/Int2Planner.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence & Department of CSE & MoE Lab of AI, Shanghai Jiao Tong University; COWAROBOT Co. Ltd.<br>
‚Ä¢ Dataset: private dataset, Samples: 680964, Modality: trajectory data, localization data, route path information<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Interaction Dataset of Autonomous Vehicles with Traffic Lights and Signs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.12536"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://uwmadison.box.com/s/dbysk2jl15w0j56hd02rfaosuhvx3zu0"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Civil and Environmental Engineering, University of Wisconsin-Madison, Madison, WI 53706, United States<br>
‚Ä¢ Dataset: Interaction Dataset of Autonomous Vehicles with Traffic Lights and Signs, Samples: 82132, Modality: Vehicle trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>TOFFE -- Temporally-binned Object Flow from Events for High-speed and Energy-Efficient Object Detection and Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.12482"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Elmore Family School of Electrical and Computer Engineering, Purdue University<br>
‚Ä¢ Dataset: TOFFE dataset, Samples: None, Modality: Synthetic data from Gazebo simulator including RGB frames, depth, event camera data, 6-DoF object pose, and object velocity.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>TalkingEyes: Pluralistic Speech-Driven 3D Eye Gaze Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.09921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lkjkjoiuiu.github.io/TalkingEyes Home/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Informatics, Xiamen University, China<br>
‚Ä¢ Dataset: TalKingEyesDataset (TKED), Samples: 5982, Modality: Audio + 3D mesh motion sequences (FLAME parameters for eye gaze, head, and face)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>SMPLest-X: Ultimate Scaling for Expressive Human Pose and Shape Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.09782"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wqyin/SMPLest-X"><img src="https://img.shields.io/github/stars/wqyin/SMPLest-X.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Tokyo, SenseTime Research<br>
‚Ä¢ Dataset: SynHand, Samples: 462800, Modality: Synthetic RGB images + SMPL-X annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Robust Single Object Tracking in LiDAR Point Clouds under Adverse Weather Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.07133"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Mathematical Sciences, Dalian University of Technology, China<br>
‚Ä¢ Dataset: KITTI-A, Samples: 2730, Modality: LiDAR point clouds<br>
‚Ä¢ Dataset: nuScenes-A, Samples: 82770, Modality: LiDAR point clouds<br>
‚Ä¢ Dataset: CADC-SOT, Samples: 7375, Modality: LiDAR point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian Splatting Based Data Augmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.02845"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://iscas3dv.github.io/HOGSA/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Software, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: HOGSA, Samples: 2400000, Modality: RGB images + 3D hand/object poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal Conditions and Larger Scene</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.02807"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Computer Engineering, Peking University, China<br>
‚Ä¢ Dataset: AE-NeRF Synthetic Event Dataset, Samples: 8, Modality: Synthetic event streams, RGB images, ground truth camera poses, estimated camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>WorldPose: A World Cup Dataset for Global 3D Human Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.02771"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://eth-ait.github.io/WorldPoseDataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich<br>
‚Ä¢ Dataset: WorldPose, Samples: 88 sequences, Modality: RGB videos + 3D human poses (SMPL) + global trajectories + camera parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>JoyGen: Audio-Driven 3D Depth-Aware Talking-Face Video Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.01798"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JOY-MM/JoyGen"><img src="https://img.shields.io/github/stars/JOY-MM/JoyGen.svg?style=social&label=Star"></a><br><a href="https://joy-mm.github.io/JoyGen"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: JD.Com, Inc.<br>
‚Ä¢ Dataset: Chinese talking-face dataset, Samples: 1100, Modality: RGB videos + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Free-Form Motion Control: A Synthetic Video Generation Dataset with Controllable Camera and Object Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.01425"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://henghuiding.github.io/SynFMC"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: SynFMC, Samples: 26000, Modality: Synthetic videos + 6D object poses + 6D camera poses + segmentation masks + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>L3D-Pose: Lifting Pose for 3D Avatars from a Single Camera in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.01174"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://soumyaratnadebnath.github.io/L3D-Pose"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IIT Gandhinagar, India<br>
‚Ä¢ Dataset: Deep Macaque, Samples: 8000, Modality: Synthetic renderings + 2D/3D pose data<br>
‚Ä¢ Dataset: Deep Horse, Samples: 6000, Modality: Synthetic renderings + 2D/3D pose data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>DynamicLip: Shape-Independent Continuous Authentication via Lip Articulator Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.01032"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xxxxx"><img src="https://img.shields.io/github/stars/github.com/xxxxx.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100085, China<br>
‚Ä¢ Dataset: Dynamic Lip Authentication Dataset, Samples: 50 subjects, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>T-DOM: A Taxonomy for Robotic Manipulation of Deformable Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.20998"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/t-dom"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institut de Rob√≤tica i Inform√†tica Industrial, CSIC-UPC, Barcelona, Spain<br>
‚Ä¢ Dataset: Deformable Object Manipulation Dataset, Samples: 10 tasks, Modality: RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>DAVE: Diverse Atomic Visual Elements Dataset with High Representation of Vulnerable Road Users in Complex and Unpredictable Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.20042"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: DA VE, Samples: 1231, Modality: RGB videos + GPS + Bounding Box/Action Annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>UniAvatar: Taming Lifelike Audio-Driven Talking Head Generation with Comprehensive Motion and Lighting Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.19860"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Li Auto<br>
‚Ä¢ Dataset: DH-FaceDrasMvVid-100, Samples: None, Modality: RGB videos<br>
‚Ä¢ Dataset: DH-FaceReliVid-200, Samples: None, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>RobotDiffuse: Motion Planning for Redundant Manipulator based on Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.19500"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ACRoboT-buaa/RobotDiffuse"><img src="https://img.shields.io/github/stars/ACRoboT-buaa/RobotDiffuse.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software, Beihang University, Beijing, China.<br>
‚Ä¢ Dataset: Robot-obtalcles-panda (ROP), Samples: 140000, Modality: Robot pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Learning Monocular Depth from Events via Egomotion Compensation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.19067"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not specified in the document<br>
‚Ä¢ Dataset: EventCitySim, Samples: 5, Modality: RGB images, depth maps, event data, IMU measurements, gyroscope data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Mimicking-Bench: A Benchmark for Generalizable Humanoid-Scene Interaction Learning via Human Mimicking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.17730"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mimicking-bench.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: Mimicking-Bench, Samples: 23490, Modality: Human skeleton motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>V$^2$-SfMLearner: Learning Monocular Depth and Ego-motion for Multimodal Wireless Capsule Endoscopy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.17595"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China<br>
‚Ä¢ Dataset: Multimodal-WCE-1 (MM-WCE-1), Samples: 11, Modality: RGB videos + Vibration signals + Depth maps + Ego-motion trajectories<br>
‚Ä¢ Dataset: Multimodal-WCE-2 (MM-WCE-2), Samples: 11, Modality: RGB videos + Vibration signals + Depth maps + Ego-motion trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>WildPPG: A Real-World PPG Dataset of Long Continuous Recordings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.17540"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://siplab.org/projects/WildPPG"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science ETH Z√ºrich, Switzerland<br>
‚Ä¢ Dataset: WildPPG, Samples: 216 hours, Modality: PPG, Accelerometer, ECG, Temperature, Altitude<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>InterDance:Reactive 3D Dance Generation with Realistic Duet Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.16982"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://inter-dance.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: InterDance, Samples: None, Modality: MoCap (SMPL-X format)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Long-Term Upper-Limb Prosthesis Myocontrol via High-Density sEMG and Incremental Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.16271"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DarioDiDomenico/IncrHDsEMG"><img src="https://img.shields.io/github/stars/DarioDiDomenico/IncrHDsEMG.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/record/10801000"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Rehab Technologies Lab, Istituto Italiano di Tecnologia (IIT), Genoa, Italy, DET, Politecnico di Torino, Turin, Italy<br>
‚Ä¢ Dataset: DELTA, Samples: 2940, Modality: High-Density surface electromyography (HD-sEMG) signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>AutoLife: Automatic Life Journaling with Smartphones and LLMs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.15714"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: A self-collected human life dataset for life journaling, Samples: 58, Modality: smartphone sensor data (accelerometer, gyroscope, barometer, GPS speed, GPS location, WiFi signals)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.15664"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://virtualhumans.mpi-inf.mpg.de/scenic/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: T√ºbingen AI Center, University of T√ºbingen; Max Planck Institute for Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: The SCENIC Dataset, Samples: 15000, Modality: SMPL motion + text annotations + terrain meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Event-assisted 12-stop HDR Imaging of Dynamic Scene</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.14705"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://openimaginglab.github.io/Event-Assisted-12stops-HDR/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory<br>
‚Ä¢ Dataset: ESHDR, Samples: None, Modality: Synchronized LDR images and event camera streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Learning from Massive Human Videos for Universal Humanoid Pose Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.14172"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://usc-gvl.github.io/UH-1"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Southern California<br>
‚Ä¢ Dataset: Humanoid-X, Samples: 163800, Modality: RGB videos, text descriptions, SMPL human poses, humanoid keypoints, humanoid target DoF positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>TH√ñR-MAGNI Act: Actions for Human Motion Modeling in Robot-Shared Industrial Spaces</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.13729"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tmralmeida/thor-magni-actions"><img src="https://img.shields.io/github/stars/tmralmeida/thor-magni-actions.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AASS, ¬®Orebro University<br>
‚Ä¢ Dataset: TH¬®OR-MAGNI Act, Samples: 8.3 hours of labeled actions, Modality: Action labels, MoCap trajectories, Egocentric video, Gaze data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Move-in-2D: 2D-Conditioned Human Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.13185"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hhsinping.github.io/Move-in-2D"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Adobe Research, University of California, Merced<br>
‚Ä¢ Dataset: Humans-in-Context Motion (HiC-Motion), Samples: 300000, Modality: RGB videos + SMPL motion + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Continuous Patient Monitoring with AI: Real-Time Analysis of Video in Hospital Care Settings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.13152"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lookdeep/ai-norms-2024"><img src="https://img.shields.io/github/stars/lookdeep/ai-norms-2024.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: LookDeep Health<br>
‚Ä¢ Dataset: ai-norms-2024, Samples: 1466 patient-days, Modality: Computer vision predictions (object detection, role classification, motion estimation) from RGB/NIR video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>A New Adversarial Perspective for LiDAR-based 3D Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.13017"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Informatics, Xiamen University, China; Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, China<br>
‚Ä¢ Dataset: ROLiD, Samples: 1964 water mist sequences, 664 smoke sequences, Modality: LiDAR point cloud sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Task-Parameter Nexus: Task-Specific Parameter Learning for Model-Based Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.12448"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ILLINOIS.EDU<br>
‚Ä¢ Dataset: Trajectory Bank, Samples: 1200, Modality: 2D quadrotor polynomial trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Can video generation replace cinematographers? Research on the cinematic language of generated video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.12223"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tongji University<br>
‚Ä¢ Dataset: Cinematic2K, Samples: 2000, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Instruction-based Image Manipulation by Watching How Things Move</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.12087"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo<br>
‚Ä¢ Dataset: InstructMove dataset, Samples: 6000000, Modality: RGB image pairs + text instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.11974"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/declare-lab/EMMA-X"><img src="https://img.shields.io/github/stars/declare-lab/EMMA-X.svg?style=social&label=Star"></a><br><a href="https://declare-lab.github.io/Emma-X/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Singapore University of Technology and Design<br>
‚Ä¢ Dataset: hierarchical embodiment dataset, Samples: 60000, Modality: Robot manipulation trajectories (7D actions) + images + textual annotations (grounded reasoning, 3D movement plans) + 2D gripper positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.11198"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vita-epfl/GEM"><img src="https://img.shields.io/github/stars/vita-epfl/GEM.svg?style=social&label=Star"></a><br><a href="https://vita-epfl.github.io/GEM.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: √âcole Polytechnique F√©d√©rale de Lausanne (EPFL)<br>
‚Ä¢ Dataset: GEM Curated and Pseudo-Labeled Dataset, Samples: None, Modality: RGB videos + pseudo-labeled depth, ego-trajectories, and human poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>A Pioneering Neural Network Method for Efficient and Robust Fluid Simulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.10748"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software Engineering, Xi‚Äôan Jiaotong University, Xi‚Äôan, 710049, China<br>
‚Ä¢ Dataset: Fueltank dataset, Samples: 320000, Modality: SPH fluid particle simulation<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SUGAR: Subject-Driven Video Customization in a Zero-Shot Manner</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.10533"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Adobe Research<br>
‚Ä¢ Dataset: Unnamed synthetic dataset for subject-driven video customization, Samples: 2500000, Modality: RGB videos + text + images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.09623"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lwq20020127.github.io/OmniDrag"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Computer Engineering, Peking University<br>
‚Ä¢ Dataset: Move360, Samples: 1580, Modality: Omnidirectional videos (ODV)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.09283"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/NJU-PCALab/InstanceCap"><img src="https://img.shields.io/github/stars/NJU-PCALab/InstanceCap.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: InstanceVid, Samples: 22000, Modality: RGB videos + structured captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SyncViolinist: Music-Oriented Violin Motion Generation Based on Bowing and Fingering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.08343"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Kakanat/SyncViolinist"><img src="https://img.shields.io/github/stars/Kakanat/SyncViolinist.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Waseda University<br>
‚Ä¢ Dataset: SyncViolinist Dataset, Samples: 61, Modality: MoCap joints, audio, MIDI, bowing/fingering annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Generative Zoo</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.08101"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://genzoo.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T ¬®ubingen, Germany<br>
‚Ä¢ Dataset: GenZoo, Samples: 1000000, Modality: RGB images + 3D pose and shape parameters<br>
‚Ä¢ Dataset: GenZoo-Felidae, Samples: 100, Modality: RGB images + 3D pose and shape parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Repurposing Pre-trained Video Diffusion Models for Event-based Video Interpolation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07761"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vdm-evfi.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, Park<br>
‚Ä¢ Dataset: Clear-Motion, Samples: 9 sequences, Modality: RGB videos + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07759"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://fuxiao0719.github.io/projects/3dtrajmaster"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: 360‚ó¶-Motion Dataset, Samples: 54000, Modality: RGB videos + 6DoF pose trajectories + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07755"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Boston University<br>
‚Ä¢ Dataset: SAT (Spatial Aptitude Training), Samples: 175000, Modality: Simulated 2D images + Question-Answer pairs about static and dynamic (ego/object motion) spatial reasoning<br>
‚Ä¢ Dataset: SAT real-image dynamic test set, Samples: 150, Modality: Real-world images + Question-Answer pairs about dynamic spatial reasoning<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>On Motion Blur and Deblurring in Visual Place Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07751"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/bferrarini/MotionBlurGenerator"><img src="https://img.shields.io/github/stars/bferrarini/MotionBlurGenerator.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronics and Computer Science, University of Southampton, SO17 1BJ Southampton, U.K.<br>
‚Ä¢ Dataset: Blurry Places benchmark, Samples: 9 traverses, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Benchmarking Vision-Based Object Tracking for USVs in Complex Maritime Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07392"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Muhayyuddin/tracking"><img src="https://img.shields.io/github/stars/Muhayyuddin/tracking.svg?style=social&label=Star"></a><br><a href="https://muhayyuddin.github.io/tracking/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University, United Arab Emirates.<br>
‚Ä¢ Dataset: Real-world USV tracking dataset, Samples: 21, Modality: RGB videos<br>
‚Ä¢ Dataset: Simulated USV tracking dataset, Samples: 5, Modality: Simulated RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view RGB and Event Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.06770"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/4dqv/DynEventNeRF"><img src="https://img.shields.io/github/stars/4dqv/DynEventNeRF.svg?style=social&label=Star"></a><br><a href="https://4dqv.mpi-inf.mpg.de/DynEventNeRF/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Informatics, SIC; Saarland University<br>
‚Ä¢ Dataset: Dynamic EventNeRF Real Dataset, Samples: 16, Modality: multi-view (6) RGB and event streams<br>
‚Ä¢ Dataset: Dynamic EventNeRF Synthetic Dataset, Samples: 5, Modality: multi-view (5) synthetic RGB frames and event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Object Detection using Event Camera: A MoE Heat Conduction based Detector and A New Benchmark Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.06647"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/OpenEvDET1"><img src="https://img.shields.io/github/stars/Event-AHU/OpenEvDET1.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: EvDET200K, Samples: 10054, Modality: Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>doScenes: An Autonomous Driving Dataset with Natural Language Instruction for Human Interaction and Vision-Language Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05893"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.github.com/rossgreer/doScenes"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Merced<br>
‚Ä¢ Dataset: doScenes, Samples: 1000, Modality: vehicle trajectories, multimodal sensor data (cameras, LiDAR, radar), natural language instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>MotionShop: Zero-Shot Motion Transfer in Video Diffusion Models with Mixture of Score Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05355"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="motionshop-diffusion.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Virginia Tech<br>
‚Ä¢ Dataset: MotionBench, Samples: 1200, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>ACT-Bench: Towards Action Controllable World Models for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05337"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Turing Inc.<br>
‚Ä¢ Dataset: ACT-BENCH, Samples: 2286, Modality: RGB videos + trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Œª: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile Manipulation Robotics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05313"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="lambdabenchmark.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: LAMBDA (Œª), Samples: 571, Modality: RGB-D egocentric observations, segmentations, robot/object poses, discrete actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Text to Blind Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05277"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://blindways.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Boston University<br>
‚Ä¢ Dataset: BlindWays, Samples: 1029, Modality: IMU-based 3D joints, text descriptions, synchronized RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Monocular Dynamic Gaussian Splatting: Fast, Brittle, and Scene Complexity Rules</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.04457"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: instructive synthetic dataset, Samples: 30, Modality: Synthetic RGB videos + camera poses<br>
‚Ä¢ Dataset: D-NeRF, Nerfies, HyperNeRF, NeRF-DS, iPhone dataset (extensions), Samples: 50, Modality: Segmentation masks and improved camera poses for existing RGB video datasets<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.04244"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ivl.cs.brown.edu/research/gigahands.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: GigaHands, Samples: 14000, Modality: Multi-view RGB videos, 3D hand poses (keypoints, MANO meshes), 3D object poses, text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.04037"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://grisoon.github.io/INFP/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Bytedance<br>
‚Ä¢ Dataset: DyConv, Samples: over 200 hours of video clips, Modality: Video clips of dyadic conversations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Dense Scene Reconstruction from Light-Field Images Affected by Rolling Shutter</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.03518"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ICB-Vision-AI/DenseRSLF"><img src="https://img.shields.io/github/stars/ICB-Vision-AI/DenseRSLF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universit ¬¥e de Bourgogne, CNRS UMR 6303 ICB; Universit ¬¥e de Franche-Comt ¬¥e, CNRS UMR 6174 FEMTO-ST<br>
‚Ä¢ Dataset: RSLF+, Samples: None, Modality: Light-field images + depth maps + visibility masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Video LLMs for Temporal Reasoning in Long Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02930"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.retrocausal.ai"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Retrocausal, Inc.<br>
‚Ä¢ Dataset: IndustryASM, Samples: 4803, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Adaptive LiDAR Odometry and Mapping for Autonomous Agricultural Mobile Robots in Unmanned Farms</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02899"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UCR-Robotics/AG-LOAM"><img src="https://img.shields.io/github/stars/UCR-Robotics/AG-LOAM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, University of California-Riverside<br>
‚Ä¢ Dataset: AG-LOAM dataset, Samples: 18, Modality: LiDAR point clouds, robot odometry, GPS-RTK<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>emg2pose: A Large and Diverse Benchmark for Surface Electromyographic Hand Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02725"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/emg2pose"><img src="https://img.shields.io/github/stars/facebookresearch/emg2pose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Reality Labs, Meta<br>
‚Ä¢ Dataset: emg2pose, Samples: 25253, Modality: sEMG + motion capture joint angles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02449"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://byencoder.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of Freiburg, 79110 Freiburg, Germany<br>
‚Ä¢ Dataset: BYE Dataset, Samples: 29, Modality: RGB-D images, instance masks, camera pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>It Takes Two: Real-time Co-Speech Two-person's Interaction Generation via Reactive Auto-regressive Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02419"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: InterAct++, Samples: 402, Modality: MoCap joints + Face + Audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Dual Exposure Stereo for Extended Dynamic Range 3D Imaging</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02351"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: POSTECH<br>
‚Ä¢ Dataset: Real-world Dataset, Samples: 7432, Modality: stereo RGB videos + LiDAR point clouds<br>
‚Ä¢ Dataset: Synthetic Dataset, Samples: 1200, Modality: synthetic stereo videos + depth maps + disparity maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Continuous-Time Human Motion Field from Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.01747"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Pennsylvania, USA<br>
‚Ä¢ Dataset: Beam-splitter Event Agile Human Motion Dataset (BEAHM), Samples: 40, Modality: Events + RGB videos + 3D meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Articulate3D: Holistic Understanding of 3D Scenes as Universal Scene Description</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.01398"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: INSAIT, Sofia University ‚ÄúSt. Kliment Ohridski‚Äù<br>
‚Ä¢ Dataset: Articulate3D, Samples: 280, Modality: 3D scene scans with part-level articulation kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Object Agnostic 3D Lifting in Space and Time</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.01166"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Adelaide<br>
‚Ä¢ Dataset: AnimalSyn3D, Samples: 678, Modality: 3D skeleton motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Human Action CLIPs: Detecting AI-generated Human Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.00526"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.huggingface.co/datasets/faridlab/deepaction"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google, Stanford University<br>
‚Ä¢ Dataset: DeepAction, Samples: 3200, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.00174"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://solami-ai.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SenseTime Research<br>
‚Ä¢ Dataset: SynMSI, Samples: 6300, Modality: SMPL-X joint rotations and synthesized speech<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>ETAP: Event-based Tracking of Any Point</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.00133"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tub-rip/ETAP"><img src="https://img.shields.io/github/stars/tub-rip/ETAP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit√§t Berlin<br>
‚Ä¢ Dataset: EventKubric, Samples: 10173, Modality: Events + RGB videos + point tracks + optical flow + depth + segmentations<br>
‚Ä¢ Dataset: EVIMO2, Samples: None, Modality: ground truth point tracks (new annotation)<br>
‚Ä¢ Dataset: E2D2, Samples: None, Modality: ground truth point tracks (new annotation)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.00115"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fudan-generative-vision.github.io/OpenHumanVid"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: OpenHumanVid, Samples: 13200000, Modality: RGB videos + skeleton pose + speech audio + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>SIMS: Simulating Stylized Human-Scene Interactions with Retrieval-Augmented Script Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.19921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wenjiawang0312.github.io/projects/sims/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: ViconStyle, Samples: 415, Modality: MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>SkelMamba: A State Space Model for Efficient Skeleton Action Recognition of Neurological Disorders</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.19544"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Udine<br>
‚Ä¢ Dataset: Neurological Disorders (ND), Samples: 396, Modality: RGB videos + skeleton joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.19167"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/hand-tracking-toolkit"><img src="https://img.shields.io/github/stars/facebookresearch/hand-tracking-toolkit.svg?style=social&label=Star"></a><br><a href="https://facebookresearch.github.io/hot3d"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta Reality Labs<br>
‚Ä¢ Dataset: HOT3D, Samples: 3832, Modality: Multi-view egocentric RGB/monochrome videos + motion-capture poses (hands and objects) + eye gaze + SLAM point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Lifting Motion to the 3D World via 2D Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.18808"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: NicoleMove, Samples: None, Modality: RGB videos + 2D pose sequences<br>
‚Ä¢ Dataset: CatPlay, Samples: None, Modality: RGB videos + 2D keypoint sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>GaussianSpeech: Audio-Driven Gaussian Avatars</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.18675"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://shivangi-aneja.github.io/projects/gaussianspeech"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich<br>
‚Ä¢ Dataset: Multi-view Audio-Visual Dataset (unnamed), Samples: 2500, Modality: Multi-view RGB videos, audio, and 3D face trackings<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.18654"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Motion-AILab/AToM"><img src="https://img.shields.io/github/stars/Motion-AILab/AToM.svg?style=social&label=Star"></a><br><a href="https://atom-motion.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen International Graduate School, Tsinghua University<br>
‚Ä¢ Dataset: MotionPrefer, Samples: 47100, Modality: Generated 3D human motion sequences with text-based preference labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>MotionCharacter: Identity-Preserving and Motion Controllable Human Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.18281"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motioncharacter.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huazhong University of Science and Technology<br>
‚Ä¢ Dataset: Human-Motion, Samples: 106292, Modality: RGB videos + text captions + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Snake-Inspired Mobile Robot Positioning with Hybrid Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.17430"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ansfl/MoRPINet"><img src="https://img.shields.io/github/stars/ansfl/MoRPINet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hatter Department of Marine Technologies, University of Haifa, Israel<br>
‚Ä¢ Dataset: MoRPINet, Samples: 13, Modality: IMU + GNSS-RTK<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Enhancing Lane Segment Perception and Topology Reasoning with Crowdsourcing Trajectory Priors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.17161"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wowlza/TrajTopo"><img src="https://img.shields.io/github/stars/wowlza/TrajTopo.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Vehicle and Mobility, Tsinghua University<br>
‚Ä¢ Dataset: Supplementary Trajectory Dataset for OpenLane-V2, Samples: None, Modality: crowdsourcing trajectory data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Leveraging Foundation Models To learn the shape of semi-fluid deformable objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16802"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Alstom transports, IMVIA EA 7535 laboratory, university of Burgundy<br>
‚Ä¢ Dataset: Weld Pool Dataset, Samples: 9, Modality: RGB videos + keypoint labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Bundle Adjusted Gaussian Avatars Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16758"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Artificial Intelligence Laboratory, The University of Tokyo<br>
‚Ä¢ Dataset: Synthetic ZJU-MoCap Deblurring Dataset, Samples: 6, Modality: RGB videos + SMPL parameters<br>
‚Ä¢ Dataset: 360-degree Hybrid-Exposure Human Motion Dataset, Samples: 8, Modality: Multi-view RGB videos (blurry and sharp) + SMPL parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>DiM-Gestor: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16729"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zf223669/DiMGestures"><img src="https://img.shields.io/github/stars/zf223669/DiMGestures.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Media Engineering, Communication University of Zhejiang, China<br>
‚Ä¢ Dataset: Chinese Co-Speech Gestures (CCG) dataset, Samples: 391, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Deep Learning for Motion Classification in Ankle Exoskeletons Using Surface EMG and IMU Signals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16273"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Sr933/Exoskeleton-Data-Acquisition-and-Processing-Code"><img src="https://img.shields.io/github/stars/Sr933/Exoskeleton-Data-Acquisition-and-Processing-Code.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.17863/CAM.113504"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Electrical Engineering Division, Department of Engineering, University of Cambridge, Cambridge CB3 0FA, UK; School of Clinical Medicine, University of Cambridge, Cambridge CB2 0SP, UK<br>
‚Ä¢ Dataset: None, Samples: 1504, Modality: sEMG and IMU signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>SMGDiff: Soccer Motion Generation using diffusion probabilistic models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16216"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Soccer-X, Samples: 2398, Modality: MoCap sequences (SMPL)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>KinMo: Kinematic-aware Human Motion Understanding and Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.15472"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://andypinxinliu.github.io/KinMo"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Irvine<br>
‚Ä¢ Dataset: KinMo, Samples: 14616, Modality: MoCap joints + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>A Benchmark Dataset for Collaborative SLAM in Service Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.14775"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vision3d-lab/CSE_Dataset"><img src="https://img.shields.io/github/stars/vision3d-lab/CSE_Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Artificial Intelligence Graduate School, UNIST, South Korea<br>
‚Ä¢ Dataset: C-SLAM dataset in Service Environments (CSE), Samples: 18, Modality: stereo RGB, stereo depth, IMU, GT pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>sEMG-based Gesture-Free Hand Intention Recognition: System, Dataset, Toolbox, and Benchmark Results</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.14131"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Intelligence Science and Technology, National University of Defense Technology, Changsha 410073, China<br>
‚Ä¢ Dataset: sEMG-based Gesture-Free Hand Intention Recognition Dataset, Samples: None, Modality: 8-channel sEMG signals, 3-channel IMU data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Sparse Input View Synthesis: 3D Representations and Reliable Priors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.13631"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nagabhushansn95.github.io/publications.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Electrical Communication Engineering, Indian Institute of Science<br>
‚Ä¢ Dataset: Indian Institute of Science Virtual Environment Exploration Dataset - Dynamic Scenes (IISc VEED-Dynamic), Samples: 800, Modality: RGB videos + depth + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>VioPose: Violin Performance 4D Pose Estimation by Hierarchical Audiovisual Inference</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.13607"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SeongJong-Yoo/VioPose"><img src="https://img.shields.io/github/stars/SeongJong-Yoo/VioPose.svg?style=social&label=Star"></a><br><a href="https://sj-yoo.info/viopose/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: VioDat, Samples: 639, Modality: 3D motion capture (kinematic joints), synchronized multi-view video (4 cameras), synchronized audio (4 microphones)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Can Reasons Help Improve Pedestrian Intent Estimation? A Cross-Modal Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.13302"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Visual Information Technology (CVIT) Lab, IIIT Hyderabad<br>
‚Ä¢ Dataset: PIE++, Samples: 1842, Modality: RGB videos + multi-label textual reason annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Enhancing Thermal MOT: A Novel Box Association Method Leveraging Thermal Identity and Motion Similarity</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.12943"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wassimea/thermalMOT"><img src="https://img.shields.io/github/stars/wassimea/thermalMOT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Ottawa<br>
‚Ä¢ Dataset: RGB-Thermal MOT dataset, Samples: 30, Modality: RGB videos + Thermal videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>EROAM: Event-based Camera Rotational Odometry and Mapping in Real-time</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.11004"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wlxing1901/EROAM"><img src="https://img.shields.io/github/stars/wlxing1901/EROAM.svg?style=social&label=Star"></a><br><a href="https://wlxing1901.github.io/eroam/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, The University of Hong Kong, Hong Kong SAR, China<br>
‚Ä¢ Dataset: EROAM-campus, Samples: 10, Modality: Event camera + LiDAR + 3DoF rotational motion trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>The Oxford Spires Dataset: Benchmarking Large-Scale LiDAR-Visual Localisation, Reconstruction and Radiance Field Methods</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.10546"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Oxford Robotics Inst., Dept. of Eng. Science, Univ. of Oxford, UK<br>
‚Ä¢ Dataset: Oxford Spires Dataset, Samples: 24, Modality: RGB images, LiDAR, IMU, Ground Truth Trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.10504"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chenkang455/USP-Gaussian"><img src="https://img.shields.io/github/stars/chenkang455/USP-Gaussian.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Peking University<br>
‚Ä¢ Dataset: Synthetic Spike Dataset (based on Deblur-NeRF), Samples: None, Modality: Spike streams<br>
‚Ä¢ Dataset: Real-world Spike Dataset, Samples: None, Modality: Spike streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Gait Kinematics in Healthy Participants: A Motion Capture Dataset Under Weight Load and Knee Brace Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.10485"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://figshare.com/articles/dataset/IMU-Based_Motion_Capture_Data_for_Various_Walking_Tasks/26090200"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Electrical Engineering, K. N. Toosi University of Technology, Tehran, 1631714191, Iran<br>
‚Ä¢ Dataset: IMU-Based Motion Capture Data for Various Walking Tasks, Samples: None, Modality: IMU data (raw sensor, processed, Euler angles, joint kinematics)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.09921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://groundmore.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CRCV, University of Central Florida<br>
‚Ä¢ Dataset: GROUND-MORE, Samples: 1715, Modality: RGB videos + text questions + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>SINETRA: a Versatile Framework for Evaluating Single Neuron Tracking in Behaving Animals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.09462"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/raphaelreme/SINETRA"><img src="https://img.shields.io/github/stars/raphaelreme/SINETRA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institut Pasteur, Universit ¬¥e de Paris-Cit ¬¥e, CNRS UMR 3691, BioImage Analysis Unit F-75015 Paris, France<br>
‚Ä¢ Dataset: SINETRA Synthetic Dataset, Samples: 15, Modality: Synthetic 2D/3D fluorescence videos with ground truth particle trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.08380"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://egovid.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Alibaba<br>
‚Ä¢ Dataset: EgoVid-5M, Samples: 5000000, Modality: RGB videos + kinematic control (VIO) + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.08279"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WU-CVGL/MBA-SLAM"><img src="https://img.shields.io/github/stars/WU-CVGL/MBA-SLAM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology at Zhejiang University and the School of Engineering at Westlake University, Hangzhou, China<br>
‚Ä¢ Dataset: real-world motion-blurred SLAM dataset, Samples: 3, Modality: RGB-D video + MoCap ground truth trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.06757"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/quzefan/LuSh-NeRF"><img src="https://img.shields.io/github/stars/quzefan/LuSh-NeRF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science City University of Hong Kong<br>
‚Ä¢ Dataset: LOL-BlurNeRF, Samples: 10, Modality: RGB videos + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>GraV: Grasp Volume Data for the Design of One-Handed XR Interfaces</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.05245"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HAL-UCSB/grav-sim"><img src="https://img.shields.io/github/stars/HAL-UCSB/grav-sim.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California Santa Barbara, CA, USA<br>
‚Ä¢ Dataset: GraV, Samples: 367, Modality: point clouds + motion cost<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Pose2Trajectory: Using Transformers on Body Pose to Predict Tennis Player's Trajectory</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.04501"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/alshami52/Pose2Trajectory.git"><img src="https://img.shields.io/github/stars/alshami52/Pose2Trajectory.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Science Department, University of Colorado, Colorado Springs<br>
‚Ä¢ Dataset: None, Samples: , Modality: 2D joint positions, bounding boxes, ball coordinates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>MA^2: A Self-Supervised and Motion Augmenting Autoencoder for Gait-Based Automatic Disease Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.03129"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/EchoItLiu/MA2-PyTorch"><img src="https://img.shields.io/github/stars/EchoItLiu/MA2-PyTorch.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information Technology and Media, Hexi University, Zhangye, 734000, P.R. China<br>
‚Ä¢ Dataset: tRGG, Samples: 101125, Modality: Ground Reaction Force (GRF)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Real-Time Detection for Small UAVs: Combining YOLO and Multi-frame Motion Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.02582"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Aerospace Engineering, Beijing Institute of Technology, Beijing 100081, China<br>
‚Ä¢ Dataset: Fixed-Wings Dataset, Samples: 13, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>GenXD: Generating Any 3D and 4D Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.02319"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gen-x-d.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National University of Singapore<br>
‚Ä¢ Dataset: CamVid-30K, Samples: 30000, Modality: videos with camera poses and object motion strength<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Object segmentation from common fate: Motion energy processing enables human-like zero-shot generalization to random dot stimuli</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.01505"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mtangemann/motion_energy_segmentation"><img src="https://img.shields.io/github/stars/mtangemann/motion_energy_segmentation.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of T√ºbingen, T√ºbingen AI Center<br>
‚Ä¢ Dataset: Synthetic video dataset for motion segmentation (unnamed), Samples: 1001, Modality: Synthetic RGB videos with ground truth masks and optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Nightbeat: Heart Rate Estimation From a Wrist-Worn Accelerometer During Sleep</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.00731"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/eth-siplab/Nightbeat"><img src="https://img.shields.io/github/stars/eth-siplab/Nightbeat.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, ETH Zurich, Z√ºrich, Switzerland<br>
‚Ä¢ Dataset: Nightbeat-DB, Samples: 42, Modality: 3-axis accelerometer signals + ECG signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Muscles in Time: Learning to Understand Human Motion by Simulating Muscle Activations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.00128"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/simplexsigil/MusclesInTime"><img src="https://img.shields.io/github/stars/simplexsigil/MusclesInTime.svg?style=social&label=Star"></a><br><a href="https://simplexsigil.github.io/mint"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Karlsruhe Institute of Technology<br>
‚Ä¢ Dataset: Muscles in Time (MinT), Samples: None, Modality: Simulated muscle activations + pose sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Learning Video Representations without Natural Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.24213"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://unicorn53547.github.io/video_syn_rep/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Accelerating and transforming textures, Samples: 9537, Modality: Synthetic RGB videos<br>
‚Ä¢ Dataset: Accelerating and transforming StyleGAN crops, Samples: 9537, Modality: Synthetic RGB videos<br>
‚Ä¢ Dataset: Accelerating and transforming image crops, Samples: 9537, Modality: Synthetic RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Exploiting Information Theory for Intuitive Robot Programming of Manual Activities</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.23963"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.13846970"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Human-Robot Interfaces and Interaction, Istituto Italiano di Tecnologia, Genoa, Italy<br>
‚Ä¢ Dataset: HANDSOME (HAND Skills demOnstrated by Multi-subjEcts), Samples: 250, Modality: RGB videos + 6D poses (from ArUco markers)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided Mixture-of-Experts</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.23836"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: HDAV (High-definition Audio-Visual dataset), Samples: 2203, Modality: RGB videos + audio + 3D human template parameter annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>GS-Blur: A 3D Scene-Based Dataset for Realistic Image Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.23658"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dongwoohhh/GS-Blur"><img src="https://img.shields.io/github/stars/dongwoohhh/GS-Blur.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of ECE&ASRI, Seoul National University, Korea<br>
‚Ä¢ Dataset: GS-Blur, Samples: 752335, Modality: Paired blurry and sharp RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>bit2bit: 1-bit quanta video reconstruction via self-supervised photon prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.23247"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lyehe/ssunet"><img src="https://img.shields.io/github/stars/lyehe/ssunet.svg?style=social&label=Star"></a><br><a href="https://drive.google.com/drive/folders/1M5bsmsaLBkYmO7nMUjK5_m71RonOp-P9"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Case Western Reserve University<br>
‚Ä¢ Dataset: bit2bit SPAD video dataset, Samples: 7 real SPAD videos (100k-130k frames each) and 1 synthetic video (3990 frames), Modality: 1-bit SPAD high-speed videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>NYC-Event-VPR: A Large-Scale High-Resolution Event-Based Visual Place Recognition Dataset in Dense Urban Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.21615"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ai4ce/NYC-Event-VPR"><img src="https://img.shields.io/github/stars/ai4ce/NYC-Event-VPR.svg?style=social&label=Star"></a><br><a href="https://ai4ce.github.io/NYC-Event-VPR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: New York University, Brooklyn, NY 11201, USA<br>
‚Ä¢ Dataset: NYC-Event-VPR, Samples: None, Modality: event streams, RGB videos, GPS trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Skinned Motion Retargeting with Dense Geometric Interaction Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20986"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/abcyzj/MeshRet"><img src="https://img.shields.io/github/stars/abcyzj/MeshRet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Technology, BNRist, Tsinghua University; Key Laboratory of Pervasive Computing, Ministry of Education<br>
‚Ä¢ Dataset: ScanRet, Samples: 8298, Modality: Motion capture sequences on 3D scanned meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>CardiacNet: Learning to Reconstruct Abnormalities for Cardiac Disease Assessment from Echocardiogram Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20769"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xmed-lab/CardiacNet"><img src="https://img.shields.io/github/stars/xmed-lab/CardiacNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: CardiacNet-PAH, Samples: 496, Modality: Echocardiogram videos<br>
‚Ä¢ Dataset: CardiacNet-ASD, Samples: 231, Modality: Echocardiogram videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>YourSkatingCoach: A Figure Skating Video Benchmark for Fine-Grained Element Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20427"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Science, Academia Sinica<br>
‚Ä¢ Dataset: YourSkatingCoach, Samples: 454, Modality: RGB videos + 2D skeletons<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>NT-VOT211: A Large-Scale Benchmark for Night-time Visual Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20421"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LiuYuML/NV-VOT211"><img src="https://img.shields.io/github/stars/LiuYuML/NV-VOT211.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xinjiang University<br>
‚Ä¢ Dataset: NT-VOT211, Samples: 211, Modality: RGB videos + bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>SFTrack: A Robust Scale and Motion Adaptive Algorithm for Tracking Small and Fast Moving Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20079"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Immersive Media Engineering & Department of Computer Science Education, Sungkyunkwan University<br>
‚Ä¢ Dataset: Refined UAVDT, Samples: 55, Modality: RGB videos + bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>On Occlusions in Video Action Detection: Benchmark Datasets And Training Recipes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.19553"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rajatmodi62/OccludedActionBenchmark"><img src="https://img.shields.io/github/stars/rajatmodi62/OccludedActionBenchmark.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CRCV, University of Central Florida<br>
‚Ä¢ Dataset: O-UCF, Samples: 20306, Modality: RGB videos<br>
‚Ä¢ Dataset: OVIS-UCF, Samples: 20306, Modality: RGB videos<br>
‚Ä¢ Dataset: O-JHMDB, Samples: 5896, Modality: RGB videos<br>
‚Ä¢ Dataset: OVIS-JHMDB, Samples: 5896, Modality: RGB videos<br>
‚Ä¢ Dataset: Real-OUCF, Samples: 1743, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>x-RAGE: eXtended Reality -- Action & Gesture Events Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.19486"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.com/NVM_IITD_Research/xrage"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Indian Institute of Technology Delhi, New Delhi, India - 110016<br>
‚Ä¢ Dataset: X-RAGE, Samples: 8064, Modality: event camera<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>OVT-B: A New Large-Scale Benchmark for Open-Vocabulary Multi-Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.17534"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Coo1Sea/OVT-B-Dataset"><img src="https://img.shields.io/github/stars/Coo1Sea/OVT-B-Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software Technology, Zhejiang University<br>
‚Ä¢ Dataset: OVT-B, Samples: 1973, Modality: RGB videos + bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>E-3DGS: Gaussian Splatting with Exposure and Motion Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.16995"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MasterHow/E-3DGS"><img src="https://img.shields.io/github/stars/MasterHow/E-3DGS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, China<br>
‚Ä¢ Dataset: EME-3D, Samples: 9, Modality: Event data (motion and exposure), camera parameters, sparse point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Pedestrian motion prediction evaluation for urban autonomous driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.16864"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dmytrozabolotnii/autoware-mini"><img src="https://img.shields.io/github/stars/dmytrozabolotnii/autoware-mini.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Computer Science, University of Tartu<br>
‚Ä¢ Dataset: Tartu Autonomous Vehicle Dataset, Samples: 18 scenes, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.16695"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chyangyu/MPT.git"><img src="https://img.shields.io/github/stars/chyangyu/MPT.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology, Ocean University of China, Qingdao, China<br>
‚Ä¢ Dataset: MPT (Multiple Phytoplankton Tracking), Samples: 140, Modality: High-resolution videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MotionGlot: A Multi-Embodied Motion Generation Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.16623"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ivl.cs.brown.edu/research/motionglot.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University, USA<br>
‚Ä¢ Dataset: QUAD-LOCO, Samples: 48000 trajectories, Modality: Quadruped robot trajectories (velocities) + text annotations<br>
‚Ä¢ Dataset: QUES-CAP, Samples: 23000 prompts, Modality: Situational text prompts (questions) paired with human motion data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MSGField: A Unified Scene Representation Integrating Motion, Semantics, and Geometry for Robotic Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.15730"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="MSGField.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei 230026, China<br>
‚Ä¢ Dataset: MSGField Dataset, Samples: 4, Modality: RGB video sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation and Rigorous Verification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.15154"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MCCodeAI/MCCoder"><img src="https://img.shields.io/github/stars/MCCodeAI/MCCoder.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Thrust of Data Science and Analytics, The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: MCEVAL, Samples: 186, Modality: Programming tasks (natural language instruction, canonical Python code) which generate motion trajectories for a soft-motion controller.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Quanta Video Restoration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.14994"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chennuriprateek/Quanta_Video_Restoration-QUIVER-"><img src="https://img.shields.io/github/stars/chennuriprateek/Quanta_Video_Restoration-QUIVER-.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Purdue University<br>
‚Ä¢ Dataset: I 2-2000FPS, Samples: 280, Modality: High-speed grayscale videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Unlabeled Action Quality Assessment Based on Multi-dimensional Adaptive Constrained Dynamic Time Warping</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.14161"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China<br>
‚Ä¢ Dataset: BGym, Samples: 153, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Skill Generalization with Verbs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.14118"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://rachelma80000.github.io/SkillGenVerbs/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University, Providence, RI, USA<br>
‚Ä¢ Dataset: No explicit name given, Samples: 41688, Modality: RGB image sequences of object trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.13830"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dreamvideo2.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: DreamVideo-2 Dataset, Samples: 230160, Modality: RGB videos with captions, frame-level masks, and bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MotionBank: A Large-scale Video Motion Benchmark with Disentangled Rule-based Annotations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.13790"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/liangxuy/MotionBank"><img src="https://img.shields.io/github/stars/liangxuy/MotionBank.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Eastern Institute of Technology, Ningbo<br>
‚Ä¢ Dataset: MotionBank, Samples: 1240000, Modality: SMPL parameters extracted from videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Arc-Length-Based Warping for Robot Skill Synthesis from Multiple Demonstrations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.13322"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AutoLabModena/AutoLab-Co-Manipulation-Dataset.git"><img src="https://img.shields.io/github/stars/AutoLabModena/AutoLab-Co-Manipulation-Dataset.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Italian Institute of Technology, Genova, Italy<br>
‚Ä¢ Dataset: AutoLab Co-Manipulation Dataset, Samples: 126, Modality: robot end-effector position recordings<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>QueensCAMP: an RGB-D dataset for robust Visual SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.12520"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/larocs/queenscamp-dataset"><img src="https://img.shields.io/github/stars/larocs/queenscamp-dataset.svg?style=social&label=Star"></a><br><a href="https://larocs.github.io/queenscamp-dataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Computing, Universidade de Campinas<br>
‚Ä¢ Dataset: QueensCAMP, Samples: 112, Modality: RGB-D videos + 6DoF pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.11838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hifi-diffusion.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google<br>
‚Ä¢ Dataset: LaMoR, Samples: 19, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Look Ma, no markers: holistic performance capture without the hassle</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.11520"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://aka.ms/SynthMoCap"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Microsoft<br>
‚Ä¢ Dataset: SynthBody, Samples: 100000, Modality: Synthetic RGB images + 2D landmarks + semantic segmentation + 3D joint locations + SMPL-H parameters<br>
‚Ä¢ Dataset: SynthFace, Samples: 100000, Modality: Synthetic RGB images + 2D landmarks + semantic segmentation + head pose<br>
‚Ä¢ Dataset: SynthHand, Samples: 100000, Modality: Synthetic RGB images + 2D/3D joint locations + MANO parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.10818"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://TemporalBench.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Wisconsin-Madison<br>
‚Ä¢ Dataset: TemporalBench, Samples: 2179, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>LVD-2M: A Long-take Video Dataset with Temporally Dense Captions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.10816"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SilentView/LVD-2M"><img src="https://img.shields.io/github/stars/SilentView/LVD-2M.svg?style=social&label=Star"></a><br><a href="https://silentview.github.io/LVD-2M/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: LVD-2M, Samples: 2000000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Self-Assessed Generation: Trustworthy Label Generation for Optical Flow and Stereo Matching in Real-world</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.10453"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HanLingsgjk/UnifiedGeneralization"><img src="https://img.shields.io/github/stars/HanLingsgjk/UnifiedGeneralization.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University of Science and Technology<br>
‚Ä¢ Dataset: GS58, Samples: 58800, Modality: RGB image pairs + optical flow labels / stereo disparity labels<br>
‚Ä¢ Dataset: NeRF58, Samples: 58800, Modality: RGB image pairs + optical flow labels / stereo disparity labels<br>
‚Ä¢ Dataset: 3D Flight Foreground Database, Samples: 250000, Modality: RGB image pairs + optical flow labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.09374"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/NAIL-HNU/ESVO2.git"><img src="https://img.shields.io/github/stars/NAIL-HNU/ESVO2.git.svg?style=social&label=Star"></a><br><a href="https://youtu.be/gmAU32Oeiv8"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Neuromorphic Automation and Intelligence Lab (NAIL) at School of Robotics, Hunan University, Changsha, China<br>
‚Ä¢ Dataset: hnu mapping and hnu tracking, Samples: 2, Modality: Stereo event camera, IMU, GNSS (RTK)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Enhanced Kalman with Adaptive Appearance Motion SORT for Grounded Generic Multiple Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.09243"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://UARK-AICV.github.io/G2MOT"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Arkansas<br>
‚Ä¢ Dataset: G2MOT, Samples: 253, Modality: RGB videos + bounding box tracks + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MMHead: Towards Fine-grained Multi-modal 3D Facial Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.07757"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wsj-sjtu.github.io/MMHead/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: MMHead, Samples: 35903, Modality: 3D facial motion (FLAME parameters) + speech audio + hierarchical text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.07718"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fudan-generative-vision.github.io/hallo2"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: Wild, Samples: 2019, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.07659"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northwestern University, Yellow.ai<br>
‚Ä¢ Dataset: WebVid-10M-recaptioned, Samples: 10000000, Modality: RGB videos + text captions<br>
‚Ä¢ Dataset: Curated YouTube-VOS for Sketch-guided Inpainting, Samples: None, Modality: RGB videos + binary masks + sketches + text captions<br>
‚Ä¢ Dataset: Curated DAVIS for Sketch-guided Inpainting, Samples: None, Modality: RGB videos + binary masks + sketches + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>CoPESD: A Multi-Level Surgical Motion Dataset for Training Large Vision-Language Models to Co-Pilot Endoscopic Submucosal Dissection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.07540"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gkw0010/CoPESD"><img src="https://img.shields.io/github/stars/gkw0010/CoPESD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China.<br>
‚Ä¢ Dataset: CoPESD, Samples: 88395, Modality: RGB images + bounding boxes + text-based motion instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Learning to Generate Diverse Pedestrian Movements from Web Videos with Noisy Labels</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.07500"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://genforce.github.io/PedGen/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of California, Los Angeles<br>
‚Ä¢ Dataset: CityWalkers, Samples: 120914, Modality: RGB videos + SMPL parameters + depth maps + semantic segmentation maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.06963"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://movin3d.github.io/ELMO_SIGASIA2024/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MOVIN Inc., South Korea<br>
‚Ä¢ Dataset: ELMO dataset, Samples: 20 subjects, Modality: LiDAR point clouds + Optical Motion Capture + Video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.06437"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kt2024-hal/LocoVR"><img src="https://img.shields.io/github/stars/kt2024-hal/LocoVR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California Santa Barbara, Toyota Motor North America<br>
‚Ä¢ Dataset: LocoVR, Samples: 7071, Modality: 3D motion capture trajectories (head, hands, waist, feet) in virtual reality<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Are Minimal Radial Distortion Solvers Necessary for Relative Pose Estimation?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.05984"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kocurvik/rd"><img src="https://img.shields.io/github/stars/kocurvik/rd.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague<br>
‚Ä¢ Dataset: ROTUNDA and CATHEDRAL Benchmark, Samples: 2891, Modality: RGB images + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly Detection in Surveillance Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.05900"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: VCA Group, Department of Electrical Engineering, Eindhoven University of Technology<br>
‚Ä¢ Dataset: Video Anomaly Detection Dataset (VADD), Samples: 2591, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>F√ºrElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.05791"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University, USA<br>
‚Ä¢ Dataset: F√ºrElise, Samples: 153, Modality: 3D hand motions + audio + MIDI<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>A Unified Framework for Motion Reasoning and Generation in Human Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.05628"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vim-motion-language.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea University<br>
‚Ä¢ Dataset: Inter-MT2, Samples: 153000, Modality: MoCap joints (SMPL-X) + Text Instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Real-Time Truly-Coupled Lidar-Inertial Motion Correction and Spatiotemporal Dynamic Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.05152"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://uts-ri.github.io/lidar inertial motion correction/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics Institute at the University of Technology Sydney, Australia<br>
‚Ä¢ Dataset: Techlab dataset, Samples: 2, Modality: LiDAR, IMU, MoCap<br>
‚Ä¢ Dataset: Newer College dataset (extended), Samples: 3, Modality: LiDAR, IMU, dynamicity labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Lost in Tracking: Uncertainty-guided Cardiac Cine MRI Segmentation at Right Ventricle Base</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.03320"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.tudelft.nl/yidongzhao/rvot_seg"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Imaging Physics, Delft University of Technology, Lorentzweg 1, 2628 CJ Delft, The Netherlands<br>
‚Ä¢ Dataset: ACDC dataset with refined RV base annotation, Samples: 150, Modality: Cardiac cine MRI + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Scaling Large Motion Models with Million-Level Human Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.03311"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://beingbeyond.github.io/Being-M0/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Renmin University of China<br>
‚Ä¢ Dataset: MotionLib, Samples: 1210000, Modality: 3D human motion (SMPL parameters) from RGB videos with hierarchical text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Autonomous Character-Scene Interaction Synthesis from Text Instruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.03187"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lingomotions.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for AI, Peking University, China and National Key Lab of General AI, BIGAI, China<br>
‚Ä¢ Dataset: LINGO, Samples: 16 hours of motion sequences, Modality: MoCap sequences (SMPL-X parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>AirLetters: An Open Video Dataset of Characters Drawn in the Air</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.02921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="developer.qualcomm.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto<br>
‚Ä¢ Dataset: AirLetters, Samples: 161652, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater Scene Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.01517"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WangHaoran16/UW-GS"><img src="https://img.shields.io/github/stars/WangHaoran16/UW-GS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, University of Bristol, Bristol, UK<br>
‚Ä¢ Dataset: S-UW, Samples: 4, Modality: RGB videos + dynamic object masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Learning Physics From Video: Unsupervised Physical Parameter Estimation for Continuous Dynamical Systems</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.01376"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Alejandro-neuro/Learning_physics_from_video"><img src="https://img.shields.io/github/stars/Alejandro-neuro/Learning_physics_from_video.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Delft University of Technology<br>
‚Ä¢ Dataset: Delfys75, Samples: 75, Modality: RGB videos + object masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Towards Native Generative Model for 3D Head Avatar</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.01226"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory for Novel Software Technology, Nanjing University, China<br>
‚Ä¢ Dataset: SYNHEAD100, Samples: 5200, Modality: 3D mesh models with blendshape rigging<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.00371"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="aha-vlm.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA, University of Washington<br>
‚Ä¢ Dataset: AHAdataset, Samples: 49000, Modality: RGB images of robot failure trajectories + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MM-Conv: A Multi-modal Conversational Dataset for Virtual Humans</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.00253"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mm-conv.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology, Stockholm, Sweden<br>
‚Ä¢ Dataset: MM-Conv, Samples: 6.7 hours of recordings, Modality: MoCap joints (50-marker skeleton), speech, gaze, facial expressions, scene graphs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>UniAff: A Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.20551"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SiyuanH-SJTu/UniAff"><img src="https://img.shields.io/github/stars/SiyuanH-SJTu/UniAff.svg?style=social&label=Star"></a><br><a href="https://sites.google.com/view/uni-aff/home"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University, China<br>
‚Ä¢ Dataset: UniAff Dataset, Samples: 63200, Modality: RGB-D images + 6D pose, joint axis, affordance annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Tracking Everything in Robotic-Assisted Surgery</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.19821"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zhanbh1019/SurgicalMotion"><img src="https://img.shields.io/github/stars/zhanbh1019/SurgicalMotion.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hamlyn Centre for Robotic Surgery, Imperial College London, SW7 2AZ, UK<br>
‚Ä¢ Dataset: SurgicalMotion, Samples: 20, Modality: RGB videos + 2D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>High Quality Human Image Animation using Regional Supervision and Motion Blur Condition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.19580"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Showlab, National University of Singapore<br>
‚Ä¢ Dataset: HumanDance, Samples: 3802, Modality: RGB videos + 2D keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>S2O: Static to Openable Enhancement for Articulated 3D Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.18896"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/3dlg-hcvc/s2o"><img src="https://img.shields.io/github/stars/3dlg-hcvc/s2o.svg?style=social&label=Star"></a><br><a href="https://3dlg-hcvc.github.io/s2o/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Simon Fraser University<br>
‚Ä¢ Dataset: Articulated Containers Dataset (ACD), Samples: 1350, Modality: 3D meshes with annotated part kinematics (part segmentation, motion type, motion axis, motion origin)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>EyeTrAES: Fine-grained, Low-Latency Eye Tracking via Adaptive Event Slicing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.18813"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/EyeTrAES"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Indian Institute of Technology Kharagpur, India<br>
‚Ä¢ Dataset: EyeTrAES, Samples: 40, Modality: Event camera data (events, grayscale frames) and eye tracker data (grayscale frames, point of gaze)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.17988"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wengflow.github.io/deblur-e-nerf"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The NUS Graduate School‚Äôs Integrative Sciences and Engineering Programme (ISEP)<br>
‚Ä¢ Dataset: Deblur e-NeRF Synthetic Event Dataset, Samples: None, Modality: Synthetic event streams with ground truth camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>BlinkTrack: Feature Tracking over 100 FPS via Events and Images</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.17981"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Lab of CAD&CG, Zhejiang University<br>
‚Ä¢ Dataset: MultiTrack, Samples: 2000, Modality: Synthetic images, events, and point trajectories with occlusion<br>
‚Ä¢ Dataset: EC-occ, Samples: None, Modality: RGB frames, events, and point trajectories with synthetic occlusions<br>
‚Ä¢ Dataset: EDS-occ, Samples: None, Modality: RGB frames, events, and point trajectories with synthetic occlusions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Subjective and Objective Quality-of-Experience Evaluation Study for Live Video Streaming</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.17596"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: TaoLive QoE Database, Samples: 1155, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>A vision-based framework for human behavior understanding in industrial assembly lines</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.17356"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zenodo.org/uploads/13370888"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Computer Science, Foundation for Research and Technology - Hellas, Greece<br>
‚Ä¢ Dataset: CarDA, Samples: 25, Modality: RGB-D videos, motion capture data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Adverse Weather Optical Flow: Cumulative Homogeneous-Heterogeneous Adaptation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.17001"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hyzhouboy/CH2DA-Flow"><img src="https://img.shields.io/github/stars/hyzhouboy/CH2DA-Flow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China<br>
‚Ä¢ Dataset: Real-Weather World, Samples: None, Modality: RGB videos + optical flow labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>TrackNetV4: Enhancing Fast Sports Object Tracking with Motion Attention Maps</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.14543"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arjunraj-09/TrackNetv4"><img src="https://img.shields.io/github/stars/arjunraj-09/TrackNetv4.svg?style=social&label=Star"></a><br><a href="https://sites.google.com/view/tracknetv4"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computing, Australian National University<br>
‚Ä¢ Dataset: Challenging Multi-ball Tracking Dataset, Samples: over 23,000 training frames and more than 1,000 testing frames, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>T2M-X: Learning Expressive Text-to-Motion Generation from Partially Annotated Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.13251"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Iowa State University<br>
‚Ä¢ Dataset: T2M-X Dataset, Samples: 61400, Modality: SMPL-X parametric model<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Infrared Small Target Detection in Satellite Videos: A New Dataset and A Novel Recurrent Feature Refinement Framework</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.12448"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XinyiYing/RFR"><img src="https://img.shields.io/github/stars/XinyiYing/RFR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Electronic Science and Technology, National University of Defense Technology<br>
‚Ä¢ Dataset: IRSatVideo-LEO, Samples: 200, Modality: Synthesized short-wave infrared (SWIR) videos with satellite motion and target trajectory data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>GS-Net: Generalizable Plug-and-Play 3D Gaussian Splatting Module</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.11307"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The School of Vehicle and Mobility, Tsinghua University, China<br>
‚Ä¢ Dataset: CARLA-NVS, Samples: 20 scenes (10 dynamic), 100 frames each, Modality: RGB images, Depth images, Semantic segmentation images, LiDAR point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>MotIF: Motion Instruction Fine-tuning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.10683"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motif-1k.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Massachusetts Institute of Technology<br>
‚Ä¢ Dataset: MotIF-1K, Samples: 1022, Modality: RGBD videos, joint states, language annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Dynamic Layer Detection of a Thin Materials using DenseTact Optical Tactile Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.09849"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://armlabstanford.github.io/dynamic-cloth-detection"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ARMLab in the Mechanical Engineering Department, Stanford University<br>
‚Ä¢ Dataset: None, Samples: 568, Modality: RGB videos, 6-axis wrench, joint states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>DreamMover: Leveraging the Prior of Diffusion Models for Image Interpolation with Large Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.09605"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dreamm0ver.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of AIA, Huazhong University of Science and Technology<br>
‚Ä¢ Dataset: InterpBench, Samples: 100, Modality: image pairs with large motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.08494"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/axle-lab/WheelPoser"><img src="https://img.shields.io/github/stars/axle-lab/WheelPoser.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: WheelPoser-IMU, Samples: 167 minutes of motion data, Modality: IMU sensor data + Motion Capture (SMPL)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>USTC-TD: A Test Dataset and Benchmark for Image and Video Coding in 2020s</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.08481"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/esakak/USTC-TD"><img src="https://img.shields.io/github/stars/esakak/USTC-TD.svg?style=social&label=Star"></a><br><a href="https://esakak.github.io/USTC-TD"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: the MOE Key Laboratory of Brain-Inspired Intelligent Perception and Cognition, University of Science and Technology of China, Hefei 230027, China<br>
‚Ä¢ Dataset: USTC-TD, Samples: 10, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>FORS-EMG: A Novel sEMG Dataset for Hand Gesture Recognition Across Multiple Forearm Orientations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.07484"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Engineering, Varendra University, Rajshahi, Bangladesh<br>
‚Ä¢ Dataset: FORS-EMG, Samples: 3420, Modality: sEMG signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>VMAS: Video-to-Music Generation via Semantic Alignment in Web Music Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.07450"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://genjib.github.io/project_page/VMAs/index.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UNC Chapel Hill<br>
‚Ä¢ Dataset: DISCO-MV, Samples: 2200000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>LSE-NeRF: Learning Sensor Modeling Errors for Deblured Neural Radiance Fields with RGB-Event Stereo</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.06104"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ubc-vision/LSENeRF"><img src="https://img.shields.io/github/stars/ubc-vision/LSENeRF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of British Columbia<br>
‚Ä¢ Dataset: LSE-NeRF Dataset, Samples: 10, Modality: blurry RGB videos + event streams + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>KRONC: Keypoint-based Robust Camera Optimization for 3D Car Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.05407"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Modena and Reggio Emilia<br>
‚Ä¢ Dataset: KRONC-dataset, Samples: 7, Modality: RGB images + camera poses + semantic keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>FacialFlowNet: Advancing Facial Optical Flow Estimation with a Diverse Dataset and a Decomposed Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.05396"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University<br>
‚Ä¢ Dataset: FacialFlowNet, Samples: 105970, Modality: RGB images + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>CD-NGP: A Fast Scalable Continual Representation for Dynamic Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.05166"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Automation, Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: long multi-view video dataset, Samples: 5, Modality: multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>HelmetPoser: A Helmet-Mounted IMU Dataset for Data-Driven Estimation of Human Head Motion in Diverse Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.05006"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lqiutong.github.io/HelmetPoser"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: China-Singapore International Joint Research Institute (CSIJRI) and School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: HelmetPoser, Samples: 30, Modality: IMU + VICON ground truth poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Heterogeneous LiDAR Dataset for Benchmarking Robust Localization in Diverse Degenerate Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.04961"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/PengYu-Team/GEODE_dataset"><img src="https://img.shields.io/github/stars/PengYu-Team/GEODE_dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sun Yat-sen University, The University of Hong Kong<br>
‚Ä¢ Dataset: GEODE, Samples: 64, Modality: LiDAR, Stereo Camera, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Synergy and Synchrony in Couple Dances</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.04440"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://von31.github.io/synNsync"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: 3D Human Couple Dancing Dataset, Samples: 30 hours of footage, Modality: RGB videos + 3D pseudo ground truth motion (SMPL parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>TP-GMOT: Tracking Generic Multiple Object by Textual Prompt with Motion-Appearance Cost (MAC) SORT</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.02490"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fsoft-aic.github.io/TP-GMOT"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: FPT Software AI Center, Vietnam<br>
‚Ä¢ Dataset: Refer-GMOT, Samples: 98, Modality: RGB videos + tracking annotations + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Snapshot: Towards Application-centered Models for Pedestrian Trajectory Prediction in Urban Traffic Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.01971"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/TUMFTM/Snapshot"><img src="https://img.shields.io/github/stars/TUMFTM/Snapshot.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich, Germany<br>
‚Ä¢ Dataset: Pedestrian-focused benchmark from Argoverse 2, Samples: over 1,000,000, Modality: 2D pedestrian trajectories and semantic map polylines<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>3D-LEX v1.0: 3D Lexicons for American Sign Language and Sign Language of the Netherlands</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.01901"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OlineRanum/GLEX_Controller"><img src="https://img.shields.io/github/stars/OlineRanum/GLEX_Controller.svg?style=social&label=Star"></a><br><a href="https://osf.io/g7u9c/?view_only=8090319e12aa4fd991d81e369a1cbd88"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Logic, Language and Computation, University of Amsterdam<br>
‚Ä¢ Dataset: 3D-LEX v1.0, Samples: 2000, Modality: MoCap (optical markers, data gloves), Facial blendshapes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>AMG: Avatar Motion Guided Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.01502"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zshyang/amg"><img src="https://img.shields.io/github/stars/zshyang/amg.svg?style=social&label=Star"></a><br><a href="https://github.com/zshyang/amg"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Arizona State University<br>
‚Ä¢ Dataset: AMG Dataset, Samples: 5788, Modality: Paired real RGB videos, synthetic avatar videos (driven by extracted SMPL poses), and text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.00904"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Information Engineering, Chang‚Äôan University, Xi‚Äôan, Shaanxi 710018, PR China<br>
‚Ä¢ Dataset: IArgoverse, Samples: 130565, Modality: Vehicle trajectories (2D coordinates)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and Body-Worn IMUs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.17168"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: PICO<br>
‚Ä¢ Dataset: EMHI, Samples: 885, Modality: stereo egocentric images, IMU, SMPL pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Efficient Camera Exposure Control for Visual Odometry via Deep Reinforcement Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.17005"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ShuyangUni/drl_exposure_ctrl"><img src="https://img.shields.io/github/stars/ShuyangUni/drl_exposure_ctrl.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: the Department of Electronic and Computer Engineering, the Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: Unnamed dataset for DRL-based exposure control, Samples: 3 sequences, Modality: Bracketed RGB images, LiDAR, IMU, pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>3D Pose-Based Temporal Action Segmentation for Figure Skating: A Fine-Grained and Jump Procedure-Aware Annotation Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.16638"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ryota-skating/FS-Jump3D"><img src="https://img.shields.io/github/stars/ryota-skating/FS-Jump3D.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nagoya University<br>
‚Ä¢ Dataset: FS-Jump3D, Samples: 253, Modality: Multi-view RGB videos + 3D poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>ESPARGOS: Phase-Coherent WiFi CSI Datasets for Wireless Sensing Research</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.16377"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Jeija/ESPARGOS-WiFi-ChannelCharting"><img src="https://img.shields.io/github/stars/Jeija/ESPARGOS-WiFi-ChannelCharting.svg?style=social&label=Star"></a><br><a href="https://espargos.net/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Telecommunications, Pfaffenwaldring 47, University of Stuttgart, 70569 Stuttgart, Germany<br>
‚Ä¢ Dataset: espargos-0002, Samples: 569190, Modality: WiFi CSI + robot trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.15511"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Aerospace Information Research Institute, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: SkyAgent-Act3k, Samples: 3000, Modality: Drone action sequences and pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>CMTA: Cross-Modal Temporal Alignment for Event-guided Video Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.14930"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/intelpro/CMTA"><img src="https://img.shields.io/github/stars/intelpro/CMTA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea Advanced Institute of Science and Technology<br>
‚Ä¢ Dataset: EVRB, Samples: 17 sequences (11 training, 6 test), Modality: blurred RGB videos, sharp RGB videos, event data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Towards Real-world Event-guided Low-light Video Enhancement and Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.14916"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/intelpro/ELEDNet"><img src="https://img.shields.io/github/stars/intelpro/ELEDNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea Advanced Institute of Science and Technology<br>
‚Ä¢ Dataset: RELED (Real-world Event-guided Low-light video Enhancement and Deblurring), Samples: 42, Modality: RGB videos + Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Temporally-consistent 3D Reconstruction of Birds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.13629"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/seabirds/common_murre_temporal"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Linkoping University, Sweden<br>
‚Ä¢ Dataset: common_murre_temporal, Samples: 10000 video frames; 100 test frames with 2D keypoint labels, Modality: RGB videos + 2D keypoint labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>N-DriverMotion: Driver motion learning and prediction using an event-based camera and directly trained spiking neural networks on Loihi 2</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.13379"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Engineering and Applied Sciences, Stony Brook University<br>
‚Ä¢ Dataset: N-DriverMotion, Samples: 1239, Modality: event-based camera streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>SynPlay: Importing Real-world Diversity for a Synthetic Human Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.11814"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://synplaydataset.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland College Park<br>
‚Ä¢ Dataset: SynPlay, Samples: 257, Modality: MoCap sequences rendered as RGB images with 2D/3D bounding boxes, instance-level segmentation masks, depth maps, and human keypoint locations.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>MCDubber: Multimodal Context-Aware Expressive Video Dubbing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.11593"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XiaoYuanJun-zy/MCDubber"><img src="https://img.shields.io/github/stars/XiaoYuanJun-zy/MCDubber.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Inner Mongolia University, Hohhot, China<br>
‚Ä¢ Dataset: Context Chem dataset, Samples: 3506, Modality: RGB videos + transcripts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>RaNDT SLAM: Radar SLAM Based on Intensity-Augmented Normal Distributions Transform</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.11576"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/IGMR-RWTH/RaNDT-SLAM"><img src="https://img.shields.io/github/stars/IGMR-RWTH/RaNDT-SLAM.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/record/8199947"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Mechanism Theory, Machine Dynamics and Robotics, RWTH Aachen University, Aachen, Germany<br>
‚Ä¢ Dataset: RaNDT SLAM Dataset / RaNDT Radar Benchmark, Samples: 2, Modality: LiDAR, Radar, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>EmoFace: Emotion-Content Disentangled Speech-Driven 3D Talking Face Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.11518"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: South China University of Technology<br>
‚Ä¢ Dataset: 3D-RAVDESS, Samples: 1440, Modality: 3D face mesh sequences + speech audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>RP1M: A Large-Scale Motion Dataset for Piano Playing with Bi-Manual Dexterous Robot Hands</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.11048"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://rp1m.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Aalto University, Finland<br>
‚Ä¢ Dataset: RP1M (Robot Piano 1 Million), Samples: 1M expert trajectories, Modality: Simulated robot kinematic trajectories (hand states, fingertip positions, joint positions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>DEGAS: Detailed Expressions on Full-Body Gaussian Avatars</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.10588"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://initialneil.github.io/DEGAS"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: DREAMS-Avatar, Samples: 12, Modality: multiview videos and registered SMPL-X<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Event Stream based Sign Language Translation: A High-Definition Benchmark Dataset and A New Algorithm</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.10488"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/OpenESL"><img src="https://img.shields.io/github/stars/Event-AHU/OpenESL.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: Event-CSL, Samples: 14827, Modality: Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>NeRF-US: Removing Ultrasound Imaging Artifacts from Neural Radiance Fields in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.10258"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="rishitdagli.com/nerf-us/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Departments of Computer Science; Medical Imaging, University of Toronto, Canada<br>
‚Ä¢ Dataset: Ultrasound in the Wild, Samples: 10, Modality: Ultrasound videos + probe poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Event Stream based Human Action Recognition: A High-Definition Benchmark Dataset and Algorithms</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.09764"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/CeleX-HAR"><img src="https://img.shields.io/github/stars/Event-AHU/CeleX-HAR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: CeleX-HAR, Samples: 124625, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>ALS-HAR: Harnessing Wearable Ambient Light Sensors to Enhance IMU-based Human Activity Recogntion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.09527"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: German Research Center for Artificial Intelligence, Kaiserslautern, Germany<br>
‚Ä¢ Dataset: Not explicitly named in the paper (referred to as a novel multi-modal HAR dataset), Samples: None, Modality: right wrist IMU signal, right wrist ALS signal, video footage, SMPL pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>OPPH: A Vision-Based Operator for Measuring Body Movements for Personal Healthcare</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.09409"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://groups.inf.ed.ac.uk/vision/DATASETS/HUMOLS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Edinburgh<br>
‚Ä¢ Dataset: HuMoLs, Samples: 67, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>EEPPR: Event-based Estimation of Periodic Phenomena Rate using Correlation in 3D</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.06899"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic<br>
‚Ä¢ Dataset: Unnamed in the paper, Samples: 12, Modality: Event stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>ViMo: Generating Motions from Casual Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.06614"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Chinese University of Hong Kong (Shenzhen)<br>
‚Ä¢ Dataset: Chinese classic dancing dataset, Samples: 750, Modality: 3D joint rotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>DEEPTalk: Dynamic Emotion Embedding for Probabilistic Speech-Driven 3D Face Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.06010"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://whwjdqls.github.io/deeptalk website/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Yonsei University<br>
‚Ä¢ Dataset: Emo-Vox, Samples: 1.3 hours of test data, Modality: Curated emotional audio clips from VoxCeleb2<br>
‚Ä¢ Dataset: 3D-MEAD, Samples: 36 hours, Modality: Pseudo-3D FLAME parameters from MEAD video dataset<br>
‚Ä¢ Dataset: 3D-CREMA-D, Samples: 5.23 hours, Modality: Pseudo-3D FLAME parameters from CREMA-D video dataset<br>
‚Ä¢ Dataset: 3D-RAVDESS, Samples: 1.5 hours, Modality: Pseudo-3D FLAME parameters from RAVDESS video dataset<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>FADE: A Dataset for Detecting Falling Objects around Buildings in Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.05750"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Zhengbo-Zhang/FADE"><img src="https://img.shields.io/github/stars/Zhengbo-Zhang/FADE.svg?style=social&label=Star"></a><br><a href="https://fadedataset.github.io/FADE.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan 430079, China<br>
‚Ä¢ Dataset: FADE, Samples: 1881, Modality: RGB and grayscale videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>CryoBench: Diverse and challenging datasets for the heterogeneity problem in cryo-EM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.05526"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ml-struct-bio/CryoBench"><img src="https://img.shields.io/github/stars/ml-struct-bio/CryoBench.svg?style=social&label=Star"></a><br><a href="https://cryobench.cs.princeton.edu"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Princeton University<br>
‚Ä¢ Dataset: IgG-1D, Samples: 100, Modality: Conformational states (atomic models) from a simulated 1D circular motion, and derived synthetic cryo-EM images.<br>
‚Ä¢ Dataset: IgG-RL, Samples: 100, Modality: Conformational states (atomic models) from sampling a flexible peptide linker, and derived synthetic cryo-EM images.<br>
‚Ä¢ Dataset: Spike-MD, Samples: 46789, Modality: Conformational states (atomic models) sampled from a molecular dynamics simulation, and derived synthetic cryo-EM images.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>GesturePrint: Enabling User Identification for mmWave-based Gesture Recognition Systems</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.05358"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: GesturePrint, Samples: 9332, Modality: mmWave radar point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.04631"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vgg-puppetmaster.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Geometry Group, University of Oxford<br>
‚Ä¢ Dataset: Objaverse-Animation, Samples: 16000, Modality: RGB videos + 2D motion trajectories from 3D animations<br>
‚Ä¢ Dataset: Objaverse-Animation-HQ, Samples: 10000, Modality: RGB videos + 2D motion trajectories from 3D animations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Incorporating Spatial Awareness in Data-Driven Gesture Generation for Virtual Agents</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.04127"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/spaces/annadeichler/spatial-gesture"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: Synthetic Spatially-Aware Gesture Dataset, Samples: 1160, Modality: MoCap joints, speech audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>PLANRL: A Motion Planning and Imitation Learning Framework to Bootstrap Reinforcement Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.04054"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: PLANRL Real-World Demonstrations, Samples: 30, Modality: Teleoperated robot trajectories, RGB videos<br>
‚Ä¢ Dataset: PLANRL ModeNet/NavNet Training Data, Samples: 7700, Modality: RGB images with mode and waypoint labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Line-based 6-DoF Object Pose Estimation and Tracking With an Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.03225"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Zibin6/LOPET"><img src="https://img.shields.io/github/stars/Zibin6/LOPET.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Aerospace Science and Engineering, National University of Defense Technology, Changsha 410073, China<br>
‚Ä¢ Dataset: event-based moving object dataset, Samples: 9, Modality: Event streams + 3D pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Compositional Physical Reasoning of Objects and Events from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.02687"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zfchenZFC/ComPhy-PCR"><img src="https://img.shields.io/github/stars/zfchenZFC/ComPhy-PCR.svg?style=social&label=Star"></a><br><a href="https://physicalconceptreasoner.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MIT-IBM Watson AI lab<br>
‚Ä¢ Dataset: ComPhy (Synthetic), Samples: 12000, Modality: Synthetic RGB videos<br>
‚Ä¢ Dataset: ComPhy (Real-world), Samples: 492, Modality: Real-world RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.00762"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/X-niper/UniTalker"><img src="https://img.shields.io/github/stars/X-niper/UniTalker.svg?style=social&label=Star"></a><br><a href="https://github.com/X-niper/UniTalker"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SenseTime Research, China<br>
‚Ä¢ Dataset: A2F-Bench, Samples: 8654, Modality: Audio + 3D facial motion (Vertices, FLAME parameters, ARKit blendshape weights)<br>
‚Ä¢ Dataset: Ours(Faceforensics++), Samples: 1714, Modality: Audio + FLAME parameters<br>
‚Ä¢ Dataset: Ours(Speech), Samples: 789, Modality: Audio + ARKit blendshape weights<br>
‚Ä¢ Dataset: Ours(Song), Samples: 1349, Modality: Audio + ARKit blendshape weights<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>MotionFix: Text-Driven 3D Human Motion Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.00712"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motionfix.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, Germany<br>
‚Ä¢ Dataset: MotionFix, Samples: 6730, Modality: MoCap SMPL parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.00297"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nju-3dv.github.io/projects/EmoTalk3D"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory for Novel Software Technology, Nanjing University, China<br>
‚Ä¢ Dataset: EmoTalk3D, Samples: 600, Modality: Multi-view RGB videos, audio, emotion annotations, per-frame 3D facial shapes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Head360: Learning a Parametric 3D Full-Head for Free-View Synthesis in 360¬∞</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.00296"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nju-3dv.github.io/projects/Head360"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory for Novel Software Technology, Nanjing University, China<br>
‚Ä¢ Dataset: SynHead100, Samples: 5200, Modality: 3D mesh models<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>PEAR: Phrase-Based Hand-Object Interaction Anticipation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.21510"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: EGO-HOIP, Samples: 5000, Modality: RGB images + text phrases + 3D hand trajectories + 3D hand poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Benchmarking Multi-dimensional AIGC Video Quality Assessment: A Dataset and Unified Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.21408"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zczhang-sjtu/UGVQ.git"><img src="https://img.shields.io/github/stars/zczhang-sjtu/UGVQ.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University, China<br>
‚Ä¢ Dataset: Large-scale Generated Video Quality assessment (LGVQ), Samples: 2808, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>MotionCraft: Crafting Whole-Body Motion with Plug-and-Play Multimodal Controls</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.21136"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cure-lab.github.io/MotionCraft"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: MC-Bench, Samples: None, Modality: SMPL-X motion data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>A Dataset for Multi-intensity Continuous Human Activity Recognition through Passive Sensing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.21125"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arghasen10/mmdoppler"><img src="https://img.shields.io/github/stars/arghasen10/mmdoppler.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IIT Kharagpur, India<br>
‚Ä¢ Dataset: mmDoppler, Samples: 75000, Modality: mmWave point cloud, mmWave range-doppler heatmaps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Restoring Real-World Degraded Events Improves Deblurring Quality</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.20502"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yeeesir/DVS_RDNet"><img src="https://img.shields.io/github/stars/Yeeesir/DVS_RDNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Megvii<br>
‚Ä¢ Dataset: DavisMCR, Samples: 100 sequences (over 16,000 pairs of images and events), Modality: blurry/sharp images + events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>XS-VID: An Extremely Small Video Object Detection Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.18137"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gjhhust.github.io/XS-VID/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huazhong University of Science and Technology<br>
‚Ä¢ Dataset: XS-VID, Samples: 38, Modality: RGB videos + bounding box trajectories + motion attributes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.17438"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zhenzhiwang/HumanVid"><img src="https://img.shields.io/github/stars/zhenzhiwang/HumanVid.svg?style=social&label=Star"></a><br><a href="https://humanvid.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: HumanVid (Real), Samples: 20000, Modality: RGB videos + estimated human pose + estimated camera pose<br>
‚Ä¢ Dataset: HumanVid (Synthetic SMPL-X), Samples: 50000, Modality: Rendered videos + ground truth human pose (SMPL-X) + ground truth camera pose<br>
‚Ä¢ Dataset: HumanVid (Synthetic Anime), Samples: 25000, Modality: Rendered videos + ground truth human pose + ground truth camera pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Deep Learning Assisted Inertial Dead Reckoning and Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.16387"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hatter Department of Marine Technologies, Charney School of Marine Sciences, University of Haifa, Israel<br>
‚Ä¢ Dataset: Mobile Robot and Quadrotor Inertial Dataset, Samples: 49, Modality: IMU (accelerometer, gyroscope), RTK-GNSS (ground truth)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Motion Capture from Inertial and Vision Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.16341"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of science and technology of China, Hefei, China<br>
‚Ä¢ Dataset: MINIONS, Samples: 5500000, Modality: IMU signals, RGB videos, 3D SMPL Mesh, 2D/3D joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>SAFNet: Selective Alignment Fusion Network for Efficient HDR Imaging</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.16308"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ltkong218/SAFNet"><img src="https://img.shields.io/github/stars/ltkong218/SAFNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: vivo Mobile Communication Co., Ltd, China<br>
‚Ä¢ Dataset: Challenge123, Samples: 123, Modality: multi-exposure LDR image sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Cluster Haptic Texture Database: Haptic Texture Database with Varied Velocity-Direction Sliding Contacts</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.16206"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cluster-lab/Cluster-Haptic-Texture-Database"><img src="https://img.shields.io/github/stars/cluster-lab/Cluster-Haptic-Texture-Database.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.6084/m9.figshare.29438288"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Cluster Metaverse Lab, 8-9-5 Nishigotanda, Shinagawa, Tokyo, Japan; Graduate School of Comprehensive Human Sciences, University of Tsukuba, 1-2 Kasuga, Tsukuba, Ibaraki, Japan<br>
‚Ä¢ Dataset: Cluster Haptic Texture Database, Samples: 18880, Modality: position (X, Y trajectories), audio, acceleration, force, images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.15708"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/bupt-ai-cz/SwinSF"><img src="https://img.shields.io/github/stars/bupt-ai-cz/SwinSF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing University of Posts and Telecommunications, China<br>
‚Ä¢ Dataset: spike-X4K, Samples: 1245, Modality: spike streams + ground truth images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Hierarchical Windowed Graph Attention Network and a Large Scale Dataset for Isolated Indian Sign Language Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.14224"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cs.rkmvu.ac.in/~isl"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: RKMVERI<br>
‚Ä¢ Dataset: FDMSE-ISL, Samples: 40033, Modality: RGB videos + depth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>PointRegGPT: Boosting 3D Point Cloud Registration using Generative Point-Cloud Pairs for Training</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.14054"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Chen-Suyi/PointRegGPT"><img src="https://img.shields.io/github/stars/Chen-Suyi/PointRegGPT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: PointRegGPT, Samples: 160000, Modality: Point cloud pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>The Effects of Selected Object Features on a Pick-and-Place Task: a Human Multimodal Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.13425"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lindalastrico/objectsManipulationDataset"><img src="https://img.shields.io/github/stars/lindalastrico/objectsManipulationDataset.svg?style=social&label=Star"></a><br><a href="https://www.kaggle.com/dataset/cec218d6597e7c2cac28c7d6a1e8cbd381e451a77192c16b648d2b4c5de70697"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Engine Room, Department of Informatics, Bioengineering, Robotics, and Systems Engineering (DIBRIS), University of Genoa, Italy; Cognitive Architecture for Collaborative Technologies Unit (CONTACT), Italian Institute of Technology, Italy<br>
‚Ä¢ Dataset: The Effects of Selected Object Features on a Pick-and-Place Task: a Human Multimodal Dataset, Samples: 1200, Modality: MoCap markers, IMU, RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>EvSign: Sign Language Recognition and Translation with Streaming Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.12593"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zhang-pengyu/EVSign"><img src="https://img.shields.io/github/stars/zhang-pengyu/EVSign.svg?style=social&label=Star"></a><br><a href="https://zhang-pengyu.github.io/EVSign"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dalian University of Technology<br>
‚Ä¢ Dataset: EvSign, Samples: 6773, Modality: Event streams + RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>SNAIL Radar: A large-scale diverse benchmark for evaluating 4D-radar-based SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.11705"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/snail-radar/dataset_tools"><img src="https://img.shields.io/github/stars/snail-radar/dataset_tools.svg?style=social&label=Star"></a><br><a href="https://snail-radar.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The state key Lab of Information Engineering in Surveying, Mapping And Remote Sensing (LIESMARS), Wuhan University, Hubei, China 430079<br>
‚Ä¢ Dataset: SNAIL Radar, Samples: 44, Modality: 4D Radar, 3D Lidar, Stereo Camera, IMU, GNSS/INS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Animate3D: Animating Any 3D Model with Multi-view Video Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.11398"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://animate3d.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CASIA<br>
‚Ä¢ Dataset: MV-Video, Samples: 115566, Modality: multi-view videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Towards High-Quality 3D Motion Transfer with Realistic Apparel Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.11266"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rongakowang/MMDMC"><img src="https://img.shields.io/github/stars/rongakowang/MMDMC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Australian National University<br>
‚Ä¢ Dataset: MMDMC, Samples: 120, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>High-Quality and Full Bandwidth Seismic Signal Synthesis using Operational GANs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.11040"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OzerCanDevecioglu/High-Quality-and-High-Bandwidth-Seismic-Signal-Synhesis-using-Operational-GANs"><img src="https://img.shields.io/github/stars/OzerCanDevecioglu/High-Quality-and-High-Bandwidth-Seismic-Signal-Synhesis-using-Operational-GANs.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computing Sciences, Tampere University, Tampere, Finland<br>
‚Ä¢ Dataset: SimGM Dataset, Samples: 201, Modality: 1D seismic acceleration signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Motion-prior Contrast Maximization for Dense Continuous-Time Motion Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.10802"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tub-rip/MotionPriorCMax"><img src="https://img.shields.io/github/stars/tub-rip/MotionPriorCMax.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Berlin and SCIoI Excellence Cluster, Berlin, Germany<br>
‚Ä¢ Dataset: EVIMO2 Continuous Flow Dataset, Samples: None, Modality: Event camera data + dense optical flow ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Kinetic Typography Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.10476"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://seonmip.github.io/kinety"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AI Graduate School, GIST, South Korea<br>
‚Ä¢ Dataset: KineTy dataset, Samples: 600000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>SpikeGS: 3D Gaussian Splatting from Spike Streams with High-Speed Camera Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.10062"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: SpikeGS Synthetic Dataset, Samples: 7, Modality: Spike stream + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>LiveHPS++: Robust and Coherent Motion Capture in Dynamic Free Environment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.09833"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dvlab.github.io/project_page/LiveHPS2.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: NoiseMotion, Samples: 1,021,802 human motions, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>IoT-LM: Large Multisensory Language Models for the Internet of Things</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.09801"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Multi-IoT/MultiIoT"><img src="https://img.shields.io/github/stars/Multi-IoT/MultiIoT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: MULTI IOT, Samples: 1150000, Modality: Inertial Measurement Units (IMU), thermal sensors, GPS, LiDAR, gaze, pose, capacitance sensors, images, audio, video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Apprenticeship-Inspired Elegance: Synergistic Knowledge Distillation Empowers Spiking Neural Networks for Efficient Single-Eye Emotion Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.09521"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Key Laboratory of Social Computing and Cognitive Intelligence, Dalian University of Technology<br>
‚Ä¢ Dataset: Diverse Single-eye Event-based Emotion (DSEE), Samples: 6235, Modality: intensity video frames + event data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>AirSketch: Generative Motion to Sketch</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.08906"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hxgr4ce/DoodleFusion"><img src="https://img.shields.io/github/stars/hxgr4ce/DoodleFusion.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Central Florida<br>
‚Ä¢ Dataset: Synthetic Air-Drawing Dataset, Samples: 5000, Modality: RGB videos<br>
‚Ä¢ Dataset: Real Air-Drawing Dataset, Samples: 500, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Robotic Control via Embodied Chain-of-Thought Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.08693"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/m-zawalski/embodied-cot"><img src="https://img.shields.io/github/stars/m-zawalski/embodied-cot.svg?style=social&label=Star"></a><br><a href="https://embodied-cot.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley, University of Warsaw<br>
‚Ä¢ Dataset: Embodied Chain-of-Thought (ECoT) dataset, Samples: more than 2.5M transitions, Modality: Synthetic textual reasoning chains (plans, sub-tasks, bounding boxes, gripper positions, etc.) annotating robot trajectories and image observations from the Bridge v2 dataset.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Infinite Motion: Extended Motion Generation via Long Text Instructions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.08443"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai University, Fudan University<br>
‚Ä¢ Dataset: HumanML3D-Extend, Samples: 35000, Modality: 3D human motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Long-range Turbulence Mitigation: A Large-scale Dataset and A Coarse-to-fine Framework</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.08377"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://shengqi77.github.io/RLR-AT.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Lab of Multispectral Information Intelligent Processing Technology, Huazhong University of Science and Technology, China<br>
‚Ä¢ Dataset: RLR-AT, Samples: 1500, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.06358"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mira-space/MiraData"><img src="https://img.shields.io/github/stars/mira-space/MiraData.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ARC Lab, Tencent PCG; The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: MiraData, Samples: 788000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>TAPVid-3D: A Benchmark for Tracking Any Point in 3D</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.05921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/google-deepmind/tapnet/tree/main/tapvid3d"><img src="https://img.shields.io/github/stars/main/tapvid3d.svg?style=social&label=Star"></a><br><a href="https://tapvid3d.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google DeepMind<br>
‚Ä¢ Dataset: TAPVid-3D, Samples: 4569, Modality: RGB videos + 3D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>WOMD-Reasoning: A Large-Scale Dataset for Interaction Reasoning in Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.04281"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yhli123/WOMD-Reasoning"><img src="https://img.shields.io/github/stars/yhli123/WOMD-Reasoning.svg?style=social&label=Star"></a><br><a href="https://waymo.com/open/download/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: WOMD-Reasoning, Samples: 63000, Modality: Motion trajectories, Textual Q&A, rendered BEV and front-view videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>LiDAR-based Real-Time Object Detection and Tracking in Dynamic Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.04115"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MISTLab/lidar dynamic objects detection.git"><img src="https://img.shields.io/github/stars/MISTLab/lidar dynamic objects detection.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Polytechnique Montreal<br>
‚Ä¢ Dataset: lidar dynamic objects detection, Samples: 4, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>BVI-RLV: A Fully Registered Dataset and Benchmarks for Low-Light Video Enhancement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.03535"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.21227/mzny-8c77"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Information Laboratory, Bristol Vision Institute (BVI), University of Bristol, Bristol, UK BS1 5DD<br>
‚Ä¢ Dataset: BVI-RLV, Samples: 40, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Single Image Rolling Shutter Removal with Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.02906"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lhaippp/RS-Diffusion"><img src="https://img.shields.io/github/stars/lhaippp/RS-Diffusion.svg?style=social&label=Star"></a><br><a href="https://huggingface.co/Lhaippp/RS-Diffusion"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China, Megvii Technology<br>
‚Ä¢ Dataset: RS-Real, Samples: 41000, Modality: Rolling shutter images, Global shutter images, motion fields (from IMU data)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Aligning Human Motion Generation with Human Perceptions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.02272"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MotionCritic/MotionCritic"><img src="https://img.shields.io/github/stars/MotionCritic/MotionCritic.svg?style=social&label=Star"></a><br><a href="https://motioncritic.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center on Frontiers of Computing Studies, School of Compter Science, Peking University<br>
‚Ä¢ Dataset: MotionPercept, Samples: 73040, Modality: SMPL parameters with human preference annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event Stream</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.02174"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wu-cvgl/BeNeRF"><img src="https://img.shields.io/github/stars/wu-cvgl/BeNeRF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Westlake University<br>
‚Ä¢ Dataset: BeNeRF Synthetic Dataset (livingroom, whiteroom, pinkcastle), Samples: 60, Modality: blurry image + event stream<br>
‚Ä¢ Dataset: BeNeRF Synthetic Dataset (tanabata, outdoorpool), Samples: 40, Modality: blurry image + event stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Small Aerial Target Detection for Airborne Infrared Detection Systems using LightGBM and Trajectory Constraints</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.01278"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://small-infrared-aerial-target-detection.grand-challenge.org/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Aerospace Science and Engineering, National University of Defense Technology<br>
‚Ä¢ Dataset: SIATD, Samples: 350, Modality: infrared image sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>CLHOP: Combined Audio-Video Learning for Horse 3D Pose and Shape Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.01244"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH, Sweden<br>
‚Ä¢ Dataset: Outdoor Dataset, Samples: 1604.54 seconds of video, Modality: 4K RGB videos with synchronized audio, 2D keypoints, segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>DaBiT: Depth and Blur informed Transformer for Video Focal Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.01230"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/crispianm/DaBiT"><img src="https://img.shields.io/github/stars/crispianm/DaBiT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, University of Bristol, Bristol, UK<br>
‚Ä¢ Dataset: DAVIS-Blur, Samples: 50, Modality: RGB videos + blur maps + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>A Medical Low-Back Pain Physical Rehabilitation Dataset for Human Body Movement Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.00521"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nguyensmai/KeraalDataset"><img src="https://img.shields.io/github/stars/nguyensmai/KeraalDataset.svg?style=social&label=Star"></a><br><a href="http://nguyensmai.free.fr/KeraalDataset.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IMT Atlantique<br>
‚Ä¢ Dataset: Keraal, Samples: 2622, Modality: RGB videos, Kinect 3D skeleton (positions, orientations), 2D skeleton, Vicon MoCap joints, medical annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>SCOPE: Stochastic Cartographic Occupancy Prediction Engine for Uncertainty-Aware Dynamic Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.00144"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/TempleRAIL/scope"><img src="https://img.shields.io/github/stars/TempleRAIL/scope.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.7051560"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering, Temple University<br>
‚Ä¢ Dataset: OGM-Turtlebot2, Samples: 94891, Modality: robot states (pose, velocity), LiDAR measurements<br>
‚Ä¢ Dataset: OGM-Jackal, Samples: None, Modality: robot states (pose, velocity), LiDAR measurements<br>
‚Ä¢ Dataset: OGM-Spot, Samples: None, Modality: robot states (pose, velocity), LiDAR measurements<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.19353"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://core4d.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: CORE4D-Real, Samples: 1000, Modality: MoCap (human mesh, object pose), allocentric RGB-D videos, egocentric RGB videos, 2D segmentations<br>
‚Ä¢ Dataset: CORE4D-Synthetic, Samples: 10000, Modality: human mesh, object pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>AddBiomechanics Dataset: Capturing the Physics of Human Motion at Scale</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.18537"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="addbiomechanics.org/download_data.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: AddBiomechanics Dataset 1.0, Samples: 24 million+ frames, Modality: Optical marker locations, ground reaction forces/moments, joint kinematics, joint torques<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Human-Aware 3D Scene Generation with Spatially-constrained Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.18159"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Hong-xl/SHADE"><img src="https://img.shields.io/github/stars/Hong-xl/SHADE.svg?style=social&label=Star"></a><br><a href="https://hong-xl.github.io/SHADE"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Wuhan University<br>
‚Ä¢ Dataset: Calibrated 3D FRONT HUMAN, Samples: 11225, Modality: 3D human motion sequences within 3D scenes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Speech2UnifiedExpressions: Synchronous Synthesis of Co-Speech Affective Face and Body Expressions from Affordable Inputs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.18068"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UttaranB127/speech2unified_expressions"><img src="https://img.shields.io/github/stars/UttaranB127/speech2unified_expressions.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Adobe Inc.<br>
‚Ä¢ Dataset: TED Gesture+Face Dataset, Samples: 253186, Modality: 3D face landmarks, 3D body pose joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>PVUW 2024 Challenge on Complex Video Understanding: Methods and Results</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.17005"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://henghuiding.github.io/MOSE/ChallengeCVPR2024"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CVPR 2024 PVUW Workshop & Challenge organizers<br>
‚Ä¢ Dataset: MOSE, Samples: None, Modality: RGB videos + segmentation masks<br>
‚Ä¢ Dataset: MeViS, Samples: None, Modality: RGB videos + segmentation masks + text expressions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>From Perfect to Noisy World Simulation: Customizable Embodied Multi-modal Perturbations for SLAM Robustness Benchmarking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.16850"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Xiaohao-Xu/SLAM-under-Perturbation"><img src="https://img.shields.io/github/stars/Xiaohao-Xu/SLAM-under-Perturbation.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Michigan, Ann Arbor<br>
‚Ä¢ Dataset: Noisy-Replica, Samples: 1000, Modality: RGB-D video sequences + 6D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Rendering and Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.16038"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://livescenes.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: OmniSim, Samples: 20, Modality: RGBD images, camera poses, object masks, interaction variables, text prompts<br>
‚Ä¢ Dataset: InterReal, Samples: 8, Modality: RGB videos, camera poses, object masks, interaction variables, text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Multimodal Segmentation for Vocal Tract Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.15754"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rishiraij/multimodal-mri-avatar"><img src="https://img.shields.io/github/stars/rishiraij/multimodal-mri-avatar.svg?style=social&label=Star"></a><br><a href="https://rishiraij.github.io/multimodal-mri-avatar/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Berkeley<br>
‚Ä¢ Dataset: Speech MRI Open Dataset (labels), Samples: 75 speakers, Modality: RT-MRI video + 95 articulator point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and Smartphone Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.14498"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/BASHLab/LLaSA"><img src="https://img.shields.io/github/stars/BASHLab/LLaSA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Worcester Polytechnic Institute<br>
‚Ä¢ Dataset: SensorCaps, Samples: 35960, Modality: IMU sequences (accelerometer, gyroscope) + natural language captions<br>
‚Ä¢ Dataset: OpenSQA, Samples: 179727, Modality: IMU sequences (accelerometer, gyroscope) + question-answer pairs<br>
‚Ä¢ Dataset: Tune-OpenSQA, Samples: 19440, Modality: IMU sequences (accelerometer, gyroscope) + question-answer pairs<br>
‚Ä¢ Dataset: User-collected smartphone IMU dataset, Samples: None, Modality: IMU data (accelerometer, gyroscope)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Video Generation with Learned Action Prior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.14436"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://meenakshisarkar.github.io/Motion-Prediction-and-Planning/dataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Indian Institute of Science<br>
‚Ä¢ Dataset: RoAM, Samples: 307200, Modality: Stereo RGB videos + robot actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.14412"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford UK<br>
‚Ä¢ Dataset: 3DDogs-Lab, Samples: 143, Modality: MoCap joints + RGBD videos + IMUs + pressure mat<br>
‚Ä¢ Dataset: 3DDogs-Wild, Samples: 286, Modality: RGB videos + 3D poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>ViDSOD-100: A New Dataset and a Baseline Model for RGB-D Video Salient Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.12536"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: ViDSOD-100, Samples: 100, Modality: RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Holistic-Motion2D: Scalable Whole-body Human Motion Generation in 2D Space</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.11253"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://holistic-motion2d.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: Holistic-Motion2D, Samples: 1002463, Modality: 2D whole-body keypoints + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Nymeria: A Massive Collection of Multimodal Egocentric Daily Motion in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.09905"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.projectaria.com/datasets/nymeria"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta Reality Labs Research<br>
‚Ä¢ Dataset: Nymeria, Samples: 1200, Modality: Full-body motion (IMU suit), egocentric multimodal data (RGB/grayscale/eye-tracking videos, IMUs, magnetometer, barometer), third-person video, 3D scene point clouds, language descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>LAVIB: A Large-scale Video Interpolation Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.09754"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/alexandrosstergiou/LAVIB"><img src="https://img.shields.io/github/stars/alexandrosstergiou/LAVIB.svg?style=social&label=Star"></a><br><a href="https://alexandrosstergiou.github.io/datasets/LAVIB"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Twente, NL<br>
‚Ä¢ Dataset: LA VIB, Samples: 283484, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Introducing HOT3D: An Egocentric Dataset for 3D Hand and Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.09598"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://facebookresearch.github.io/hot3d"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta Reality Labs<br>
‚Ä¢ Dataset: HOT3D, Samples: 425, Modality: Multi-view RGB/monochrome videos, MoCap-based 3D hand/object/camera poses, 3D mesh models, scene point clouds, eye gaze<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.09326"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/agnJason/PianoMotion10M"><img src="https://img.shields.io/github/stars/agnJason/PianoMotion10M.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: PianoMotion10M, Samples: 10527167, Modality: RGB videos + 3D MANO hand poses + audio + MIDI<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.08858"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://omni.human2humanoid.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: OmniH2O-6, Samples: 6 tasks (40 minutes of demonstrations), Modality: RGBD images, motion goals (head and hands), joint targets<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Skim then Focus: Integrating Contextual and Fine-grained Views for Repetitive Action Counting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.08814"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huazhong University of Science and Technology, Wuhan, China.<br>
‚Ä¢ Dataset: Multi-RepCount, Samples: 984, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Vivid-ZOO: Multi-View Video Generation with Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.08659"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vividzoo.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: King Abdullah University of Science and Technology<br>
‚Ä¢ Dataset: MV-VideoNet, Samples: 14271, Modality: multi-view RGB videos + camera poses + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>From Sim-to-Real: Toward General Event-based Low-light Frame Interpolation with Per-scene Optimization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.08090"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://openimaginglab.github.io/Sim2Real/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University, China and Shanghai AI Laboratory, China<br>
‚Ä¢ Dataset: EVFI-LL, Samples: >20 sequences, Modality: RGB videos + Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Triple-domain Feature Learning with Frequency-aware Memory Enhancement for Moving Infrared Small Target Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.06949"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UESTC-nnLab/Tridos"><img src="https://img.shields.io/github/stars/UESTC-nnLab/Tridos.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China<br>
‚Ä¢ Dataset: ITSDT-15K, Samples: 60, Modality: infrared videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>MOSA: Music Motion with Semantic Annotation Dataset for Cross-Modal Music Processing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.06375"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset"><img src="https://img.shields.io/github/stars/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Science, Academia Sinica, Taiwan.<br>
‚Ä¢ Dataset: MOSA (Music mOtion with Semantic Annotation), Samples: 742, Modality: 3-D motion capture data, audio recordings, semantic annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>iMotion-LLM: Motion Prediction Instruction Tuning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.06211"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAUST<br>
‚Ä¢ Dataset: InstructWaymo, Samples: 327391, Modality: Vectorized motion/map data + Textual instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Diving Deep into the Motion Representation of Video-Text Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.05075"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chinmayad/motiondescriptions.git"><img src="https://img.shields.io/github/stars/chinmayad/motiondescriptions.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: GPT-4 Generated Motion Descriptions, Samples: 552, Modality: textual motion descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>SMART: Scene-motion-aware human action recognition framework for mental disorder group</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.04649"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Inowlzy/SMART.git"><img src="https://img.shields.io/github/stars/Inowlzy/SMART.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Key Laboratory of Navigation and Location-based Services, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai 200240, China<br>
‚Ä¢ Dataset: MentalHAD, Samples: 69, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>LipGER: Visually-Conditioned Generative Error Correction for Robust Automatic Speech Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.04432"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Sreyan88/LipGER"><img src="https://img.shields.io/github/stars/Sreyan88/LipGER.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park, USA<br>
‚Ä¢ Dataset: LipHyp, Samples: 601000, Modality: videos of lip motion + ASR hypothesis lists + text transcriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Pi-fusion: Physics-informed diffusion model for learning fluid dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.03711"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SIAT-SIH/fluid"><img src="https://img.shields.io/github/stars/SIAT-SIH/fluid.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: 2D flow past a circular cylinder, Samples: 1, Modality: Fluid dynamics simulation (velocity and pressure fields)<br>
‚Ä¢ Dataset: 3D blood flow in realistic hepatic portal vein, Samples: 1, Modality: Fluid dynamics simulation (velocity and pressure fields)<br>
‚Ä¢ Dataset: 3D blood flow in brain artery, Samples: 1, Modality: Fluid dynamics simulation (velocity and pressure fields)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Event3DGS: Event-Based 3D Gaussian Splatting for High-Speed Robot Egomotion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.02972"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/txiong23/Event3DGS"><img src="https://img.shields.io/github/stars/txiong23/Event3DGS.svg?style=social&label=Star"></a><br><a href="https://txiong23.github.io/Event3DGS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: Event3DGS real-world scenes, Samples: 6, Modality: RGB videos + camera poses + emulated event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>DeNVeR: Deformable Neural Vessel Representations for Unsupervised Video Vessel Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.01591"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="kirito878.github.io/DeNVeR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Yang Ming Chiao Tung University<br>
‚Ä¢ Dataset: XACV, Samples: 111, Modality: X-ray angiography videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Virtual avatar generation models as world navigators</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.01056"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://virtual-avatar-generation.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SABR<br>
‚Ä¢ Dataset: NAV-22M, Samples: 22000000, Modality: RGB videos + 3D pose tracks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>MotionLLM: Understanding Human Behaviors from Human Motions and Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.20340"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lhchen.top/MotionLLM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: MoVid, Samples: 496000, Modality: SMPL sequences, RGB videos, Text (captions and QA pairs)<br>
‚Ä¢ Dataset: MoVid-Bench, Samples: 1350, Modality: SMPL sequences, RGB videos, Text (QA pairs)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.20336"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vis-www.cs.umass.edu/RapVerse"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Massachusetts Amherst<br>
‚Ä¢ Dataset: RapVerse, Samples: 26.8 hours, Modality: 3D holistic body meshes (SMPL-X), vocals, lyrics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>SMPLX-Lite: A Realistic and Drivable Avatar Benchmark with Rich Geometry and Texture Annotations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.19609"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen International Graduate School, Tsinghua University<br>
‚Ä¢ Dataset: SMPLX-Lite, Samples: over 20k frames, Modality: multi-view RGB videos, 3D keypoints, 3D textured scanned meshes, SMPLX-Lite-D parametric models<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Towards Open Domain Text-Driven Synthesis of Multi-Person Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.18483"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Washington<br>
‚Ä¢ Dataset: LAION-Pose, Samples: 8000000, Modality: 3D Poses (SMPL) + Text<br>
‚Ä¢ Dataset: WebVid-Motion, Samples: 3500, Modality: 3D Motion Sequences (SMPL) + Text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Human4DiT: 360-degree Human Video Generation with 4D Diffusion Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.17405"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: Human4DiT-3D, Samples: 5000, Modality: 3D human scans + SMPL<br>
‚Ä¢ Dataset: Human4DiT-Video, Samples: 10000, Modality: RGB videos + SMPL<br>
‚Ä¢ Dataset: Human4DiT-4D, Samples: 100, Modality: Animatable 3D human models + SMPL<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Hawk: Learning to Understand Open-World Video Anomalies</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16886"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jqtangust/hawk"><img src="https://img.shields.io/github/stars/jqtangust/hawk.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: HAWK Video Anomaly Dataset, Samples: 8000, Modality: RGB videos + textual descriptions + QA pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16868"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ispc-lab/RCDN"><img src="https://img.shields.io/github/stars/ispc-lab/RCDN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tongji University<br>
‚Ä¢ Dataset: OPV2V-N, Samples: 6138, Modality: Multi-agent RGB videos + optical flow + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Diffusion4D: Fast Spatial-temporal Consistent 4D Generation via Video Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16645"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VITA-Group/Diffusion4D"><img src="https://img.shields.io/github/stars/VITA-Group/Diffusion4D.svg?style=social&label=Star"></a><br><a href="https://vita-group.github.io/Diffusion4D"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto<br>
‚Ä¢ Dataset: Curated dynamic 3D asset dataset from Objaverse, Samples: 54000, Modality: animated 3D models<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>MCGMapper: Light-Weight Incremental Structure from Motion and Visual Localization With Planar Markers and Camera Groups</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16599"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xieyuser/MCGMapper"><img src="https://img.shields.io/github/stars/xieyuser/MCGMapper.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: MCGMapper Synthetic Dataset, Samples: 6, Modality: Synthetic RGB images + Ground-truth pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Flow Snapshot Neurons in Action: Deep Neural Networks Generalize to Biological Motion Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16493"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computing and Data Science, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: Biological Motion Perception (BMP) Dataset, Samples: 62656, Modality: RGB videos, point-light display videos (Joint videos, Sequential position actor videos)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Multi-Agent Inverse Reinforcement Learning in Real World Unstructured Pedestrian Crowds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16439"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mairl-uva/mairl_uva.github.io"><img src="https://img.shields.io/github/stars/mairl-uva/mairl_uva.github.io.svg?style=social&label=Star"></a><br><a href="https://mairl-uva.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Virginia<br>
‚Ä¢ Dataset: Speedway, Samples: 500, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Video Prediction Models as General Visual Encoders</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16382"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/anonymous_VPT-5E85/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: Custom annotated BAIR Robot Pushing segmentation masks, Samples: 250, Modality: RGB frames + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.15758"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wangyuchi369.github.io/InstructAvatar/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: InstructAvatar's Instruction-Video Paired Dataset, Samples: over 60000, Modality: RGB videos + text instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Motion Segmentation for Neuromorphic Aerial Surveillance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.15209"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://samiarja.github.io/evairborne/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: International Center for Neuromorphic Systems, Western Sydney University<br>
‚Ä¢ Dataset: Ev-Airborne, Samples: 9, Modality: event camera data + ground truth annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Drones Help Drones: A Collaborative Framework for Multi-Drone Object Trajectory Prediction and Beyond</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.14674"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WangzcBruce/DHD"><img src="https://img.shields.io/github/stars/WangzcBruce/DHD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Key Laboratory of Network Information System Technology, Aerospace Information Research Institute, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: Air-Co-Pred, Samples: 200, Modality: Simulated multi-view RGB videos + 3D bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Event-based dataset for the detection and classification of manufacturing assembly tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.14626"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Robotics-and-AI/DAVIS-data-capture-system"><img src="https://img.shields.io/github/stars/Robotics-and-AI/DAVIS-data-capture-system.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/10562563"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Centre for Mechanical Engineering, Materials and Processes (CEMMPRE), ARISE, University of Coimbra, 3030 -788, Coimbra, Portugal<br>
‚Ä¢ Dataset: Event-based Dataset of Assembly Tasks (EDAT24), Samples: 400, Modality: Event camera data (events, greyscale frames)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>S3O: A Dual-Phase Approach for Reconstructing Dynamic Shape and Skeleton of Articulated Objects from Single Monocular Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.12607"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/haoz19/LIMR"><img src="https://img.shields.io/github/stars/haoz19/LIMR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, USA<br>
‚Ä¢ Dataset: PlanetZoo, Samples: None, Modality: RGB videos + 2D keypoints + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>InterAct: Capture and Modelling of Realistic, Expressive and Interactive Activities between Two Persons in Daily Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.11690"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hku-cg.github.io/interact"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: InterAct, Samples: 241, Modality: MoCap body motions, 3D facial meshes, audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.11286"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://steve-zeyu-zhang.github.io/MotionAvatar"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: La Trobe University; The Australian National University<br>
‚Ä¢ Dataset: Zoo-300K, Samples: 300000, Modality: text-motion pairs<br>
‚Ä¢ Dataset: Avatar Q&A Dataset, Samples: 1756, Modality: text-pairs (instruction/output)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>SignAvatar: Sign Language 3D Motion Reconstruction and Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.07974"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dongludeeplearning.github.io/SignAvatar.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Engineering, University at Buffalo, NY , USA<br>
‚Ä¢ Dataset: ASL3DWord, Samples: 1547, Modality: SMPL-X parameters (3D joint rotation)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Event-based Structure-from-Orbit</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.06216"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/0thane/eSfO"><img src="https://img.shields.io/github/stars/0thane/eSfO.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.10884693"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Adelaide<br>
‚Ä¢ Dataset: TOPSPIN, Samples: 72, Modality: event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Rotation Initialization and Stepwise Refinement for Universal LiDAR Calibration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.05589"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yjsx/ULC"><img src="https://img.shields.io/github/stars/yjsx/ULC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, University of Science and Technology of China, Hefei, 230026, China<br>
‚Ä¢ Dataset: COLORFUL, Samples: 120 seconds at 20Hz, Modality: LiDAR<br>
‚Ä¢ Dataset: Campus-SS, Samples: None, Modality: LiDAR<br>
‚Ä¢ Dataset: Campus-BS, Samples: None, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Audio Matters Too! Enhancing Markerless Motion Capture with Audio Signals for String Performance Capture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.04963"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yitongishere/string_performance"><img src="https://img.shields.io/github/stars/Yitongishere/string_performance.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Central Conservatory of Music, China and Tsinghua University, China<br>
‚Ä¢ Dataset: String Performance Dataset (SPD), Samples: 120, Modality: 3D MoCap annotations, multi-view RGB videos, audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>BILTS: A Bi-Invariant Similarity Measure for Robust Object Trajectory Recognition under Reference Frame Variations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.04392"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.12806232"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering, KU Leuven, Leuven, Belgium<br>
‚Ä¢ Dataset: Synthetic (SYN) rigid-body motion dataset, Samples: None, Modality: Synthetically generated rigid-body pose trajectories<br>
‚Ä¢ Dataset: adapted DLA 1, Samples: None, Modality: MoCap-derived pose trajectories<br>
‚Ä¢ Dataset: adapted DLA 2, Samples: None, Modality: MoCap-derived pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment and Spatially-variant Deformation Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.04309"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://npucvr.github.io/TSM-NRSfM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronics and Information, Northwestern Polytechnical University<br>
‚Ä¢ Dataset: H3WB-NRSfM, Samples: 5, Modality: 3D keypoint trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Exposing AI-generated Videos: A Benchmark Dataset and a Local-and-Global Temporal Defect Based Detection Method</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.04133"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Cyber Science and Engineering, Sichuan University<br>
‚Ä¢ Dataset: AI-generated Video Benchmark Dataset, Samples: 5000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>MoDiPO: text-to-motion alignment via AI-feedback-driven Direct Preference Optimization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.03803"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sapienza University of Rome<br>
‚Ä¢ Dataset: Pick-a-Move, Samples: not specified, Modality: MoCap pose vectors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Elevator, Escalator, or Neither? Classifying Conveyor State Using Smartphone under Arbitrary Pedestrian Behavior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.03218"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong<br>
‚Ä¢ Dataset: ELESON Dataset (not explicitly named), Samples: 36420, Modality: Smartphone INS (accelerometer, gyroscope, magnetometer)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Panoptic-SLAM: Visual SLAM in Dynamic Environments using Panoptic Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.02177"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/iit-DLSLab/Panoptic-SLAM"><img src="https://img.shields.io/github/stars/iit-DLSLab/Panoptic-SLAM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dynamic Legged Systems Lab, Istituto Italiano di Tecnologia, Italy; Department of Mechanical Engineering at the Pontifical Catholic University of Rio de Janeiro, Brazil<br>
‚Ä¢ Dataset: Indoor Quadruped SLAM Dataset, Samples: 3, Modality: RGB-D video + Vicon ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>IFNet: Deep Imaging and Focusing for Handheld SAR with Millimeter-wave Signals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.02023"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Cyber Science and Technology, University of Science and Technology of China<br>
‚Ä¢ Dataset: Handheld SAR imaging dataset, Samples: 200, Modality: Pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Zero-Shot Monocular Motion Segmentation in the Wild by Combining Deep Learning with Geometric Motion Model Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.01723"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Vision and Image Processing Lab, University of Waterloo<br>
‚Ä¢ Dataset: KT3DInsMoSeg, Samples: None, Modality: RGB videos + point trajectories + pixel-level segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Towards Real-World HDR Video Reconstruction: A Large-Scale Benchmark Dataset and A Two-Stage Alignment Network</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.00244"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yungsyu99/Real-HDRV"><img src="https://img.shields.io/github/stars/yungsyu99/Real-HDRV.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai University, China<br>
‚Ä¢ Dataset: Real-HDRV, Samples: 500, Modality: LDR/HDR video pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Ultra Inertial Poser: Scalable Motion Capture and Tracking from Sparse Inertial Sensors and Ultra-Wideband Ranging</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.19541"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/eth-siplab/UltraInertialPoser"><img src="https://img.shields.io/github/stars/eth-siplab/UltraInertialPoser.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, ETH Z√ºrich, Switzerland<br>
‚Ä¢ Dataset: UIP-DB, Samples: 25 motion types from 10 participants, totaling 200 minutes of data, Modality: 6-DoF IMU signals, UWB measurements and distances, SMPL parameters, Optical motion capture<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>MoST: Multi-modality Scene Tokenization for Motion Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.19531"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Waymo LLC<br>
‚Ä¢ Dataset: Waymo Open Motion Dataset (WOMD) [Augmented with Camera Embeddings], Samples: None, Modality: LiDAR, 3D bounding box tracks, road graph, traffic signals, camera embeddings<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Towards Real-world Video Face Restoration: A New Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.19500"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZiyannChen/VFR-FOS-Benchmark"><img src="https://img.shields.io/github/stars/ZiyannChen/VFR-FOS-Benchmark.svg?style=social&label=Star"></a><br><a href="https://ziyannchen.github.io/projects/VFRxBenchmark/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: FOS-V, Samples: 3316, Modality: RGB videos<br>
‚Ä¢ Dataset: FOS-real, Samples: 4253, Modality: RGB images<br>
‚Ä¢ Dataset: FOS-syn, Samples: 3150, Modality: RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.19110"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Imperial College London<br>
‚Ä¢ Dataset: FEED (Facial Extreme Emotions Dataset), Samples: 520, Modality: multi-view videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>4D-DRESS: A 4D Dataset of Real-world Human Clothing with Semantic Annotations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.18630"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ait.ethz.ch/4d-dress"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, ETH Z ¬®urich<br>
‚Ä¢ Dataset: 4D-DRESS, Samples: 520, Modality: 4D textured scans, garment meshes, SMPL(-X) body models, multi-view images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>LIKO: LiDAR, Inertial, and Kinematic Odometry for Bipedal Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.18047"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Mr-Zqr/LIKO"><img src="https://img.shields.io/github/stars/Mr-Zqr/LIKO.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Mechatronical Engineering, Beijing Institute of Technology (BIT), Beijing, China<br>
‚Ä¢ Dataset: LIKO biped robot dataset, Samples: 5, Modality: LiDAR, Inertial Measurement Unit (IMU), joint encoders, Force/Torque (F/T) sensors, motion capture ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Retrieval Robust to Object Motion Blur</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.18025"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Rong-Zou/Retrieval-Robust-to-Object-Motion-Blur"><img src="https://img.shields.io/github/stars/Rong-Zou/Retrieval-Robust-to-Object-Motion-Blur.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich<br>
‚Ä¢ Dataset: Synthetic Blurred Object Retrieval Dataset, Samples: 245760, Modality: 3D object models + motion trajectories -> RGB images<br>
‚Ä¢ Dataset: Real-world Blurred Object Retrieval Dataset, Samples: 139, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.17063"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hilab-open-source/wheelpose"><img src="https://img.shields.io/github/stars/hilab-open-source/wheelpose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Los Angeles<br>
‚Ä¢ Dataset: WheelPose Motion Sequences, Samples: 200, Modality: 3D joint trajectories<br>
‚Ä¢ Dataset: Wheelchair User Testing Dataset, Samples: 2464, Modality: RGB images + 2D keypoint annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Motor Focus: Fast Ego-Motion Prediction for Assistive Visual Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.17031"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arazi2/VisionGPT"><img src="https://img.shields.io/github/stars/arazi2/VisionGPT.svg?style=social&label=Star"></a><br><a href="https://arazi2.github.io/aisends.github.io/project/VisionGPT"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computing, Clemson University<br>
‚Ä¢ Dataset: Visual Navigation Dataset, Samples: 50, Modality: RGB videos + moving direction annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Learning Visuotactile Skills with Two Multifingered Hands</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.16823"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Toru-Lin/HATO"><img src="https://img.shields.io/github/stars/Toru-Lin/HATO.svg?style=social&label=Star"></a><br><a href="https://toru-lin.github.io/hato/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Berkeley<br>
‚Ä¢ Dataset: HATO teleoperation dataset, Samples: 800, Modality: robot kinematics, RGB-D videos, tactile sensor readings, control actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>You Think, You ACT: The New Task of Arbitrary Text to Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.14745"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Engineering Research Center for Multimedia Software, Wuhan University; School of Computer Science, Wuhan University<br>
‚Ä¢ Dataset: HUMAN ML3D++, Samples: 15000, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Non-Uniform Exposure Imaging via Neuromorphic Shutter Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.13972"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University<br>
‚Ä¢ Dataset: Neuromorphic Exposure Dataset (NED), Samples: 51, Modality: RGB frames + events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>HOIST-Former: Hand-held Objects Identification, Segmentation, and Tracking in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.13819"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://supreethn.github.io/research/hoistformer/index.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stony Brook University, USA<br>
‚Ä¢ Dataset: HOIST, Samples: 4228, Modality: RGB videos + bounding boxes + segmentation masks + tracking IDs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>A Dataset and Model for Realistic License Plate Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.13677"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/haoyGONG/LPDGAN"><img src="https://img.shields.io/github/stars/haoyGONG/LPDGAN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of AI and Advanced Computing, Xi‚Äôan Jiaotong-Liverpool University<br>
‚Ä¢ Dataset: LPBlur, Samples: 10288, Modality: Paired sharp and motion-blurred RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>MLP: Motion Label Prior for Temporal Sentence Localization in Untrimmed 3D Human Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.13657"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/eanson023/mlp"><img src="https://img.shields.io/github/stars/eanson023/mlp.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence, Chongqing University of Technology, Chongqing 401120, China<br>
‚Ä¢ Dataset: HumanML3D (Restore), Samples: 5784, Modality: 3D human poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Turb-Seg-Res: A Segment-then-Restore Pipeline for Dynamic Videos with Atmospheric Turbulence</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.13605"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/riponcs/TurbSegRes"><img src="https://img.shields.io/github/stars/riponcs/TurbSegRes.svg?style=social&label=Star"></a><br><a href="https://riponcs.github.io/TurbSegRes"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Arizona State University<br>
‚Ä¢ Dataset: Augmented URG-T Dataset, Samples: None, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>ConCLVD: Controllable Chinese Landscape Video Generation via Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.12903"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Artificial Intelligence, Xiamen University<br>
‚Ä¢ Dataset: CLV-HD, Samples: 1300, Modality: RGB videos + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Seeing Motion at Nighttime with an Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.11884"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Liu-haoyue/NER-Net"><img src="https://img.shields.io/github/stars/Liu-haoyue/NER-Net.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Lab of Multispectral Information Intelligent Processing Technology School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, China<br>
‚Ä¢ Dataset: RLED, Samples: 64200, Modality: event streams + RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Text-controlled Motion Mamba: Text-Instructed Temporal Grounding of Human Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.11375"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: BABEL-Grounding, Samples: 5339, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>VBR: A Vision Benchmark in Rome</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.11322"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.rvp-group.net/datasets/slam"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer, Control, and Management Engineering ‚ÄúAntonio Ruberti‚Äù, Sapienza University of Rome, Italy<br>
‚Ä¢ Dataset: VBR (Vision Benchmark in Rome), Samples: 6, Modality: RGB, LiDAR, IMU, GPS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Neuromorphic Vision-based Motion Segmentation with Graph Transformer Neural Network</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.10940"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yusra-alkendi/EMS-GTNN"><img src="https://img.shields.io/github/stars/Yusra-alkendi/EMS-GTNN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Propulsion and Space Research Center (PSRC) at the Technology Innovation Institute (TII), Abu Dhabi, UAE<br>
‚Ä¢ Dataset: EMS-DOMEL (Event dataset for Motion Segmentation), Samples: 9, Modality: Event streams + motion segmentation labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Generating Human Interaction Motions in Scenes with Text Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.10685"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://research.nvidia.com/labs/toronto-ai/tesmo"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA<br>
‚Ä¢ Dataset: Loco-3D-FRONT, Samples: 9500 walking motion sequences (resulting in 95k locomotion-scene pairs), Modality: MoCap joints + text descriptions + 3D scenes<br>
‚Ä¢ Dataset: SAMP (extended), Samples: approx. 16000 sub-sequences (derived from 80 original sequences), Modality: MoCap joints + text descriptions + 3D objects<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.09748"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://garageworld.letsgoproject.com/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University, China and Stereye Intelligent Technology Co.,Ltd., China<br>
‚Ä¢ Dataset: GarageWorld, Samples: 8, Modality: LiDAR point clouds, IMU data, RGB fisheye images, pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Exploring Text-to-Motion Generation with Human Preference</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.09445"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: Text-to-Motion Preference Pairs, Samples: 3528, Modality: Text prompts + tokenized motion pairs + human preference labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.08640"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/EventEgo3D/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Informatics, SIC; Saarland University, SIC<br>
‚Ä¢ Dataset: EE3D-S, Samples: 946 motion sequences, Modality: Event streams + 3D human poses + segmentation masks<br>
‚Ä¢ Dataset: EE3D-R, Samples: 464000 poses, Modality: Event streams + 3D human poses + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>FusionPortableV2: A Unified Multi-Sensor Dataset for Generalized SLAM Across Diverse Platforms and Scalable Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.08563"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fusionportable.github.io/dataset/fusionportable_v2"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China.<br>
‚Ä¢ Dataset: FusionPortableV2, Samples: 27, Modality: IMU, stereo frame cameras, stereo event cameras, 3D LiDAR, INS, wheel encoders, legged robot sensors, ground-truth trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Context-aware Video Anomaly Detection in Long-Term Datasets</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.07887"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Rensselaer Polytechnic Institute<br>
‚Ä¢ Dataset: WF dataset, Samples: 2000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.07569"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mh0797/interPlan"><img src="https://img.shields.io/github/stars/mh0797/interPlan.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of T√ºbingen<br>
‚Ä¢ Dataset: interPlan, Samples: 80, Modality: Simulator scenarios based on augmented nuPlan data (agent bounding boxes, maps, navigation goals)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.06710"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University<br>
‚Ä¢ Dataset: RGB & Spike 3D (RS-3D), Samples: 240 sequences (6 scenes x 40 viewpoints), Modality: RGB videos + Spike streams + Camera Poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Multi-agent Long-term 3D Human Pose Forecasting via Interaction-aware Trajectory Conditioning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.05218"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Jaewoo97/T2P"><img src="https://img.shields.io/github/stars/Jaewoo97/T2P.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST<br>
‚Ä¢ Dataset: JRDB-GlobMultiPose (JRDB-GMP), Samples: 5746, Modality: 3D human pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.05014"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pku-yuangroup.github.io/MagicTime"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Graduate School, Peking University<br>
‚Ä¢ Dataset: ChronoMagic, Samples: 2265, Modality: RGB videos + text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>NeRF2Points: Large-Scale Point Cloud Generation From Street Views' Radiance Field Optimization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.04875"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ruqi Mobility Inc., Guangzhou, China<br>
‚Ä¢ Dataset: None, Samples: 180000, Modality: RGB images, camera poses, LiDAR point clouds, depth maps, normal vectors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>CEAR: Comprehensive Event Camera Dataset for Rapid Perception of Agile Quadruped Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.04698"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://daroslab.github.io/cear/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Manning College of Information & Computer Sciences, University of Massachusetts Amherst, MA, USA<br>
‚Ä¢ Dataset: CEAR, Samples: 106, Modality: Event camera, RGB-D, LiDAR, IMU, Joint encoders, MoCap ground-truth pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>HDR Imaging for Dynamic Scenes with Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.03210"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lxp-whu/Self-EHDRI"><img src="https://img.shields.io/github/stars/lxp-whu/Self-EHDRI.svg?style=social&label=Star"></a><br><a href="https://lxp-whu.github.io/Self-EHDRI"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University<br>
‚Ä¢ Dataset: BL2SHD, Samples: 120, Modality: blurry LDR images, event streams, sharp LDR images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Event-assisted Low-Light Video Object Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.01945"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HebeiFast/EventLowLightVOS"><img src="https://img.shields.io/github/stars/HebeiFast/EventLowLightVOS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: LLE-DAVIS, Samples: 90, Modality: low-light videos + event streams<br>
‚Ä¢ Dataset: LLE-VOS, Samples: 70, Modality: normal-light videos + low-light videos + event streams + segmentation annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Large Motion Model for Unified Multi-Modal Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.01284"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mingyuan-zhang.github.io/projects/LMM.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: MotionVerse, Samples: 320000, Modality: Unified motion representation (positions, velocities, rotations) consolidated from 16 existing datasets (SMPL, SMPL-X, 3D keypoints).<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Gyro-based Neural Single Image Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.00916"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: POSTECH CSE<br>
‚Ä¢ Dataset: GyroBlur-Synth, Samples: 15240, Modality: RGB images + gyroscope data<br>
‚Ä¢ Dataset: GyroBlur-Real, Samples: 117, Modality: RGB images + gyroscope data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.00562"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JunukCha/Text2HOI"><img src="https://img.shields.io/github/stars/JunukCha/Text2HOI.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UNIST<br>
‚Ä¢ Dataset: H2O (extended with text prompts), Samples: 660, Modality: hand-object mesh sequences + text prompts<br>
‚Ä¢ Dataset: GRAB (extended with text prompts), Samples: 1335, Modality: hand-object mesh sequences + text prompts<br>
‚Ä¢ Dataset: ARCTIC (extended with text prompts), Samples: 4597, Modality: hand-object mesh sequences + text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Choreographing the Digital Canvas: A Machine Learning Approach to Artistic Performance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.00054"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: falling pose dataset, Samples: 150, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>A Unified Framework for Human-centric Point Cloud Video Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.20031"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Human Body Segmentation Synthetic Dataset (LiDARPart-Human), Samples: 1000000, Modality: synthetic LiDAR point clouds + body part labels<br>
‚Ä¢ Dataset: Human Motion Flow Synthetic Dataset (LiDARFlow-Human), Samples: 2378871, Modality: synthetic LiDAR point clouds + motion flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>SceneTracker: Long-term Scene Flow Estimation Network</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.19924"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wwsource/SceneTracker"><img src="https://img.shields.io/github/stars/wwsource/SceneTracker.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China<br>
‚Ä¢ Dataset: LSFOdyssey, Samples: 127527, Modality: RGB-D videos + 3D trajectories<br>
‚Ä¢ Dataset: LSFDriving, Samples: 180, Modality: RGB-D videos + 3D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.19501"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.lidarhumanmotion.net/reli11d/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University<br>
‚Ä¢ Dataset: RELI11D, Samples: 48, Modality: LiDAR point clouds, IMU, RGB videos, Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Beyond Talking -- Generating Holistic 3D Human Dyadic Motion for Communication</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.19467"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua Shenzhen International Graduate School, China<br>
‚Ä¢ Dataset: HoCo, Samples: 22913, Modality: RGB videos + audio + text transcripts + SMPL-X parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.19417"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://oakink.net/v2"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: OAKINK2, Samples: 627, Modality: Multi-view RGB videos + MoCap-derived 3D poses (human body, hands, objects)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Within the Dynamic Context: Inertia-aware 3D Human Modeling with Pose Sequence</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.19160"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Ytong-Chen/Dyco-release"><img src="https://img.shields.io/github/stars/Ytong-Chen/Dyco-release.svg?style=social&label=Star"></a><br><a href="https://ai4sports.opengvlab.com/Dyco"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Artificial Intelligence Laboratory<br>
‚Ä¢ Dataset: I3D-Human, Samples: 6, Modality: Multi-view RGB videos + SMPL poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>EgoNav: Egocentric Scene-aware Human Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.19026"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: Egocentric Navigation Dataset, Samples: 220000, Modality: 6-DoF torso pose, leg joint angles, torso velocities, gait frequency, RGBD images, semantic segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.18820"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vcai.mpi-inf.mpg.de/projects/MetaCap/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: WildDynaCap, Samples: 2, Modality: Multi-view RGB videos + skeletal motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.18811"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lisiyao21/Duolando"><img src="https://img.shields.io/github/stars/lisiyao21/Duolando.svg?style=social&label=Star"></a><br><a href="https://lisiyao21.github.io/projects/Duolando"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: DD100, Samples: 100, Modality: MoCap (SMPL-X format with inertial glove data)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Tracking-Assisted Object Detection with Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.18330"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tkyen1110/TEDNet"><img src="https://img.shields.io/github/stars/tkyen1110/TEDNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Taiwan University<br>
‚Ä¢ Dataset: 1 Megapixel Automotive Detection Dataset (Cleaned with Visibility Labels), Samples: 15.65 hours of recordings, Modality: Event camera data + bounding boxes with visibility labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.17936"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, SIC<br>
‚Ä¢ Dataset: DND GROUP GESTURE, Samples: 2.7M poses, Modality: markerless MoCap (3D body and hand poses)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.17827"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://diffh2o.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH, Switzerland and Meta, Switzerland<br>
‚Ä¢ Dataset: GRAB dataset (extended with detailed textual descriptions), Samples: 1335, Modality: Textual descriptions for MoCap sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.17610"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://metaverse-ai-lab-thu.github.io/MMVP-Dataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beihang University<br>
‚Ä¢ Dataset: MMVP, Samples: 44000, Modality: RGBD video + pressure sensor data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Benchmarking Video Frame Interpolation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.17128"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Darmstadt, hessian.AI<br>
‚Ä¢ Dataset: Ours, Samples: 666, Modality: Synthetic RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>TAIL: A Terrain-Aware Multi-Modal SLAM Dataset for Robot Locomotion in Deformable Granular Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.16875"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tailrobot.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Key Laboratory of Biomimetic Robotics and Intelligent Systems, Department of Mechanical and Energy Engineering, Southern University of Science and Technology (SUSTech), Shenzhen, 518055, China<br>
‚Ä¢ Dataset: TAIL, Samples: 14, Modality: Stereo cameras, RGB-D cameras, 3D LiDAR, IMU, RTK, Robot odometry/kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.16425"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gokulbnr/fast-slow-biased-event-vpr"><img src="https://img.shields.io/github/stars/gokulbnr/fast-slow-biased-event-vpr.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Centre for Robotics, Faculty of Engineering, Queensland University of Technology, Brisbane, QLD Australia 4000<br>
‚Ä¢ Dataset: QCR-Fast-and-Slow-Dataset, Samples: 366, Modality: event streams, conventional image data, ground truth robot poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.16169"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Takiee/Gaze-HOI"><img src="https://img.shields.io/github/stars/Takiee/Gaze-HOI.svg?style=social&label=Star"></a><br><a href="https://takiee.github.io/gaze-hoi/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the paper<br>
‚Ä¢ Dataset: GazeHOI, Samples: 1378, Modality: 3D hand poses (MANO), 6D object poses, 3D gaze points, multi-view RGB videos, egocentric RGBD videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.16080"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pku-dymvhumans.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University Shenzhen Graduate School, Peng Cheng Laboratory<br>
‚Ä¢ Dataset: PKU-DyMVHumans, Samples: 2668, Modality: Multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Contact-aware Human Motion Generation from Textual Descriptions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.15709"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://xymsh.github.io/RICH-CAT"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Sydney, Australia<br>
‚Ä¢ Dataset: RICH-CAT, Samples: 8566, Modality: SMPL-X MoCap joints + contact labels + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>DragAPart: Learning a Part-Level Motion Prior for Articulated Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.15382"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="dragapart.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Geometry Group, University of Oxford<br>
‚Ä¢ Dataset: Drag-a-Move, Samples: 40000000, Modality: Synthetic RGB image pairs + drag annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.14781"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fudan-generative-vision.github.io/champ"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: in-the-wild human video dataset, Samples: 5000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Explorative Inbetweening of Time and Space</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.14611"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://time-reversal.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems<br>
‚Ä¢ Dataset: Bounded Generation Dataset, Samples: 395, Modality: RGB image pairs and static images (some with ground-truth videos)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.13667"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Carmenw1203/DanceCamera3D-Official"><img src="https://img.shields.io/github/stars/Carmenw1203/DanceCamera3D-Official.svg?style=social&label=Star"></a><br><a href="https://github.com/Carmenw1203/DanceCamera3D-Official"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China<br>
‚Ä¢ Dataset: DCM, Samples: 108, Modality: Paired 3D dance motion (60 joints), 3D camera movement, and music audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Motion Generation from Fine-grained Textual Descriptions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.13518"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/KunhangL/finemotiondiffuse"><img src="https://img.shields.io/github/stars/KunhangL/finemotiondiffuse.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University, The University of Tokyo<br>
‚Ä¢ Dataset: FineHumanML3D, Samples: 29228, Modality: 3D human motions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>LaserHuman: Language-guided Scene-aware Human Motion Generation in Free Environment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.13307"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/4DVLab/LaserHuman"><img src="https://img.shields.io/github/stars/4DVLab/LaserHuman.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: LaserHuman, Samples: 3374, Modality: SMPL, RGB-D, text, Point Cloud, Scene-Map<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Map-Aware Human Pose Prediction for Robot Follow-Ahead</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.13294"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://qingyuan-jiang.github.io/iros2024_poseForecasting/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Minnesota<br>
‚Ä¢ Dataset: Real Indoor Motion (Real-IM), Samples: 12, Modality: RGB-D video + 3D skeleton poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>WHAC: World-grounded Humans and Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.12959"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SenseTime Research, The University of Tokyo<br>
‚Ä¢ Dataset: WHAC-A-Mole, Samples: 2434 sequences, Modality: Synthetic rendered RGB videos with SMPL-X and camera pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>EmoVOCA: Speech-Driven Emotional 3D Talking Heads</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.12886"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/miccunifi/EmoVOCA"><img src="https://img.shields.io/github/stars/miccunifi/EmoVOCA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Florence, Italy<br>
‚Ä¢ Dataset: EmoVOCAv1, Samples: 7200, Modality: 3D face mesh sequences<br>
‚Ä¢ Dataset: EmoVOCAv2, Samples: 15840, Modality: 3D face mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>ReGenNet: Towards Human Action-Reaction Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.11882"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/liangxuy/ReGenNet"><img src="https://img.shields.io/github/stars/liangxuy/ReGenNet.svg?style=social&label=Star"></a><br><a href="https://liangxuy.github.io/ReGenNet/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China; Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China<br>
‚Ä¢ Dataset: NTU120-AS, Samples: 8118, Modality: SMPL-X<br>
‚Ä¢ Dataset: InterHuman-AS, Samples: 6022, Modality: SMPL<br>
‚Ä¢ Dataset: Chi3D-AS, Samples: 373, Modality: SMPL-X<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>FE-DeTr: Keypoint Detection and Tracking in Low-quality Image Frames with Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.11662"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yuyangpoi/FE-DeTr"><img src="https://img.shields.io/github/stars/yuyangpoi/FE-DeTr.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: EIS, Wuhan University, Wuhan, China<br>
‚Ä¢ Dataset: Extreme Corner, Samples: 32, Modality: RGB frames + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures for Human Avatar Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.11589"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://alex-jyj.github.io/UV-Gaussians/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua Shenzhen International Graduate School, Tsinghua University<br>
‚Ä¢ Dataset: unnamed human motion dataset, Samples: 5, Modality: multi-view images, scanned models, SMPL-X model parameters, SMPLX-D model parameters, texture maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>FORCE: Physics-aware Human-object Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.11237"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://virtualhumans.mpi-inf.mpg.de/force/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: T¬®ubingen AI Center, University of T ¬®ubingen; Max Planck Institute for Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: FORCE, Samples: 450, Modality: MoCap (4x RGB-D cameras + 17x IMUs) yielding human (SMPL) and object poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>THOR: Text to Human-Object Interaction Diffusion via Relation Intervention</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.11208"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Text-BEHAVE, Samples: 2377, Modality: SMPL-H poses + object 6D poses + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>NetTrack: Tracking Highly Dynamic Objects with a Net</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.11186"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/george-zhuang/NetTrack"><img src="https://img.shields.io/github/stars/george-zhuang/NetTrack.svg?style=social&label=Star"></a><br><a href="https://george-zhuang.github.io/nettrack"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: Bird Flock Tracking (BFT), Samples: 106, Modality: RGB videos + tracking annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Reconfigurable Robot Identification from Motion Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.10496"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/H-Y-H-Y-H/meta_selfmodeling_id"><img src="https://img.shields.io/github/stars/H-Y-H-Y-H/meta_selfmodeling_id.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Columbia University<br>
‚Ä¢ Dataset: 12-DoF Reconfigurable Legged Robot Motion Dataset, Samples: 200000, Modality: proprioceptive robot states (body position, orientation, joint angles) and actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>CPGA: Coding Priors-Guided Aggregation Network for Compressed Video Quality Enhancement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.10362"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VQE-CPGA/CPGA.git"><img src="https://img.shields.io/github/stars/VQE-CPGA/CPGA.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: VideoCoding Priors (VCP), Samples: 300, Modality: RGB videos + motion vectors + predictive frames + residual frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>RID-TWIN: An end-to-end pipeline for automatic face de-identification in videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.10058"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AnirbanMukherjeeXD/RID-Twin"><img src="https://img.shields.io/github/stars/AnirbanMukherjeeXD/RID-Twin.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: International Institute of Information Technology, Bangalore<br>
‚Ä¢ Dataset: Proposed Custom Dataset, Samples: None, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Interactive Distance Field Mapping and Planning to Enable Human-Robot Collaboration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.09988"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UTS-RI/IDMP"><img src="https://img.shields.io/github/stars/UTS-RI/IDMP.svg?style=social&label=Star"></a><br><a href="https://uts-ri.github.io/IDMP"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Robotics (CERI) at the Technical University of Applied Sciences W√ºrzburg-Schweinfurt (THWS), Germany<br>
‚Ä¢ Dataset: Custom synthetic dataset featuring a dynamically moving ball on a table, Samples: None, Modality: simulated RGB-D camera<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.09850"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jiayi-wu-umd/MARVIS"><img src="https://img.shields.io/github/stars/jiayi-wu-umd/MARVIS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Maryland Robotics Center (MRC), University of Maryland, College Park, MD 20742, USA<br>
‚Ä¢ Dataset: MARVIS Synthetic Dataset, Samples: 3012, Modality: RGB video frame pairs + real/virtual masks<br>
‚Ä¢ Dataset: MARVIS Real Dataset, Samples: 450, Modality: Stereo RGB video frame pairs + real/virtual masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>GenAD: Generalized Predictive Model for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.09630"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OpenDriveLab/DriveAGI"><img src="https://img.shields.io/github/stars/OpenDriveLab/DriveAGI.svg?style=social&label=Star"></a><br><a href="https://github.com/OpenDriveLab/DriveAGI"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: OpenDriveLab and Shanghai AI Lab<br>
‚Ä¢ Dataset: OpenDV-2K, Samples: 65000000, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.09486"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chenkang455/S-SDM"><img src="https://img.shields.io/github/stars/chenkang455/S-SDM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University<br>
‚Ä¢ Dataset: RSB (Real-world Spike-guided Blur), Samples: 10, Modality: blurry RGB images + spike streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>LM2D: Lyrics- and Music-Driven Dance Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.09407"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://youtu.be/4XCgvYookvA"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: unnamed, Samples: 1867, Modality: 3D human motion (SMPL parameters) + music + lyrics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>TH√ñR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.09285"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tmralmeida/thor-magni-tools"><img src="https://img.shields.io/github/stars/tmralmeida/thor-magni-tools.svg?style=social&label=Star"></a><br><a href="http://thor.oru.se/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ¬®Orebro University, Sweden<br>
‚Ä¢ Dataset: TH¬®OR-MAGNI, Samples: 52, Modality: Motion capture (position, velocity, head orientation), eye tracking (gaze), 3D LiDAR, RGB-D camera, RGB camera<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Caltech Aerial RGB-Thermal Dataset in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.08997"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/aerorobotics/caltech-aerial-rgbt-dataset"><img src="https://img.shields.io/github/stars/aerorobotics/caltech-aerial-rgbt-dataset.svg?style=social&label=Star"></a><br><a href="https://github.com/aerorobotics/caltech-aerial-rgbt-dataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: California Institute of Technology<br>
‚Ä¢ Dataset: Caltech Aerial RGB-Thermal Dataset, Samples: 37, Modality: RGB video, Thermal video, GPS, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.08764"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://enriccorona.github.io/vlogger/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google Research<br>
‚Ä¢ Dataset: MENTOR, Samples: 800000, Modality: RGB videos + audio + 3D pose and expression annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Scaling Up Dynamic Human-Scene Interaction Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.08629"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jnnan.github.io/trumans/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for AI, Peking University; National Key Lab of General AI, BIGAI<br>
‚Ä¢ Dataset: TRUMANS, Samples: 1.6 million frames, Modality: MoCap, SMPL-X, RGBD videos, segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.08268"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://follow-your-click.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: HKUST<br>
‚Ä¢ Dataset: WebVid-Motion, Samples: Not specified; derived by filtering and re-annotating the WebVid-10M dataset, Modality: RGB videos + short motion prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>NeRF-Supervised Feature Point Detection and Description</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.08156"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/aliyoussef1/SiLK-PrP"><img src="https://img.shields.io/github/stars/aliyoussef1/SiLK-PrP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science University College London<br>
‚Ä¢ Dataset: NeRF-synthesised multi-view dataset, Samples: 10000, Modality: Synthetic RGB images + depth maps + camera intrinsics/extrinsics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>CMax-SLAM: Event-based Rotational-Motion Bundle Adjustment and SLAM System using Contrast Maximization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.08119"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tub-rip/cmax_slam"><img src="https://img.shields.io/github/stars/tub-rip/cmax_slam.svg?style=social&label=Star"></a><br><a href="https://github.com/tub-rip/cmax_slam"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Electrical Engineering and Computer Science of TU Berlin, Berlin, Germany<br>
‚Ä¢ Dataset: Self-recorded rotational motion dataset with VGA-resolution event camera, Samples: 10, Modality: Events + IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.07436"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Lab of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China<br>
‚Ä¢ Dataset: various-illumination event (VIE) dataset, Samples: , Modality: paired RGB-event data with manually handcrafted moving object bounding box labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic Architecture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.07347"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Jiafei127/FD4MM"><img src="https://img.shields.io/github/stars/Jiafei127/FD4MM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Information Engineering, Hefei University of Technology, China<br>
‚Ä¢ Dataset: Synthetic Motion Magnification Dataset, Samples: 10, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Physics Sensor Based Deep Learning Fall Detection System</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.06994"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WuShaoa/SensorDataClassification-TCN/tree/main/SeqClassifyCNN"><img src="https://img.shields.io/github/stars/main/SeqClassifyCNN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer, Northwestern Polytechnical University, Xi‚Äôan, Shaanxi 710072, China.<br>
‚Ä¢ Dataset: Our data, Samples: 26, Modality: Foot-worn IMU (accelerometer, gyroscope, orientation) and pressure sensor data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Real-Time Simulated Avatar from Head-Mounted Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.06862"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zhengyiluo.github.io/SimXR/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Reality Labs Research, Meta; Carnegie Mellon University<br>
‚Ä¢ Dataset: Synthetic Quest 2 Dataset, Samples: 5169 sequences (2,216,000 frames), Modality: Synthetic monochrome images (4 views), 6DoF headset poses, ground-truth MoCap poses<br>
‚Ä¢ Dataset: Real-world Quest 2 Dataset, Samples: 10 sequences (40,000 frames), Modality: Real-world monochrome SLAM images, 6DoF headset poses, third-person images for pseudo-ground truth pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Eliminating Warping Shakes for Unsupervised Online Video Stitching</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.06378"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nie-lang/StabStitch"><img src="https://img.shields.io/github/stars/nie-lang/StabStitch.svg?style=social&label=Star"></a><br><a href="https://www.youtube.com/watch?v=03kGEZJHxzI"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Science, Beijing Jiaotong University; Beijing Key Laboratory of Advanced Information Science and Network<br>
‚Ä¢ Dataset: StabStitch-D, Samples: 100+ video pairs, Modality: RGB video pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Audio-Synchronized Visual Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.05659"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lzhangbj.github.io/projects/asva/asva.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Wisconsin-Madison<br>
‚Ä¢ Dataset: AVSync15, Samples: 1500, Modality: RGB videos + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Pix2Gif: Motion-Guided Diffusion for GIF Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.04634"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hiteshk03.github.io/Pix2Gif/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Microsoft Research, India<br>
‚Ä¢ Dataset: Curated TGIF for Pix2Gif, Samples: 78692, Modality: RGB image pairs + text captions + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Out of the Room: Generalizing Event-Based Dynamic Motion Segmentation for Complex Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.04562"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rwn17/DSEC-MOTS"><img src="https://img.shields.io/github/stars/rwn17/DSEC-MOTS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huawei Technologies, Zurich Research Center<br>
‚Ä¢ Dataset: DSEC-MOTS, Samples: None, Modality: Event camera data + motion segmentation masks + tracking annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>HMD-Poser: On-Device Real-time Human Motion Tracking from Scalable Sparse Observations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.03561"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: PICO, ByteDance<br>
‚Ä¢ Dataset: free-dancing motion dataset, Samples: 74, Modality: HMD sensor data, IMU sensor data, ground-truth SMPL parameters from OptiTrack<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Biomechanical Comparison of Human Walking Locomotion on Solid Ground and Sand</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.03105"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://dx.doi.org/10.17632/jgdpjrf584.2"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Rutgers, The State University of New Jersey<br>
‚Ä¢ Dataset: Biomechanical Comparison of Human Walking Locomotion on Solid Ground and Sand, Samples: , Modality: MoCap markers, Ground Reaction Forces (GRF)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>RT-H: Action Hierarchies Using Language</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.01823"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="rt-hierarchy.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google DeepMind, Stanford University<br>
‚Ä¢ Dataset: Diverse, Samples: 30000, Modality: Robot demonstrations (visual observations, proprioception, actions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>6DoF SELD: Sound Event Localization and Detection Using Microphones and Motion Tracking Sensors on self-motioning human</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.01670"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nttrd-mdlab/6dof-seld"><img src="https://img.shields.io/github/stars/nttrd-mdlab/6dof-seld.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NTT Corporation, Japan<br>
‚Ä¢ Dataset: 6DoF SELD Dataset, Samples: None, Modality: Audio + Motion tracker (head position, posture)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Kick Back & Relax++: Scaling Beyond Ground-Truth Depth with SlowTV & CribsTV</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.01569"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jspenmar/slowtv_monodepth"><img src="https://img.shields.io/github/stars/jspenmar/slowtv_monodepth.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CVSSP, University of Surrey<br>
‚Ä¢ Dataset: SlowTV, Samples: 1700000, Modality: RGB videos<br>
‚Ä¢ Dataset: CribsTV, Samples: 330000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Tri-Modal Motion Retrieval by Learning a Joint Embedding Space</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.00691"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lavimo2023.github.io/User-Study-LAVIMO/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: HumanML3D (augmented), Samples: 14616, Modality: MoCap + Text + Rendered RGB videos<br>
‚Ä¢ Dataset: KIT-ML (augmented), Samples: 3911, Modality: MoCap + Text + Rendered RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>CustomListener: Text-guided Responsive Interaction for User-friendly Listening Head Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.00274"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://customlistener.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Vision AI Department, Meituan<br>
‚Ä¢ Dataset: Text-annotated ViCo, Samples: None, Modality: RGB videos, 3DMM coefficients, text annotations, speech transcripts<br>
‚Ä¢ Dataset: Text-annotated RealTalk, Samples: None, Modality: RGB videos, 3DMM coefficients, text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>Detection of Micromobility Vehicles in Urban Traffic Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.18503"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sabrikhalil/Micro Mobility Detection"><img src="https://img.shields.io/github/stars/sabrikhalil/Micro Mobility Detection.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Polytechnique Montr√©al<br>
‚Ä¢ Dataset: PolyMMV, Samples: 105, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.17171"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: FreeMotion, Samples: 578,775 frames, Modality: LiDAR point clouds, RGB images, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>XAI-based gait analysis of patients walking with Knee-Ankle-Foot orthosis using video cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.16175"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://tinyurl.com/5ds5f33c"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of EEE, BITS Pilani, K K Birla, Goa Campus, 403726 Goa, India<br>
‚Ä¢ Dataset: KAFO user Dataset, Samples: None, Modality: RGB videos + MoCap ground truth + metadata<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>RealDex: Towards Human-like Grasping for Robotic Dexterous Hand</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.13853"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dvlab.github.io/RealDex"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University, The University of Hong Kong<br>
‚Ä¢ Dataset: RealDex, Samples: 2630, Modality: Robotic hand kinematics, RGB-D images, point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>3D Kinematics Estimation from Video with a Biomechanical Model and Synthetic Training Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.13172"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Vision Lab, Department of Intelligent Systems, Delft University of Technology<br>
‚Ä¢ Dataset: ODAH (OpenSim Driven Animated Human), Samples: 1132, Modality: Synthetic RGB videos + OpenSim kinematics (joint angles, body segment scales)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>PIP-Net: Pedestrian Intention Prediction in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.12810"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Transport Studies, Computer Vision and Machine Learning Group, University of Leeds<br>
‚Ä¢ Dataset: Urban-PIP, Samples: 1481, Modality: multi-camera videos, LiDAR, Radar, IMU<br>
‚Ä¢ Dataset: Frontal Urban-PIP, Samples: 184, Modality: single front-camera videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>Event-Based Motion Magnification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.11957"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://openimaginglab.github.io/emm/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory, The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: SM-ERGB (Sub-pixel Motion Event-RGB dataset), Samples: 10007, Modality: RGB videos + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>DivaTrack: Diverse Bodies and Motions from Acceleration-Enhanced Three-Point Trackers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.09211"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea Advanced Institue of Science and Technology<br>
‚Ä¢ Dataset: DivaTrack, Samples: 772 motion clips, Modality: IMU signals + MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>Sophia-in-Audition: Virtual Production with a Robot Performer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.06978"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University, LumiAni Technology<br>
‚Ä¢ Dataset: Sophia-in-Audition (SiA) dataset, Samples: 50, Modality: multi-view RGB videos, facial motion capture (MoCap) data, HDR environment maps, robot motor parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>Advancing Video Anomaly Detection: A Concise Review and a New Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.04857"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Australian National University<br>
‚Ä¢ Dataset: Multi-Scenario Anomaly Detection (MSAD), Samples: 720, Modality: RGB<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.04519"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://biodrone.aitestunion.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China.<br>
‚Ä¢ Dataset: BioDrone, Samples: 600, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>Capturing the Unseen: Vision-Free Facial Motion Capture Using Inertial Measurement Units</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.03944"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: IMU-ARKit dataset, Samples: 20 participants, Modality: IMU signals, RGB videos, audio, ARKit parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>Advancing Location-Invariant and Device-Agnostic Motion Activity Recognition on Wearable Devices</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.03714"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="www.to-be-added.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Apple, USA<br>
‚Ä¢ Dataset: MotionPrint, Samples: 408 million accelerometer samples, Modality: 3D IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>MSPM: A Multi-Site Physiological Monitoring Dataset for Remote Pulse, Respiration, and Blood Pressure Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.02224"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Notre Dame<br>
‚Ä¢ Dataset: MSPM, Samples: 103, Modality: multi-view RGB videos + NIR video + physiological signals (PPG, BP, SpO2)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>BVI-Lowlight: Fully Registered Benchmark Dataset for Low-Light Video Enhancement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.01970"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.21227/mzny-8c77"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Information Laboratory
University of Bristol<br>
‚Ä¢ Dataset: BVI-Lowlight, Samples: 40, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>MUSTAN: Multi-scale Temporal Context as Attention for Robust Video Foreground Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.00918"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AI-CoE, Jio Platforms Limited (JPL), India<br>
‚Ä¢ Dataset: Indoor Surveillance Dataset (ISD), Samples: 150538 frames, Modality: RGB videos + foreground masks + depth maps + normal maps + instance semantic maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>REACT: Two Datasets for Analyzing Both Human Reactions and Evaluative Feedback to Robots Over Time</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.00190"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yale-img/react"><img src="https://img.shields.io/github/stars/yale-img/react.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Yale University<br>
‚Ä¢ Dataset: REACT-Nao, Samples: 432, Modality: RGB videos + Head pose + Gaze trajectories + Facial landmarks + Facial Action Units (AUs) + Robot game actions<br>
‚Ä¢ Dataset: REACT-Shutter, Samples: 240, Modality: RGB videos + Head pose + Gaze trajectories + Facial landmarks + Facial Action Units (AUs) + Robot pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>Physical Priors Augmented Event-Based 3D Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.17121"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Mercerai/PAEv3d"><img src="https://img.shields.io/github/stars/Mercerai/PAEv3d.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MICS Thrust, HKUST(GZ)<br>
‚Ä¢ Dataset: PAEv3d Dataset, Samples: 101 objects, Modality: event streams, RGB images, depth maps, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.15348"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Lab of CAD&CG, Zhejiang University<br>
‚Ä¢ Dataset: AniDress Dataset, Samples: 12, Modality: Multi-view RGB videos + garment masks + 3D body poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>Dataset and Benchmark: Novel Sensors for Autonomous Vehicle Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.13853"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/umautobots/nsavp_tools"><img src="https://img.shields.io/github/stars/umautobots/nsavp_tools.svg?style=social&label=Star"></a><br><a href="https://umautobots.github.io/nsavp"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics Department, University of Michigan, Ann Arbor, MI 48109 USA<br>
‚Ä¢ Dataset: Novel Sensors for Autonomous Vehicle Perception (NSAVP), Samples: 10, Modality: 6-DoF ground truth poses, stereo event cameras, stereo thermal cameras, stereo monochrome cameras, stereo RGB cameras<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>Learning Dynamics from Multicellular Graphs with Deep Neural Networks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.12196"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/GuoLab-CellMechanics/GNN-collective-cell-dynamics"><img src="https://img.shields.io/github/stars/GuoLab-CellMechanics/GNN-collective-cell-dynamics.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.13988939"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering, Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA<br>
‚Ä¢ Dataset: MCF-10A cell monolayer dataset, Samples: 240, Modality: Time-lapsed microscopy videos of cell monolayers<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.10232"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jlogkim.github.io/parahome"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Seoul National University<br>
‚Ä¢ Dataset: ParaHome, Samples: 208 captures, Modality: Multi-view RGB, IMU-based motion capture (body suit, gloves), 3D parametric models (human SMPL-X, objects), text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>TUMTraf Event: Calibration and Fusion Resulting in a Dataset for Roadside Event-Based and RGB Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.08474"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://innovation-mobility.com/tumtraf-dataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Chair of Robotics, Artificial Intelligence and Real-time Systems, TUM School of Computation, Information and Technology, Technical University of Munich, Munich, Germany<br>
‚Ä¢ Dataset: TUMTraf Event Dataset, Samples: 4111, Modality: Event-based camera images, RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.08399"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Tsinghua-MARS-Lab/TACO"><img src="https://img.shields.io/github/stars/Tsinghua-MARS-Lab/TACO.svg?style=social&label=Star"></a><br><a href="https://taco2024.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: TACO, Samples: 2500, Modality: Multi-view RGB videos, Egocentric RGBD videos, 3D hand-object meshes, Motion capture data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.06578"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://akaneqwq.github.io/360DVD/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Computer Engineering, Peking University; Peking University Shenzhen Graduate School-Rabbitpre AIGC Joint Research Laboratory<br>
‚Ä¢ Dataset: WEB360, Samples: 2114, Modality: ERP videos + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>Big Data and Deep Learning in Smart Cities: A Comprehensive Dataset for AI-Driven Traffic Accident Detection and Computer Vision Systems</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.03587"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of IT, University of Cincinnati<br>
‚Ä¢ Dataset: Traffic Accident Detection Video Dataset, Samples: 5700, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>An Event-Oriented Diffusion-Refinement Method for Sparse Events Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.03153"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Automation, Tsinghua University<br>
‚Ä¢ Dataset: Self-Captured Dataset, Samples: 21355, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.01885"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Codec Avatars Lab, Meta, Pittsburgh; University of California, Berkeley<br>
‚Ä¢ Dataset: Photorealistic conversational dataset, Samples: 8 hours, Modality: multi-view video, audio, precomputed joint angles, face expression codes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.01505"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HopLee6/Sports-QA"><img src="https://img.shields.io/github/stars/HopLee6/Sports-QA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computing and Information Systems, University of Melbourne.<br>
‚Ä¢ Dataset: Sports-QA, Samples: 5967, Modality: RGB videos + textual QA pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>Indoor Obstacle Discovery on Reflective Ground via Monocular Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.01445"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XuefengBUPT/IndoorObstacleDiscovery-RG"><img src="https://img.shields.io/github/stars/XuefengBUPT/IndoorObstacleDiscovery-RG.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Beijing University of Posts and Telecommunications, Beijing 100876, China<br>
‚Ä¢ Dataset: Obstacle on Reflective Ground (ORG), Samples: 223, Modality: RGB videos + Robot odometry/poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>3D Human Pose Perception from Egocentric Stereo Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.00889"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/UnrealEgo2/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, SIC<br>
‚Ä¢ Dataset: UnrealEgo2, Samples: 15207, Modality: Egocentric stereo fisheye videos + 3D joint poses + depth maps<br>
‚Ä¢ Dataset: UnrealEgo-RW, Samples: 591, Modality: Egocentric stereo fisheye videos + 3D joint poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.00374"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pantomatrix.github.io/EMAGE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo<br>
‚Ä¢ Dataset: BEAT2 (BEAT-SMPLX-FLAME), Samples: 1762, Modality: MoCap (SMPL-X body and FLAME head parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>iKUN: Speak to Trackers without Retraining</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.16245"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dyhBUPT/iKUN"><img src="https://img.shields.io/github/stars/dyhBUPT/iKUN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The school of Artificial Intelligence, Beijing University of Posts and Telecommunications<br>
‚Ä¢ Dataset: Refer-Dance, Samples: 65, Modality: RGB videos + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Inter-X: Towards Versatile Human-Human Interaction Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.16051"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://liangxuy.github.io/inter-x/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University, Eastern Institute of Technology, Ningbo<br>
‚Ä¢ Dataset: Inter-X, Samples: 11388, Modality: MoCap, SMPL-X parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Get a Grip: Reconstructing Hand-Object Stable Grasps in Egocentric Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.15719"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zhifanzhu/Get-a-Grip"><img src="https://img.shields.io/github/stars/zhifanzhu/Get-a-Grip.svg?style=social&label=Star"></a><br><a href="https://zhifanzhu.github.io/getagrip"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, University of Bristol, UK<br>
‚Ä¢ Dataset: EPIC-Grasps, Samples: 2431, Modality: Egocentric RGB videos + 2D segmentation masks + stable grasp temporal annotations<br>
‚Ä¢ Dataset: ARCTIC-Grasps, Samples: 1303, Modality: RGB videos + 3D hand/object meshes + stable grasp temporal annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.15004"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mingyuan-zhang/FineMoGen"><img src="https://img.shields.io/github/stars/mingyuan-zhang/FineMoGen.svg?style=social&label=Star"></a><br><a href="https://mingyuan-zhang.github.io/projects/FineMoGen.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: HuMMan-MoGen, Samples: 2968, Modality: SMPL parameters (6D rotation representation + root translation) with fine-grained spatio-temporal text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>MACS: Mass Conditioned 3D Hand and Object Motion Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.14929"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vcai.mpi-inf.mpg.de/projects/MACS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Informatics, SIC, VIA Research Center<br>
‚Ä¢ Dataset: Mass-conditioned 3D hand and object motion dataset (unnamed), Samples: 110000 frames, Modality: 3D reconstructed hand (GHUM model) and object motions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>3D Pose Estimation of Two Interacting Hands from a Monocular Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.14157"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/Ev2Hands/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Informatics, SIC; Saarland University, SIC<br>
‚Ä¢ Dataset: Ev2Hands-S, Samples: None, Modality: Synthetic event streams + 3D hand pose annotations + segmentation labels<br>
‚Ä¢ Dataset: Ev2Hands-R, Samples: 8, Modality: Event streams + RGB videos + 3D hand pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Deep Hybrid Camera Deblurring for Smartphone Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.13317"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cg.postech.ac.kr/research/HCDeblur"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: POSTECH<br>
‚Ä¢ Dataset: HCBlur, Samples: 9039, Modality: RGB images + burst RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Relightable and Animatable Neural Avatars from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.12877"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wenbin-lin/RelightableAvatar"><img src="https://img.shields.io/github/stars/wenbin-lin/RelightableAvatar.svg?style=social&label=Star"></a><br><a href="https://wenbin-lin.github.io/RelightableAvatar-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software and BNRist, Tsinghua University<br>
‚Ä¢ Dataset: Synthetic Relightable Avatar Dataset, Samples: 4, Modality: multi-view RGB videos + 3D geometry + material + lighting<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>MotionScript: Natural Language Descriptions for Expressive 3D Human Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.12634"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/pjyazdian/MotionScript"><img src="https://img.shields.io/github/stars/pjyazdian/MotionScript.svg?style=social&label=Star"></a><br><a href="https://pjyazdian.github.io/MotionScript"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computing Science, Simon Fraser University, Burnaby, BC, Canada<br>
‚Ä¢ Dataset: MotionScript Captions for HumanML3D, Samples: 1461, Modality: MoCap joints + structured text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>EVI-SAM: Robust, Real-time, Tightly-coupled Event-Visual-Inertial State Estimation and 3D Dense Mapping</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.11911"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arclab-hku/Event based VO-VIO-SLAM"><img src="https://img.shields.io/github/stars/arclab-hku/Event based VO-VIO-SLAM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: EVI-SAM Handheld Dataset, Samples: 6, Modality: monocular event camera (DAVIS346: events, grayscale frames, IMU), RGB-D camera (Intel D455)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Towards Effective Multi-Moving-Camera Tracking: A New Dataset and Lightweight Link Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.11035"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dhu-mmct.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Donghua University<br>
‚Ä¢ Dataset: MMCT, Samples: 32, Modality: RGB videos + pedestrian trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Global-Local MAV Detection under Challenging Conditions based on Appearance and Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.11008"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WestlakeIntelligentRobotics/GLAD"><img src="https://img.shields.io/github/stars/WestlakeIntelligentRobotics/GLAD.svg?style=social&label=Star"></a><br><a href="https://youtu.be/Tv473mAzHbU"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology at Zhejiang University and the School of Engineering at Westlake University, Hangzhou, China.<br>
‚Ä¢ Dataset: ARD-MAV, Samples: 60, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.10877"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zeqing-wang.github.io/Mimic/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: South China University of Technology, X-ERA.ai<br>
‚Ä¢ Dataset: 3D-HDTF, Samples: 220, Modality: audio-mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Multi-level Reasoning for Robotic Assembly: From Sequence Inference to Contact Selection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.10571"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: D4PAS (Dataset for Part Assembly Sequences), Samples: 84326, Modality: assembly trajectories, enumerated assembly sequences, viable contact points, part point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Learning-based Axial Video Motion Magnification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.09551"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://axial-momag.github.io/axial-momag/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Graduate School of AI, POSTECH<br>
‚Ä¢ Dataset: Axial Motion Magnification Synthetic Dataset, Samples: 100000, Modality: Synthetic RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.09245"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OpenGVLab/DriveMLM"><img src="https://img.shields.io/github/stars/OpenGVLab/DriveMLM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong; OpenGVLab, Shanghai AI Laboratory<br>
‚Ä¢ Dataset: DriveMLM Dataset, Samples: 50000, Modality: Multi-view images, LiDAR point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Interactive Humanoid: Online Full-Body Motion Reaction Synthesis with Social Affordance Canonicalization and Forecasting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.08983"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yunzeliu.github.io/iHuman/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: HHI, Samples: 5000, Modality: MoCap<br>
‚Ä¢ Dataset: CoChair, Samples: 3000, Modality: MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.08869"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Inertial andMulti-view Highly Dynamic human-object interactions Dataset (IMHD2), Samples: 295, Modality: Multi-view RGB videos, object-mounted IMU, ground-truth human and object meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>PhyOT: Physics-informed object tracking in surveillance cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.08650"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="smartwarehouse2023.cmkl.ac.th"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: Warehouse Dataset, Samples: 680, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Leveraging Driver Field-of-View for Multimodal Ego-Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.08558"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://meakbiyik.github.io/routeformer"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z ¬®urich<br>
‚Ä¢ Dataset: GEM, Samples: None, Modality: GPS trajectories, RGB videos (scene and first-person), gaze data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.08459"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://shivangi-aneja.github.io/projects/facetalk"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich<br>
‚Ä¢ Dataset: FaceTalk dataset, Samples: 1000, Modality: NPHM expression codes + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.07937"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: BOTH57M, Samples: 1384, Modality: SMPLH skeletal motion + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Improving the Robustness of 3D Human Pose Estimation: A Benchmark and Learning from Noisy Input</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.06797"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Illinois at Urbana-Champaign, USA<br>
‚Ä¢ Dataset: Human3.6M-C, Samples: None, Modality: Corrupted RGB videos + 3D poses<br>
‚Ä¢ Dataset: HumanEva-I-C, Samples: None, Modality: Corrupted RGB videos + 3D poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.06553"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/neu-vi/HOI-Diff"><img src="https://img.shields.io/github/stars/neu-vi/HOI-Diff.svg?style=social&label=Star"></a><br><a href="https://neu-vi.github.io/HOI-Diff/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northeastern University<br>
‚Ä¢ Dataset: BEHAVE (annotated), Samples: 1451, Modality: 3D HOI sequences (SMPL-H, object mesh, 6DoF poses) with text descriptions and affordance annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.06398"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vLAR-group/NVFi"><img src="https://img.shields.io/github/stars/vLAR-group/NVFi.svg?style=social&label=Star"></a><br><a href="https://vlar-group.github.io/NVFi.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: vLAR Group, The Hong Kong Polytechnic University<br>
‚Ä¢ Dataset: Dynamic Object Dataset, Samples: 6, Modality: Multi-view RGB videos<br>
‚Ä¢ Dataset: Dynamic Indoor Scene Dataset, Samples: 4, Modality: Multi-view RGB videos with segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Wild Motion Unleashed: Markerless 3D Kinematics and Force Estimation in Cheetahs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.05879"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/African-Robotics-Unit/torque-estimation"><img src="https://img.shields.io/github/stars/African-Robotics-Unit/torque-estimation.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Cape Town, Department of Electrical Engineering, Cape Town, 7700, South Africa<br>
‚Ä¢ Dataset: Kinetic Dataset, Samples: 5, Modality: grayscale videos + synchronised force plate data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.04867"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://handdiffuse.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University, School of Information Science and Technology, Shanghai, China<br>
‚Ä¢ Dataset: HandDiffuse12.5M, Samples: 250K temporal frames, Modality: RGB videos, 2D/3D keypoints, MANO poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Sim-to-Real Causal Transfer: A Metric Learning Approach to Causally-Aware Interaction Representations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.04540"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vita-epfl/CausalSim2Real"><img src="https://img.shields.io/github/stars/vita-epfl/CausalSim2Real.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: √âcole Polytechnique F√©d√©rale de Lausanne (EPFL)<br>
‚Ä¢ Dataset: Sim-to-Real Causal Transfer Diagnostic Dataset, Samples: 26000, Modality: Simulated 2D agent trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.04524"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rehg-lab/RAVE"><img src="https://img.shields.io/github/stars/rehg-lab/RAVE.svg?style=social&label=Star"></a><br><a href="https://rave-video.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Georgia Tech<br>
‚Ä¢ Dataset: RAVE evaluation dataset, Samples: 186, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>SingingHead: A Large-scale 4D Dataset for Singing Head Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.04369"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wsj-sjtu/SingingHead"><img src="https://img.shields.io/github/stars/wsj-sjtu/SingingHead.svg?style=social&label=Star"></a><br><a href="https://wsj-sjtu.github.io/SingingHead/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: SingingHead, Samples: 1952, Modality: RGB videos + 3D facial motion (FLAME parameters) + audio + background music<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Natural-language-driven Simulation Benchmark and Copilot for Efficient Production of Object Interactions in Virtual Road Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.04008"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tianjin University<br>
‚Ä¢ Dataset: Language-to-Interaction (L2I), Samples: 120000, Modality: Natural language descriptions paired with Python code for generating vehicle and pedestrian trajectories in virtual road scenes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Low-power, Continuous Remote Behavioral Localization with Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.03799"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tub-rip.github.io/eventpenguins/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit√§t Berlin<br>
‚Ä¢ Dataset: EventPenguins, Samples: 24, Modality: Events + Grayscale frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.02963"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: FNii, CUHKSZ; SSE, CUHKSZ<br>
‚Ä¢ Dataset: MVHumanNet, Samples: 60000, Modality: Multi-view RGB videos, human masks, camera parameters, 2D/3D keypoints, SMPL/SMPLX parameters, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Neural Sign Actors: A diffusion model for 3D sign language production from text</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.02702"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/baltatzisv/neural-sign-actors"><img src="https://img.shields.io/github/stars/baltatzisv/neural-sign-actors.svg?style=social&label=Star"></a><br><a href="https://baltatzisv.github.io/neural-sign-actors/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Imperial College London<br>
‚Ä¢ Dataset: How2Sign (SMPL-X annotations), Samples: 35000, Modality: SMPL-X parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Revisit Human-Scene Interaction via Space Occupancy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.02700"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Foruck/Occupancy-Motion-Controller"><img src="https://img.shields.io/github/stars/Foruck/Occupancy-Motion-Controller.svg?style=social&label=Star"></a><br><a href="https://foruck.github.io/occu-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: Motion Occupancy Base (MOB), Samples: 98000, Modality: SMPL(-X) motion parameters + Voxelized Occupancy Volume<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.02134"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/aipixel/GaussianAvatar"><img src="https://img.shields.io/github/stars/aipixel/GaussianAvatar.svg?style=social&label=Star"></a><br><a href="https://github.com/aipixel/GaussianAvatar"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Harbin Institute of Technology<br>
‚Ä¢ Dataset: DynVideo, Samples: 2, Modality: RGB videos + SMPL parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>DPHMs: Diffusion Parametric Head Models for Depth-based Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.01068"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tangjiapeng.github.io/projects/DPHMs"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich<br>
‚Ä¢ Dataset: DPHM-Kinect Dataset, Samples: 130, Modality: monocular depth sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>UAVs and Birds: Enhancing Short-Range Navigation through Budgerigar Flight Studies</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.00597"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: American International University -Bangladesh<br>
‚Ä¢ Dataset: Budges355, Samples: 355, Modality: Stereo RGB videos + 2D annotations + 3D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Event Recognition in Laparoscopic Gynecology Videos with Hybrid Transformers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.00593"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ftp.itec.aau.at/datasets/LapGyn6-Events/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Technology (ITEC), Klagenfurt University, Austria<br>
‚Ä¢ Dataset: LapGyn6-Events, Samples: 174, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Event-based Continuous Color Video Decompression from Single Frames</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.00113"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.cis.upenn.edu/~ziyunw/continuitycam/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Pennsylvania, USA.<br>
‚Ä¢ Dataset: Event Extreme Decompression Dataset (E2D2), Samples: , Modality: RGB images + events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Motion-Conditioned Image Animation for Video Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.18827"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="facebookresearch.github.io/MoCA"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: GenAI, Meta, UC Berkeley<br>
‚Ä¢ Dataset: VideoEdit benchmark (custom subset), Samples: 117, Modality: RGB videos + text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>E2PNet: Event to Point Cloud Registration with Spatio-Temporal Representation Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.18433"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xmu-qcj/E2PNet"><img src="https://img.shields.io/github/stars/xmu-qcj/E2PNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fujian Key Lab of Sensing and Computing for Smart Cities, School of Informatics, Xiamen University (XMU), China.<br>
‚Ä¢ Dataset: MVSEC-E2P, Samples: None, Modality: LiDAR, event camera, camera poses<br>
‚Ä¢ Dataset: VECtor-E2P, Samples: None, Modality: LiDAR, event camera, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>OmniMotionGPT: Animal Motion Generation with Limited Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.18303"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Arizona State University<br>
‚Ä¢ Dataset: AnimalML3D, Samples: 1240, Modality: 3D skeletal motion + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks, Methods, and Applications</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.18168"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Apple<br>
‚Ä¢ Dataset: VoxCeleb2-DECA, Samples: >1000000, Modality: 3D facial motion sequences + audio<br>
‚Ä¢ Dataset: VoxCeleb2-SPECTRE, Samples: >1000000, Modality: 3D facial motion sequences + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.17948"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hcis-lab/Action-slot"><img src="https://img.shields.io/github/stars/hcis-lab/Action-slot.svg?style=social&label=Star"></a><br><a href="https://hcis-lab.github.io/Action-slot/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Yang Ming Chiao Tung University<br>
‚Ä¢ Dataset: TACO, Samples: 5178, Modality: RGB videos + instance segmentation<br>
‚Ä¢ Dataset: nuScenes (annotated), Samples: 426, Modality: RGB videos + new atomic activity annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>AgentAvatar: Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.17465"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dorniwang.github.io/AgentAvatar_project/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xiaobing.AI<br>
‚Ä¢ Dataset: Fine-grained CelebvMotion Description Dataset (FilmData), Samples: 8978, Modality: RGB videos + fine-grained text descriptions of facial motion<br>
‚Ä¢ Dataset: EnvPersona, Samples: 1200, Modality: text descriptions (environment, persona, and planned facial motion)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.17057"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vcai.mpi-inf.mpg.de/projects/remos"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: German Research Center for Artificial Intelligence (DFKI)<br>
‚Ä¢ Dataset: ReMoCap, Samples: 275700 frames, Modality: multiview RGB videos + 3D full-body motion capture (including finger articulations)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Egocentric Whole-Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.16495"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://people.mpi-inf.mpg.de/jianwang/projects/egowholemocap/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: EgoWholeBody, Samples: 2629, Modality: Synthetic fisheye RGB images, depth maps, and 3D whole-body pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Dual-Stream Attention Transformers for Sewer Defect Classification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.16145"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/RedwanNewaz/ds mshvit"><img src="https://img.shields.io/github/stars/RedwanNewaz/ds mshvit.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of New Orleans<br>
‚Ä¢ Dataset: Carencro dataset, Samples: 50000, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>GAIA: Zero-shot Talking Avatar Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.15230"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://microsoft.github.io/GAIA"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Microsoft<br>
‚Ä¢ Dataset: GAIA Talking Avatar Dataset, Samples: 15,969 unique speakers / 1,169 hours, Modality: RGB videos with speech, facial landmarks, and head poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Dynamic Compositional Graph Convolutional Network for Efficient Composite Human Motion Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.13781"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WanyingZhang/DCGCN"><img src="https://img.shields.io/github/stars/WanyingZhang/DCGCN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sun Yat-Sen University<br>
‚Ä¢ Dataset: CHAMP, Samples: None, Modality: 3D joint coordinates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Intelligent Knee Sleeves: A Real-time Multimodal Dataset for 3D Lower Body Motion Estimation Using Smart Textile</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.12829"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Zhang-Wenwen/IntelligentKneeBrace"><img src="https://img.shields.io/github/stars/Zhang-Wenwen/IntelligentKneeBrace.svg?style=social&label=Star"></a><br><a href="https://feel.ece.ubc.ca/smartkneesleeve/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, University of British Columbia<br>
‚Ä¢ Dataset: Intelligent Knee Sleeves Dataset, Samples: 140000, Modality: Wearable sensors (IMU, pressure sensors), MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>MaskFlow: Object-Aware Motion Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.12476"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Imagination Technologies<br>
‚Ä¢ Dataset: MaskFlow dataset, Samples: 31550, Modality: RGB image pairs + optical flow ground truth + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Event Camera Data Dense Pre-training</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.11533"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: BDSI, Australian National University, Canberra, Australia<br>
‚Ä¢ Dataset: E-TartanAir, Samples: 1037 sequences, Modality: Event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Implicit Event-RGBD Neural SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.11013"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://delinqu.github.io/EN-SLAM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: DEV-Indoors, Samples: 9, Modality: RGB images, depth maps, event streams, ground truth meshes, ground truth trajectories<br>
‚Ä¢ Dataset: DEV-Reals, Samples: 8, Modality: RGBD, event streams, LiDAR-based ground truth trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>FOCAL: A Cost-Aware Video Dataset for Active Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.10591"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/olivesgatech/FOCAL"><img src="https://img.shields.io/github/stars/olivesgatech/FOCAL.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/10145325"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Georgia Institute of Technology<br>
‚Ä¢ Dataset: FOCAL, Samples: 126, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Video-based Sequential Bayesian Homography Estimation for Soccer Field Registration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.10361"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Paulkie99/KeypointAnnotator"><img src="https://img.shields.io/github/stars/Paulkie99/KeypointAnnotator.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical, Electronic and Computer Engineering, University of Pretoria, Lynnwood Road, Hatfield, Pretoria, 0028, South Africa<br>
‚Ä¢ Dataset: consolidated and refined WorldCup (CARWC), Samples: 4207, Modality: RGB images + homography annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Amodal Optical Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.07761"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://amodal-flow.cs.uni-freiburg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of Freiburg, Germany<br>
‚Ä¢ Dataset: AmodalSynthDrive, Samples: 150, Modality: RGB videos + amodal optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Sounding Bodies: Modeling 3D Spatial Sound of Humans Using Body Pose and Audio</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.06285"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/SoundingBodies"><img src="https://img.shields.io/github/stars/facebookresearch/SoundingBodies.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory<br>
‚Ä¢ Dataset: Sounding Bodies, Samples: 15822, Modality: 3D body pose + spatial audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Social Motion Prediction with Cognitive Hierarchies</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.04726"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/walter0807/Social-CH"><img src="https://img.shields.io/github/stars/walter0807/Social-CH.svg?style=social&label=Star"></a><br><a href="https://walter0807.github.io/Social-CH/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center on Frontiers of Computing Studies, School of Computer Science, Peking University<br>
‚Ä¢ Dataset: Wusi, Samples: 60K frames, Modality: 3D multi-person motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Exploring Event-based Human Pose Estimation with 3D Event Representations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.04591"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MasterHow/EventPointPose"><img src="https://img.shields.io/github/stars/MasterHow/EventPointPose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang University, Hangzhou 310027, China<br>
‚Ä¢ Dataset: EV-3DPW, Samples: 23,475 train samples and 40,145 test samples, Modality: synthetic events, RGB videos, 2D pose annotations<br>
‚Ä¢ Dataset: EV-JAAD, Samples: derived from 346 video clips, Modality: synthetic events, human bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Scalable and Efficient Continual Learning from Demonstration via a Hypernetwork-generated Stable Dynamics Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.03600"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sayantanauddy/clfd-snode"><img src="https://img.shields.io/github/stars/sayantanauddy/clfd-snode.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of Innsbruck, Austria<br>
‚Ä¢ Dataset: High-dimensional LASA, Samples: 210, Modality: High-dimensional trajectories<br>
‚Ä¢ Dataset: RoboTasks9, Samples: 81, Modality: Robot end-effector position and orientation trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Unsupervised Region-Growing Network for Object Segmentation in Atmospheric Turbulence</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.03572"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/turb-research/DOST"><img src="https://img.shields.io/github/stars/turb-research/DOST.svg?style=social&label=Star"></a><br><a href="https://turb-research.github.io/DOST/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Clemson University<br>
‚Ä¢ Dataset: Dynamic Object Segmentation in Turbulence (DOST), Samples: 38, Modality: RGB videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Race Against the Machine: a Fully-annotated, Open-design Dataset of Autonomous and Piloted High-speed Flight</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.02667"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tii-racing/drone-racing-dataset"><img src="https://img.shields.io/github/stars/tii-racing/drone-racing-dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Autonomous Robotics Research Center of the Technology Innovation Institute, Abu Dhabi, United Arab Emirates; University of Bologna, Bologna, Italy<br>
‚Ä¢ Dataset: TII-RATM (Race Against the Machine), Samples: 30, Modality: MoCap poses, IMU data, RGB videos, control inputs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>MAAIP: Multi-Agent Adversarial Interaction Priors for imitation from fighting demonstrations for physics-based characters</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.02502"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Inria, IRISA, University of Rennes, France<br>
‚Ä¢ Dataset: Boxing fighting interactions dataset, Samples: None, Modality: MoCap landmarks<br>
‚Ä¢ Dataset: QwanKiDo fighting interactions dataset, Samples: None, Modality: MoCap landmarks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>LocoMuJoCo: A Comprehensive Imitation Learning Benchmark for Locomotion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.02496"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/robfiras/loco-mujoco"><img src="https://img.shields.io/github/stars/robfiras/loco-mujoco.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Intelligent Autonomous Systems Group, TU Darmstadt<br>
‚Ä¢ Dataset: LocoMuJoCo, Samples: 27, Modality: Motion capture (kinematic trajectories), robot joint states and actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>ECMD: An Event-Centric Multisensory Driving Dataset for SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.02327"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://arclab-hku.github.io/ecmd/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: ECMD, Samples: 81, Modality: Stereo event cameras, Stereo RGB cameras, Infrared camera, LiDAR, IMU, GNSS-RTK/INS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.02191"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://upc-virvig.github.io/SparsePoser"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universitat Polit√®cnica de Catalunya, Spain<br>
‚Ä¢ Dataset: VR-specific motion capture database, Samples: 2000000 poses, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.02077"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/NVlabs/EmerNeRF"><img src="https://img.shields.io/github/stars/NVlabs/EmerNeRF.svg?style=social&label=Star"></a><br><a href="https://emernerf.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Southern California<br>
‚Ä¢ Dataset: NeRF On-The-Road (NOTR), Samples: 120, Modality: RGB videos, LiDAR, 3D scene flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Joint 3D Shape and Motion Estimation from Rolling Shutter Light-Field Images</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.01292"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ICB-Vision-AI/RSLF"><img src="https://img.shields.io/github/stars/ICB-Vision-AI/RSLF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universit ¬¥e de Bourgogne, CNRS UMR 6303 ICB; Universit ¬¥e de Franche-Comt ¬¥e, CNRS UMR 6174 FEMTO-ST<br>
‚Ä¢ Dataset: Rolling Shutter Light Fields (RSLF), Samples: 77, Modality: Rolling Shutter Light-Field images with ground truth depth maps and camera motion parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Enhancing Traffic Object Detection in Variable Illumination with RGB-Event Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.00436"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/YN-Yang/SFNet"><img src="https://img.shields.io/github/stars/YN-Yang/SFNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information Engineering, Chang‚Äôan University, Shaanxi, Xi‚Äôan 710000, China<br>
‚Ä¢ Dataset: DSEC-Det, Samples: 53, Modality: RGB, Event camera stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Event-based Background-Oriented Schlieren</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.00434"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tub-rip/event-based-bos"><img src="https://img.shields.io/github/stars/tub-rip/event-based-bos.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit√§t Berlin, Berlin, Germany; Department of Electronics and Electrical Engineering, Faculty of Science and Technology, Keio University, Kanagawa, Japan<br>
‚Ä¢ Dataset: Schlieren Event-Frames Dataset, Samples: 9, Modality: Event camera data, grayscale frames, derived optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Learning Cooperative Trajectory Representations for Motion Forecasting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.00371"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AIR-THU/V2X-Graph"><img src="https://img.shields.io/github/stars/AIR-THU/V2X-Graph.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for AI Industry Research (AIR), Tsinghua University<br>
‚Ä¢ Dataset: V2X-Traj, Samples: 10102, Modality: Trajectories derived from LiDAR and cameras<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.20436"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://signavatars.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Imperial College London<br>
‚Ä¢ Dataset: SignAvatars, Samples: 70000, Modality: RGB videos + SMPL-X/MANO parameters + 2D/3D keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Do we need scan-matching in radar odometry?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.18117"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kubelvla/mine-and-forest-radar-dataset"><img src="https://img.shields.io/github/stars/kubelvla/mine-and-forest-radar-dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AASS research centre at √ñrebro University, Sweden<br>
‚Ä¢ Dataset: mine-and-forest-radar-dataset, Samples: 2, Modality: 4D Radar, IMU, LiDAR, Camera<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.17462"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kiedani/Towards-Learning-Monocular-3D-Object-Localization-From-2D-Labels-Using-the-Physical-Laws-of-Motion"><img src="https://img.shields.io/github/stars/kiedani/Towards-Learning-Monocular-3D-Object-Localization-From-2D-Labels-Using-the-Physical-Laws-of-Motion.svg?style=social&label=Star"></a><br><a href="https://kiedani.github.io/3DV2024/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Augsburg<br>
‚Ä¢ Dataset: Synthetic Dataset (SD), Samples: 7200, Modality: Synthetic RGB videos + 3D trajectories<br>
‚Ä¢ Dataset: Real Dataset (RD), Samples: 366, Modality: RGB videos + 3D trajectories<br>
‚Ä¢ Dataset: Spring Dataset, Samples: 300, Modality: Synthetic RGB videos + 3D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Automatic Edge Error Judgment in Figure Skating Using 3D Pose Estimation from a Monocular Camera and IMUs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.17193"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ryota-takedalab/JudgeAI-LutzEdge"><img src="https://img.shields.io/github/stars/ryota-takedalab/JudgeAI-LutzEdge.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nagoya University<br>
‚Ä¢ Dataset: Figure Skating Single Lutz Jump Dataset, Samples: 232, Modality: RGB videos, IMU, 3D pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Language-driven Scene Synthesis using Multi-conditional Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.15948"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VinAIResearch/LSDM"><img src="https://img.shields.io/github/stars/VinAIResearch/LSDM.svg?style=social&label=Star"></a><br><a href="https://lang-scene-synth.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: FSOFT AI Center<br>
‚Ä¢ Dataset: PRO-teXt, Samples: 200, Modality: Human motions (poses) + 3D scenes (point clouds) + Text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Object Pose Estimation Annotation Pipeline for Multi-view Monocular Camera Systems in Industrial Settings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.14914"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/bop_toolkit-6F86"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Dortmund University, Dortmund, Germany<br>
‚Ä¢ Dataset: Multi-log, Samples: 26500, Modality: RGB images + 6D object poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>PACE: Human and Camera Motion Estimation from in-the-wild Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.13768"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nvlabs.github.io/PACE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA, Max Planck Institute for Intelligent Systems, Tubingen, Germany, ETH Zurich, Switzerland<br>
‚Ä¢ Dataset: HCM (Human and Camera Motion) dataset, Samples: 25, Modality: RGB videos + ground-truth 3D human motion + ground-truth camera motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.13258"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://portal-cornell.github.io/manicast/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Cornell University<br>
‚Ä¢ Dataset: Collaborative Manipulation Dataset (CoMaD), Samples: 61, Modality: Motion Capture + RGB video + Audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Simultaneous Learning of Contact and Continuous Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.12054"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mhalm/continuous-contact-nets"><img src="https://img.shields.io/github/stars/mhalm/continuous-contact-nets.svg?style=social&label=Star"></a><br><a href="https://sites.google.com/view/continuous-contact-nets/home"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: GRASP Laboratory, University of Pennsylvania<br>
‚Ä¢ Dataset: Articulated Object Toss Trajectories, Samples: over 500, Modality: Pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>MOCHA: Real-Time Motion Characterization via Context Matching</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.10079"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST and MOVIN Inc.
South Korea<br>
‚Ä¢ Dataset: MOCHA character motion dataset, Samples: 573k frames, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>MoEmo Vision Transformer: Integrating Cross-Attention and Movement Vectors in 3D Pose Estimation for HRI Emotion Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.09757"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Santa Clara University<br>
‚Ä¢ Dataset: Naturalistic Motion Database, Samples: 1512, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Dynamic Appearance Particle Neural Radiance Field</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.07916"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Cenbylin/DAP-NeRF"><img src="https://img.shields.io/github/stars/Cenbylin/DAP-NeRF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Australian Artificial Intelligence Institute (AAII), University of Technology Sydney, Sydney, NSW 2007, Australia<br>
‚Ä¢ Dataset: Motion Modeling Evaluation Dataset, Samples: 3, Modality: Synthetic RGB videos + Ground-truth 3D velocity fields<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Saturation-Aware Angular Velocity Estimation: Extending the Robustness of SLAM to Aggressive Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.07844"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/norlab-ulaval/Norlab_wiki/wiki/TIGS-Dataset"><img src="https://img.shields.io/github/stars/wiki/TIGS-Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northern Robotics Laboratory, Universit√© Laval, Quebec City, Quebec, Canada<br>
‚Ä¢ Dataset: TIGS, Samples: 32, Modality: Lidar, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Towards More Efficient Depression Risk Recognition via Gait</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.06283"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence, Beijing Normal University, Beijing 100875, China<br>
‚Ä¢ Dataset: Gait-based Depression Risk Recognition Dataset, Samples: 40281, Modality: gait silhouettes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.05934"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Image and Video Systems Lab, KAIST<br>
‚Ä¢ Dataset: 3D-HDTF, Samples: 10000, Modality: 3D face mesh sequences + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Universal Humanoid Motion Representations for Physics-Based Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.04582"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zhengyiluo/pulse"><img src="https://img.shields.io/github/stars/zhengyiluo/pulse.svg?style=social&label=Star"></a><br><a href="https://zhengyiluo.github.io/PULSE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Reality Labs Research, Meta; Carnegie Mellon University<br>
‚Ä¢ Dataset: Cleaned AMASS dataset, Samples: 11313 training sequences, 138 testing sequences, Modality: MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>SwimXYZ: A large-scale dataset of synthetic swimming motions and videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.04360"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://g-fiche.github.io/research-pages/swimxyz/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CentraleSup√©lec, IETR UMR CNRS 6164, France<br>
‚Ä¢ Dataset: SwimXYZ, Samples: 240, Modality: SMPL parameters, synthetic RGB videos, 2D/3D joint annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>A Dataset of Anatomical Environments for Medical Robots: Modeling Respiratory Deformation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.04289"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UNC-Robotics/Med-RAD"><img src="https://img.shields.io/github/stars/UNC-Robotics/Med-RAD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of North Carolina at Chapel Hill<br>
‚Ä¢ Dataset: Medical Robotics Anatomical Dataset (Med-RAD), Samples: 3, Modality: 3D anatomical models (from CT) + 3D respiratory deformation fields<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>GroundLink: A Dataset Unifying Human Body Movement and Ground Reaction Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.03930"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://csr.bu.edu/groundlink/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Science, Boston University<br>
‚Ä¢ Dataset: GroundLink, Samples: 368, Modality: MoCap markers, SMPL-X parameters, Ground Reaction Force (GRF), Center of Pressure (CoP)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.03205"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://neuface-dataset.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Electrical Engineering, POSTECH<br>
‚Ä¢ Dataset: NeuFace-dataset, Samples: 1245000, Modality: 3D face mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>A Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.02523"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://share.weiyun.com/9qOxwpqz"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Jinan University<br>
‚Ä¢ Dataset: STSCB, Samples: 44670, Modality: RGB videos + behavior annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>EvDNeRF: Reconstructing Event Data with Dynamic Neural Radiance Fields</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.02437"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/anish-bhattacharya/EvDNeRF"><img src="https://img.shields.io/github/stars/anish-bhattacharya/EvDNeRF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Pennsylvania<br>
‚Ä¢ Dataset: Simulated Dynamic Event Scenes (Jet-Down, Jet-Spiral, Jet-Land, Multi, Lego), Samples: 5, Modality: multi-view eventstreams, RGB frames<br>
‚Ä¢ Dataset: Real-World Dynamic Event Scenes (Real-Fork, Real-Knife), Samples: 2, Modality: multi-view eventstreams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>RSRD: A Road Surface Reconstruction Dataset and Benchmark for Safe and Comfortable Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.02262"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://thu-rsxd.com/rsrd/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Vehicle and Mobility, Tsinghua University<br>
‚Ä¢ Dataset: RSRD (Road Surface Reconstruction Dataset), Samples: 191, Modality: Stereo images, LiDAR point clouds, IMU, RTK (pose, location, velocity)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Visual Temporal Fusion Based Free Space Segmentation for Autonomous Surface Vessels</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.00879"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ORCA-Uboat, Shaanxi, 710075 China<br>
‚Ä¢ Dataset: video sequence dataset for ASVs free space segmentation, Samples: 10, Modality: video sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Propagating Semantic Labels in Video Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.00783"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dataverse.tdl.org/dataverse/robotics"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Departent of Computer Science, The University of Texas at Austin<br>
‚Ä¢ Dataset: not explicitly named, Samples: 5, Modality: RGB videos + semantic masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Music- and Lyrics-driven Dance Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.00455"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yy1lab/LMD"><img src="https://img.shields.io/github/stars/yy1lab/LMD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: JustLMD, Samples: 1867, Modality: 3D dance motion (24-joint SMPL format)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.00434"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://diffposetalk.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: BNRist, Tsinghua University, China<br>
‚Ä¢ Dataset: Talking Face with Head Poses (TFHP), Samples: 1052, Modality: Audio-visual videos + reconstructed 3DMM FLAME parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Berkeley Open Extended Reality Recordings 2023 (BOXRR-23): 4.7 Million Motion Capture Recordings from 105,852 Extended Reality Device Users</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.00430"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/metaguard/xror"><img src="https://img.shields.io/github/stars/metaguard/xror.svg?style=social&label=Star"></a><br><a href="https://rdi.berkeley.edu/metaverse/boxrr-23/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: BOXRR-23, Samples: 4717215, Modality: 6DoF head and hand motion capture<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>EGVD: Event-Guided Video Deraining</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.17239"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/booker-max/EGVD"><img src="https://img.shields.io/github/stars/booker-max/EGVD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electronic Engineering and Information Science, University of Science and Technology of China<br>
‚Ä¢ Dataset: Rain-DAVIS, Samples: 10, Modality: RGB videos + event streams<br>
‚Ä¢ Dataset: N-NTURain, Samples: 33, Modality: RGB videos + event streams<br>
‚Ä¢ Dataset: N-GoproRain, Samples: 77, Modality: RGB videos + event streams<br>
‚Ä¢ Dataset: N-AdobeRainH, Samples: 128, Modality: RGB videos + event streams<br>
‚Ä¢ Dataset: N-AdobeRainL, Samples: 128, Modality: RGB videos + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>UniQuadric: A SLAM Backend for Unknown Rigid Object 3D Tracking and Light-Weight Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.17036"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Linghao-Yang/Synthesized-Multimotion-and-Modeling-Dataset"><img src="https://img.shields.io/github/stars/Linghao-Yang/Synthesized-Multimotion-and-Modeling-Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xi‚Äôan Precision Machinery Research Institute Kunming Branch, China<br>
‚Ä¢ Dataset: SMMD (Synthesized Multimotion and Modeling dataset), Samples: 6 sequences mentioned (Subway, car, Cuboid, Cube, SkateBoard, Ball), Modality: RGB images, depth images, simulated LiDAR data, camera pose, object pose, object scale<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>SpikeMOT: Event-based Multi-Object Tracking with Sparse Motion Features</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16987"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Electronic Engineering at The University of Hong Kong, Hong Kong, China<br>
‚Ä¢ Dataset: DSEC-MOT, Samples: 12, Modality: Event streams + RGB images + Bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>CrossZoom: Simultaneously Motion Deblurring and Event Super-Resolving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16949"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Bestrivenzc/CZ-Net"><img src="https://img.shields.io/github/stars/Bestrivenzc/CZ-Net.svg?style=social&label=Star"></a><br><a href="https://bestrivenzc.github.io/CZ-Net/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University<br>
‚Ä¢ Dataset: Cross-Resolution Deblurring and Resolving (CRDR), Samples: 76, Modality: RGB images (sharp, blurry) and event streams (HR, LR)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16940"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MediaBrain-SJTU/CoBEVFlow"><img src="https://img.shields.io/github/stars/MediaBrain-SJTU/CoBEVFlow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Cooperative Medianet Innovation Center, Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: IRregular V2V (IRV2V), Samples: 8449, Modality: LiDAR + RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Decaf: Monocular Deformation Capture for Face and Hand Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16670"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/soshishimada/Decaf_release"><img src="https://img.shields.io/github/stars/soshishimada/Decaf_release.svg?style=social&label=Star"></a><br><a href="https://vcai.mpi-inf.mpg.de/projects/Decaf"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Informatics, SIC, VIA Research Center, Germany<br>
‚Ä¢ Dataset: Decaf Dataset, Samples: 100K frames, Modality: RGB videos, 2D hand keypoints, 2D face landmarks, foreground segmentation masks, hand-face bounding boxes, 3D mesh for hand and face, 3D surface deformations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Deep Geometrized Cartoon Line Inbetweening</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16643"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lisiyao21/AnimeInbet"><img src="https://img.shields.io/github/stars/lisiyao21/AnimeInbet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: MixamoLine240, Samples: 240, Modality: 2D line drawing sequences + vertex coordinates/topology + vertex correspondences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Radar Instance Transformer: Reliable Moving Instance Segmentation in Sparse Radar Point Clouds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16435"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CARIAD SE and University of Bonn<br>
‚Ä¢ Dataset: RadarScenes Moving Instance Segmentation Benchmark, Samples: 158, Modality: Radar point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Object Motion Guided Human Motion Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16237"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University, USA<br>
‚Ä¢ Dataset: Unnamed (OMOMO dataset), Samples: approx. 10 hours duration, Modality: 3D object geometry, object motion, and full-body human motion (MoCap as SMPL-X parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Interaction-Aware Sampling-Based MPC with Learned Local Goal Predictions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.14931"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://autonomousrobots.nl/pubpage/IA_MPPI_LBM.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Cognitive Robotics Department, TU Delft<br>
‚Ä¢ Dataset: Artificial Vessel Trajectory Dataset, Samples: 4696, Modality: position and velocity trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>AgriSORT: A Simple Online Real-time Tracking-by-Detection framework for robotics in precision agriculture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.13393"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sapienza University of Rome<br>
‚Ä¢ Dataset: Agricultural MOT benchmark for table grape vineyards, Samples: 4, Modality: RGB-D video sequences with MOT annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>CloudGripper: An Open Source Cloud Robotics Testbed for Robotic Manipulation Research, Benchmarking and Data Collection at Scale</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.12786"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cloudgripper.org"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: CloudGripper-Rope-100, Samples: None, Modality: robot motion commands, RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>DIOR: Dataset for Indoor-Outdoor Reidentification -- Long Range 3D/2D Skeleton Gait Collection Pipeline, Semi-Automated Gait Keypoint Labeling and Baseline Evaluation Methods</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.12429"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Engineering, University at Buffalo, SUNY<br>
‚Ä¢ Dataset: DIOR, Samples: 112, Modality: RGB videos + 3D/2D skeleton labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>PanoVOS: Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.12303"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/shilinyan99/PanoVOS"><img src="https://img.shields.io/github/stars/shilinyan99/PanoVOS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University<br>
‚Ä¢ Dataset: PanoVOS, Samples: 150, Modality: Panoramic RGB videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>A Novel Deep Neural Network for Trajectory Prediction in Automated Vehicles Using Velocity Vector Field</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.10948"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Amir-Samadi/VVF-TP"><img src="https://img.shields.io/github/stars/Amir-Samadi/VVF-TP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Warwick Manufacturing Group (WMG), The University of Warwick<br>
‚Ä¢ Dataset: VVF dataset for highD, Samples: 110000, Modality: Velocity Vector Field (VVF) images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>DRIVE: Data-driven Robot Input Vector Exploration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.10718"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/norlab-ulaval/DRIVE"><img src="https://img.shields.io/github/stars/norlab-ulaval/DRIVE.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northern Robotics Laboratory, Universit√© Laval, Quebec City, Quebec, Canada<br>
‚Ä¢ Dataset: DRIVE, Samples: 1.8 hours / 7 km of driving data, Modality: Robot kinematics (commands, velocities, pose trajectories), Point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Asteroids co-orbital motion classification based on Machine Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.10603"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IFAC-CNR, Istituto di Fisica Applicata ‚ÄúNello Carrara‚Äù, Consiglio Nazionale delle Ricerche, via Madonna del Piano 10, 50019 Sesto Fiorentino (FI), Italy<br>
‚Ä¢ Dataset: Co-orbital Asteroid Motion Dataset (Real), Samples: 50, Modality: Time series of asteroid orbital elements (resonant angle Œ∏)<br>
‚Ä¢ Dataset: Co-orbital Asteroid Motion Dataset (Ideal Simulated), Samples: 1999, Modality: Time series of asteroid orbital elements (resonant angle Œ∏)<br>
‚Ä¢ Dataset: Co-orbital Asteroid Motion Dataset (Perturbed Simulated), Samples: 347, Modality: Time series of asteroid orbital elements (resonant angle Œ∏)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Deep Visual Odometry with Events and Frames</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.09947"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/uzh-rpg/rampvo"><img src="https://img.shields.io/github/stars/uzh-rpg/rampvo.svg?style=social&label=Star"></a><br><a href="https://github.com/uzh-rpg/rampvo"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics and Perception Group, University of Zurich, Switzerland<br>
‚Ä¢ Dataset: Apollo landing, Samples: 6, Modality: real RGB images, event camera data, MoCap pose trajectories<br>
‚Ä¢ Dataset: Malapert landing, Samples: 2, Modality: simulated RGB images, synthetic events, pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Sparse and Privacy-enhanced Representation for Human Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.09515"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lyhsieh.github.io/sphp/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Vision Science Lab, National Tsing Hua University<br>
‚Ä¢ Dataset: SPHP (Sparse and Privacy-enhanced Dataset for Human Pose Estimation), Samples: 640, Modality: edge images, two-directional motion vector images, grayscale images, 2D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Learning Parallax for Stereo Event-based Motion Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.09513"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Mingyuan-Lin/St-EDNet"><img src="https://img.shields.io/github/stars/Mingyuan-Lin/St-EDNet.svg?style=social&label=Star"></a><br><a href="https://mingyuan-lin.github.io/St-EDweb/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University, Wuhan 430072, China<br>
‚Ä¢ Dataset: StEIC, Samples: 65, Modality: RGB-D images, Event streams, Disparity maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>MOVIN: Real-time Motion Capture using a Single LiDAR</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.09314"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://movin3d.github.io/movin_pg2023/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MOVIN Inc., Korea Advanced Institute of Science and Technology (KAIST)<br>
‚Ä¢ Dataset: MOVIN dataset, Samples: 161179, Modality: LiDAR point cloud and optical motion capture data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>RMP: A Random Mask Pretrain Framework for Motion Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.08989"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/KTH-RPL/RMP"><img src="https://img.shields.io/github/stars/KTH-RPL/RMP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology, Scania CV AB<br>
‚Ä¢ Dataset: Post-processed INTERACTION dataset, Samples: None, Modality: Agent trajectories with occlusion labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.08596"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wengflow/robust-e-nerf"><img src="https://img.shields.io/github/stars/wengflow/robust-e-nerf.svg?style=social&label=Star"></a><br><a href="https://wengflow.github.io/robust-e-nerf"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The NUS Graduate School‚Äôs Integrative Sciences and Engineering Programme (ISEP)<br>
‚Ä¢ Dataset: Robust e-NeRF Synthetic Event Dataset, Samples: 7 scenes with multiple sequences generated under varying conditions, Modality: Event stream + camera pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.08588"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fabiendelattre.com/robust-rotation-estimation"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Massachusetts Amherst<br>
‚Ä¢ Dataset: BUsy Street Scenes (BUSS), Samples: 17, Modality: RGB videos + IMU + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Learning Quasi-Static 3D Models of Markerless Deformable Linear Objects for Bimanual Robotic Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.07609"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/PPI-PUT/neural_dlo_model"><img src="https://img.shields.io/github/stars/PPI-PUT/neural_dlo_model.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Robotics and Machine Intelligence, Poznan University of Technology, Poznan, Poland<br>
‚Ä¢ Dataset: two-wire cable DLO dataset (50 cm), Samples: 54080, Modality: Robot end-effector poses and DLO 3D point sequences from RGBD<br>
‚Ä¢ Dataset: two-wire cable DLO dataset (45 cm), Samples: 14606, Modality: Robot end-effector poses and DLO 3D point sequences from RGBD<br>
‚Ä¢ Dataset: two-wire cable DLO dataset (40 cm), Samples: 13264, Modality: Robot end-effector poses and DLO 3D point sequences from RGBD<br>
‚Ä¢ Dataset: solar cable DLO dataset (50 cm), Samples: 5344, Modality: Robot end-effector poses and DLO 3D point sequences from RGBD<br>
‚Ä¢ Dataset: braided cable DLO dataset (50 cm), Samples: 6136, Modality: Robot end-effector poses and DLO 3D point sequences from RGBD<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>STUPD: A Synthetic Dataset for Spatial and Temporal Relation Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.06680"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Frontier AI Research, Agency for Science, Technology, and Research, Singapore<br>
‚Ä¢ Dataset: STUPD (Spatial and Temporal Understanding of Prepositions Dataset), Samples: 200000, Modality: RGB videos + 3D coordinates + bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>STAR-loc: Dataset for STereo And Range-based localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.05518"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/utiasASRL/starloc"><img src="https://img.shields.io/github/stars/utiasASRL/starloc.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto<br>
‚Ä¢ Dataset: STAR-loc, Samples: 22, Modality: Stereo images, IMU, UWB range measurements, Vicon MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>HiLM-D: Enhancing MLLMs with Multi-Scale High-Resolution Details for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.05186"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://usa.honda-ri.com/drama"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China.<br>
‚Ä¢ Dataset: DRAMA-ROLISP, Samples: 17785, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>FreeMan: Towards Benchmarking 3D Human Pose Estimation under Real-World Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.05073"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/IDEA-Research/deepdataspace"><img src="https://img.shields.io/github/stars/IDEA-Research/deepdataspace.svg?style=social&label=Star"></a><br><a href="https://wangjiongw.github.io/freeman"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong, Shenzhen, Tencent<br>
‚Ä¢ Dataset: FreeMan, Samples: 8000, Modality: RGB videos + 2D/3D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Stereo Matching in Time: 100+ FPS Video Stereo Matching for Extended Reality</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.04183"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tencent XR Vision Labs<br>
‚Ä¢ Dataset: XR-Stereo, Samples: 17, Modality: RGB videos + 6-DoF camera trajectories + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Leveraging Geometrical Acoustic Simulations of Spatial Room Impulse Responses for Improved Sound Event Detection and Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.03337"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ChrisIck/DCASE_Synth_Data"><img src="https://img.shields.io/github/stars/ChrisIck/DCASE_Synth_Data.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Music and Audio Research Laboratory, New York University<br>
‚Ä¢ Dataset: SIM-SRIR, Samples: 38530, Modality: Simulated Spatial Room Impulse Responses (SRIRs) along motion trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.01372"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/axdfhj/MDD"><img src="https://img.shields.io/github/stars/axdfhj/MDD.svg?style=social&label=Star"></a><br><a href="https://github.com/axdfhj/MDD"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: WildMotion-Caption (WMC), Samples: 8888, Modality: SMPL-based skeleton keypoints and motion features<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>BodySLAM++: Fast and Tightly-Coupled Visual-Inertial Camera and Human Motion Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.01236"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Smart Robotics Lab, Dept. of Computing, Imperial College London, UK<br>
‚Ä¢ Dataset: BodySLAM dataset, Samples: 30, Modality: Stereo visual-inertial data, ground truth 6D camera poses, ground truth 3D human joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>An Asynchronous Linear Filter Architecture for Hybrid Event-Frame Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.01159"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ziweiwwang/Event-Asynchronous-Filter"><img src="https://img.shields.io/github/stars/ziweiwwang/Event-Asynchronous-Filter.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Systems Theory and Robotics (STR) Group, College of Engineering and Computer Science, Australian National University, Canberra, ACT 2601, Australia<br>
‚Ä¢ Dataset: HDR Hybrid Event-Frame Dataset, Samples: 6, Modality: Stereo hybrid event-frame camera (event camera data, LDR RGB videos, HDR reference images)<br>
‚Ä¢ Dataset: AHDR (Artificial HDR) Dataset, Samples: 4, Modality: Stereo hybrid event-frame camera (event camera data, simulated LDR videos, HDR reference images)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Dense Voxel 3D Reconstruction Using a Monocular Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.00385"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, The University of Sydney<br>
‚Ä¢ Dataset: None, Samples: 39739, Modality: simulated event data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.16894"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ait.ethz.ch/emdb"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich, Department of Computer Science<br>
‚Ä¢ Dataset: EMDB, Samples: 81, Modality: RGB-D videos, EM sensor measurements, 3D SMPL poses, shapes, and global trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.16876"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://neu-vi.github.io/SportsSlomo/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC San Diego<br>
‚Ä¢ Dataset: SportsSloMo, Samples: 131464, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Learning to Predict 3D Rotational Dynamics from Images of a Rigid Body with Unknown Mass Distribution</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.14666"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical and Aerospace Engineering, Princeton University, Princeton, NJ 08544, USA; The Aerospace Corporation, El Segundo, CA 90245, USA<br>
‚Ä¢ Dataset: Uniform mass density cube, Samples: 1000, Modality: synthetic images<br>
‚Ä¢ Dataset: Uniform mass density prism, Samples: 1000, Modality: synthetic images<br>
‚Ä¢ Dataset: Non-uniform mass density cube, Samples: 1000, Modality: synthetic images<br>
‚Ä¢ Dataset: Non-uniform mass density prism, Samples: 1000, Modality: synthetic images<br>
‚Ä¢ Dataset: Uniform density synthetic-satellites (CALIPSO), Samples: 1000, Modality: synthetic images<br>
‚Ä¢ Dataset: Uniform density synthetic-satellites (CloudSat), Samples: 1000, Modality: synthetic images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>LAC: Latent Action Composition for Skeleton-based Action Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.14500"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/walker1126/LAC"><img src="https://img.shields.io/github/stars/walker1126/LAC.svg?style=social&label=Star"></a><br><a href="https://walker1126.github.io/LAC/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Inria, Universit√© C√¥te d‚ÄôAzur<br>
‚Ä¢ Dataset: Charades, Samples: None, Modality: 2D skeleton data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>4D Myocardium Reconstruction with Decoupled Motion and Shape Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.14083"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yuan-xiaohan/4D-Myocardium-Reconstruction-with-Decoupled-Motion-and-Shape-Model"><img src="https://img.shields.io/github/stars/yuan-xiaohan/4D-Myocardium-Reconstruction-with-Decoupled-Motion-and-Shape-Model.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southeast University, China<br>
‚Ä¢ Dataset: 4D myocardial dataset, Samples: 55, Modality: cine magnetic resonance (CMR) derived 3D shape sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>A Conflict Resolution Dataset Derived from Argoverse-2: Analysis of the Safety and Efficiency Impacts of Autonomous Vehicles at Intersections</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.13839"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/RomainLITUD/conflict_resolution_dataset"><img src="https://img.shields.io/github/stars/RomainLITUD/conflict_resolution_dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Transport and Planning, Civil Engineering and Geosciences, Delft University of Technology<br>
‚Ä¢ Dataset: Conflict Resolution Dataset, Samples: 21431, Modality: Processed trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Dance with You: The Diversity Controllable Dancer Generation via Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.13551"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JJessicaYao/AIST-M-Dataset"><img src="https://img.shields.io/github/stars/JJessicaYao/AIST-M-Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong, Shenzhen<br>
‚Ä¢ Dataset: AIST-M, Samples: 340 lead-partner dancer pairs, Modality: 2D keypoints, 3D keypoints, SMPL skeletons, music<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>AccFlow: Backward Accumulation for Long-Range Optical Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.13133"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mulns/AccFlow"><img src="https://img.shields.io/github/stars/mulns/AccFlow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: CVO, Samples: 12000, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>ROAM: Robust and Object-Aware Motion Generation Using Neural Pose Descriptors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.12969"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vcai.mpi-inf.mpg.de/projects/ROAM/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: ROAM Dataset, Samples: around 90 minutes of motion, Modality: Skeletal motion from markerless motion capture using multi-view RGB cameras, with foot contact and phase labels.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>The GENEA Challenge 2023: A large scale evaluation of gesture generation models in monadic and dyadic settings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.12646"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="svito-zar.github.io/GENEAchallenge2023/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SEED ‚Äì Electronic Arts (EA), Sweden<br>
‚Ä¢ Dataset: GENEA Challenge 2023 Dataset, Samples: 70 test chunks, Modality: MoCap joints + speech audio + text transcriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>NPF-200: A Multi-Modal Eye Fixation Dataset and Method for Non-Photorealistic Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.12163"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yangziyu/NPF200"><img src="https://img.shields.io/github/stars/Yangziyu/NPF200.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: South China University of Technology<br>
‚Ä¢ Dataset: NPF-200, Samples: 200, Modality: RGB videos + audio + eye fixation points<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>The TYC Dataset for Understanding Instance-Level Semantics and Motions of Cells in Microstructures</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.12116"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://christophreich1996.github.io/tyc_dataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering and Information Technology, Centre for Synthetic Biology, Technische Universit ¬®at Darmstadt<br>
‚Ä¢ Dataset: TYC dataset, Samples: 261, Modality: brightfield microscopy videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Machine Learning-based Positioning using Multivariate Time Series Classification for Factory Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.11670"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Telematics, Hamburg University of Technology<br>
‚Ä¢ Dataset: Motion-Ambient, Samples: 4635217, Modality: IMU measurements (accelerometer, gyroscope, magnetometer), pressure, temperature, humidity, spectrum<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>TOPIC: A Parallel Association Paradigm for Multi-Object Tracking under Complex Motions and Diverse Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.11157"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/holmescao/TOPICTrack"><img src="https://img.shields.io/github/stars/holmescao/TOPICTrack.svg?style=social&label=Star"></a><br><a href="https://drive.google.com/file/d/1KvqQTZWWhyDsQuEIqQY3XLM8QlYjDqi/view?usp=sharing"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Key Laboratory for Urban Habitat Environmental Science and Technology, School of Environment and Energy, Peking University Shenzhen Graduate School<br>
‚Ä¢ Dataset: BEE24, Samples: 36, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Recursive Video Lane Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.11106"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dongkwonjin/RVLD"><img src="https://img.shields.io/github/stars/dongkwonjin/RVLD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea University<br>
‚Ä¢ Dataset: OpenLane-V, Samples: 590, Modality: RGB videos + temporally consistent lane annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>PsyMo: A Dataset for Estimating Self-Reported Psychological Traits from Gait</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.10631"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bit.ly/3Q91ypD"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University Politehnica of Bucharest<br>
‚Ä¢ Dataset: PsyMo, Samples: 14976, Modality: silhouettes, 2D / 3D human skeletons, 3D SMPL human meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.08544"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://henghuiding.github.io/MeViS"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanyang Technological University<br>
‚Ä¢ Dataset: MeViS, Samples: 2006, Modality: RGB videos + text expressions + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Leveraging Next-Active Objects for Context-Aware Anticipation in Egocentric Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.08303"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Pattern Analysis and Computer Vision (PA VIS), Istituto Italiano di Tecnologia (IIT)<br>
‚Ä¢ Dataset: Epic-Kitchens-100 (Next-Active-Object annotations), Samples: None, Modality: RGB videos + next-active-object bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Neural-Network-Driven Method for Optimal Path Planning via High-Accuracy Region Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.07974"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Japan<br>
‚Ä¢ Dataset: Complex Environment Motion Planning (CEMP), Samples: 16000, Modality: 2D RGB maps with optimal path regions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Real-time Automatic M-mode Echocardiography Measurement with Panel Attention from Local-to-Global Pixels</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.07717"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hanktseng131415go/RAMEM"><img src="https://img.shields.io/github/stars/hanktseng131415go/RAMEM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science
The University of Manchester<br>
‚Ä¢ Dataset: MEIS, Samples: 2639, Modality: M-mode echocardiography images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Understanding User Behavior in Volumetric Video Watching: Dataset, Analysis and Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.07578"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cuhksz-inml.github.io/user-behavior-in-vv-watching/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Future Network of Intelligence Institute, The Chinese University of Hong Kong, Shenzhen; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen<br>
‚Ä¢ Dataset: Volumetric Video Viewing Behavior Dataset, Samples: 300, Modality: 6DoF headset trajectories, 6DoF controller trajectories, Gaze data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Generalizing Event-Based Motion Deblurring in Real-World Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.05932"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XiangZ-0/GEM"><img src="https://img.shields.io/github/stars/XiangZ-0/GEM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Wuhan University<br>
‚Ä¢ Dataset: Multi-Scale Real-world Blurry Dataset (MS-RBD), Samples: 32, Modality: RGB frames + events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Joint-Relation Transformer for Multi-Person Motion Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.04808"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MediaBrain-SJTU/JRTransformer"><img src="https://img.shields.io/github/stars/MediaBrain-SJTU/JRTransformer.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: 3DPW-SoMoF/RC, Samples: None, Modality: 3D skeleton joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>SODFormer: Streaming Object Detection with Transformer Using Events and Frames</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.04047"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dianzl/SODFormer"><img src="https://img.shields.io/github/stars/dianzl/SODFormer.svg?style=social&label=Star"></a><br><a href="https://www.pkuml.org/research/pku-davis-sod-dataset.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Engineering Research Center for Visual Technology, School of Computer Science, Peking University, Beijing 100871, China<br>
‚Ä¢ Dataset: PKU-DAVIS-SOD, Samples: 220, Modality: asynchronous events, frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Who Is Alyx? A new Behavioral Biometric Dataset for User Identification in XR</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.03788"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cschell/who-is-alyx"><img src="https://img.shields.io/github/stars/cschell/who-is-alyx.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Human-Computer Interaction (HCI) Group, Informatik, University of W¬®urzburg, W¬®urzburg, Germany<br>
‚Ä¢ Dataset: Who Is Alyx?, Samples: 71 users, 2 sessions/user, 45 mins/session, Modality: VR motion tracking (HMD, controllers), eye-tracking, physiological signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Reconstructing Three-Dimensional Models of Interacting Humans</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.01854"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ci3d.imar.ro"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Mathematics of the Romanian Academy<br>
‚Ä¢ Dataset: CHI3D, Samples: 631, Modality: MoCap joints, GHUM/SMPLX parameters, multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>MVFlow: Deep Optical Flow Estimation of Compressed Videos with Motion Vector Prior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.01568"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: Compressed FlyingThings3D, Samples: None, Modality: compressed RGB videos + motion vectors + optical flow<br>
‚Ä¢ Dataset: Compressed MPI Sintel, Samples: None, Modality: compressed RGB videos + motion vectors + optical flow<br>
‚Ä¢ Dataset: Compressed KITTI 2012, Samples: None, Modality: compressed RGB videos + motion vectors + optical flow<br>
‚Ä¢ Dataset: Compressed KITTI 2015, Samples: None, Modality: compressed RGB videos + motion vectors + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Efficient neural supersampling on a novel gaming dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.01483"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Qualcomm AI Research<br>
‚Ä¢ Dataset: QRISP (Qualcomm Rasterized Images for Super-resolution Processing), Samples: 8760, Modality: RGB videos + depth + motion vectors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>On the Generation of a Synthetic Event-Based Vision Dataset for Navigation and Landing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.00394"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.com/EuropeanSpaceAgency/trajectory-to-events"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Advanced Concepts Team, European Space Agency, European Space Research and Technology Centre (ESTEC), Keplerlaan 1, 2201 AZ Noordwijk, The Netherlands<br>
‚Ä¢ Dataset: None, Samples: 500, Modality: ['optimal landing trajectories', 'photorealistic video', 'event streams', 'motion field ground truth']<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.16897"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="ivl.cs.brown.edu/research/diva"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: DiVa-360, Samples: 54, Modality: Multi-view RGB videos, foreground-background segmentation masks, synchronized audio, text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Towards Imbalanced Motion: Part-Decoupling Network for Video Portrait Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.16565"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing 100191 , China<br>
‚Ä¢ Dataset: MVPS, Samples: 101, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.16062"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.11237258"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Eindhoven University of Technology<br>
‚Ä¢ Dataset: None, Samples: 544, Modality: Motion tracker trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.15942"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XiaRho/CMDA"><img src="https://img.shields.io/github/stars/XiaRho/CMDA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: East China University of Science and Technology<br>
‚Ä¢ Dataset: DSEC Night-Semantic, Samples: 1692 training samples, 150 testing samples, Modality: Images + Events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Supervised Homography Learning with Realistic Dataset Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.15353"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JianghaiSCU/RealSH"><img src="https://img.shields.io/github/stars/JianghaiSCU/RealSH.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sichuan University, Megvii Technology<br>
‚Ä¢ Dataset: CA-sup, Samples: 800000, Modality: RGB image pairs + homography labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.15055"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pointodyssey.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: PointOdyssey, Samples: 104, Modality: Synthetic RGB videos + 2D/3D point trajectories, depth, normals, segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Event-based Vision for Early Prediction of Manipulation Actions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.14332"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DaniDeniz/DavisHandDataset-Events"><img src="https://img.shields.io/github/stars/DaniDeniz/DavisHandDataset-Events.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Architecture and Technology, CITIC, University of Granada<br>
‚Ä¢ Dataset: Event-based Manipulation Action Dataset (E-MAD), Samples: 750, Modality: event stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.10713"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jspenmar/slowtv_monodepth"><img src="https://img.shields.io/github/stars/jspenmar/slowtv_monodepth.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Surrey<br>
‚Ä¢ Dataset: SlowTV, Samples: 40, Modality: monocular video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.10173"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dna-rendering.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory<br>
‚Ä¢ Dataset: DNA-Rendering, Samples: 5000, Modality: multi-view RGB videos, depth data, 2D/3D keypoints, SMPLX models, foreground masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>AGAR: Attention Graph-RNN for Adaptative Motion Prediction of Point Clouds of Deformable Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.09936"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/pedro-dm-gomes/AGAR"><img src="https://img.shields.io/github/stars/pedro-dm-gomes/AGAR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University College London<br>
‚Ä¢ Dataset: Mixamo Human Bodies Activities, Samples: 9527, Modality: Point Cloud Sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Online Self-Supervised Thermal Water Segmentation for Aerial Vehicles</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.09027"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/connorlee77/uav-thermal-water-segmentation"><img src="https://img.shields.io/github/stars/connorlee77/uav-thermal-water-segmentation.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: California Institute of Technology<br>
‚Ä¢ Dataset: Aerial and Ground Thermal Near-shore Dataset, Samples: 20, Modality: Thermal video sequences + IMU data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>A Study in Zucker: Insights on Interactions Between Humans and Small Service Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.08668"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motion-lab.github.io/ZuckerDataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computing at Clemson University, SC, USA<br>
‚Ä¢ Dataset: Zucker Dataset, Samples: 309 human trajectories, 97 robot trajectories, Modality: Pose trajectories from HTC Vive Trackers<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Video Frame Interpolation with Stereo Event and Intensity Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.08228"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dingchao1214.github.io/web sevfi/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University, Wuhan 430072, China<br>
‚Ä¢ Dataset: Stereo Event-Intensity Dataset (SEID), Samples: 34, Modality: Stereo events, RGB frames, and depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>TVPR: Text-to-Video Person Retrieval and a New Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.07184"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing Tech University<br>
‚Ä¢ Dataset: Text-to-Video Person Re-identification (TVPReid), Samples: 6559, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Towards Anytime Optical Flow Estimation with Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.05033"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yaozhuwa/EVA-Flow"><img src="https://img.shields.io/github/stars/Yaozhuwa/EVA-Flow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, College of Optical Science and Engineering, Zhejiang University, Hangzhou 310027, China<br>
‚Ä¢ Dataset: EVA-FlowSet, Samples: 4, Modality: Event camera data for optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>FTFDNet: Learning to Detect Talking Face Video Manipulation with Tri-Modality Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.03990"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ASGO, School of Computer Science, Northwestern Polytechnical University, Xi‚Äôan, China<br>
‚Ä¢ Dataset: Fake Talking Face Detection Dataset (FTFDD), Samples: 64679, Modality: RGB videos + audio + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Ground-Challenge: A Multi-sensor SLAM Dataset Focusing on Corner Cases for Ground Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.03890"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sjtuyinjie/Ground-Challenge"><img src="https://img.shields.io/github/stars/sjtuyinjie/Ground-Challenge.svg?style=social&label=Star"></a><br><a href="https://github.com/sjtuyinjie/Ground-Challenge"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: independent researchers<br>
‚Ä¢ Dataset: Ground-Challenge, Samples: 36, Modality: RGB-D camera, IMU, wheel odometer, 3D LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Safe & Accurate at Speed with Tendons: A Robot Arm for Exploring Dynamic Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.02654"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="webdav.tuebingen.mpg.de/pamy2"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, 72076 T ¬®ubingen, Germany.<br>
‚Ä¢ Dataset: Pamy2 Proprioceptive Dataset, Samples: 25 days, Modality: Proprioceptive data: joint positions, joint velocities, observed and desired muscle pressures.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Unveiling the Potential of Spike Streams for Foreground Occlusion Removal from Densely Continuous Views</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.00821"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: S-OCC, Samples: 128, Modality: Spike streams and ground truth background images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.00818"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/IDEA-Research/Motion-X"><img src="https://img.shields.io/github/stars/IDEA-Research/Motion-X.svg?style=social&label=Star"></a><br><a href="https://motion-x-dataset.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: International Digital Economy Academy (IDEA)<br>
‚Ä¢ Dataset: Motion-X, Samples: 81100, Modality: SMPL-X parameters + Text descriptions + RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.00595"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="rh20t.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: RH20T, Samples: 110000, Modality: RGB images, depth images, binocular IR images, 6DoF force-torque, audio, proprioception (joint angles, joint torques, end-effector Cartesian pose, gripper states), fingertip tactile<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.17010"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Toytiny/milliFlow"><img src="https://img.shields.io/github/stars/Toytiny/milliFlow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Edinburgh<br>
‚Ä¢ Dataset: milliFlow Dataset, Samples: 216, Modality: mmWave radar point clouds, RGB-D images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.16940"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bedlam.is.tue.mpg.de/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T√ºbingen, Germany<br>
‚Ä¢ Dataset: BEDLAM, Samples: 2311, Modality: RGB videos + SMPL-X ground truth + depth maps + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.16917"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://davidrecasens.github.io/TheDrunkard‚ÄôsOdometry/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Zaragoza<br>
‚Ä¢ Dataset: The Drunkard's Dataset, Samples: 19, Modality: RGB images, depth maps, optical flow, normal maps, camera pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Action-conditioned Deep Visual Prediction with RoAM, a new Indoor Human Motion Dataset for Autonomous Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.15852"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tinyurl.com/RoAMData"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Aerospace Engineering Dept, Indian Institute of Science, Bangalore<br>
‚Ä¢ Dataset: RoAM, Samples: 25, Modality: Stereo color images, 2D LiDAR scan, Odometry, IMU, depth maps, robot control actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Detector-Free Structure from Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.15669"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zju3dv/DetectorFreeSfM"><img src="https://img.shields.io/github/stars/zju3dv/DetectorFreeSfM.svg?style=social&label=Star"></a><br><a href="https://zju3dv.github.io/DetectorFreeSfM/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: Texture-Poor SfM Dataset, Samples: 1020, Modality: RGB videos + ground-truth camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Self-supervised Learning of Event-guided Video Frame Interpolation for Rolling Shutter Frames</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.15507"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yunfanLu/Self-EvRSVFI"><img src="https://img.shields.io/github/stars/yunfanLu/Self-EvRSVFI.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AI Thrust, HKUST(GZ)<br>
‚Ä¢ Dataset: ERS, Samples: 29, Modality: RGB videos + events<br>
‚Ä¢ Dataset: UAV-RS, Samples: 9, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>AutoGraph: Predicting Lane Graphs from Traffic Observations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.15410"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://autograph.cs.uni-freiburg.de/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Engineering, University of Freiburg, 79115 Freiburg, Germany<br>
‚Ä¢ Dataset: UrbanTracklet, Samples: 882592, Modality: Vehicle tracklets from LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>BotanicGarden: A High-Quality Dataset for Robot Navigation in Unstructured Natural Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.14137"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/robot-pesg/BotanicGarden"><img src="https://img.shields.io/github/stars/robot-pesg/BotanicGarden.svg?style=social&label=Star"></a><br><a href="https://github.com/robot-pesg/BotanicGarden"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Sensing Science and Engineering, Shanghai Jiao Tong University, Shanghai 200240, China<br>
‚Ä¢ Dataset: BotanicGarden, Samples: 33, Modality: Stereo cameras (Gray, RGB), 3D LiDAR, IMU, wheel odometry, pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>The MI-Motion Dataset and Benchmark for 3D Multi-Person Motion Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.13566"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mi-motion.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hangzhou Dianzi University<br>
‚Ä¢ Dataset: MI-Motion, Samples: 210, Modality: Game engine synthesized 3D skeleton poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Audio-Driven 3D Facial Animation from In-the-Wild Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.11541"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://faw3d.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Internation Digital Economy Academy<br>
‚Ä¢ Dataset: HDTF-3D, Samples: 133, Modality: 3D facial motion parameters (FLAME)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Robot Learning with Sensorimotor Pre-training</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.10007"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/irados/rpt"><img src="https://img.shields.io/github/stars/irados/rpt.svg?style=social&label=Star"></a><br><a href="https://bair.berkeley.edu/blog/2023/06/20/rpt/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Berkeley<br>
‚Ä¢ Dataset: N/A, Samples: 20000, Modality: multi-view RGB images + proprioceptive robot states + actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.09126"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sony/audio-visual-seld-dcase2023"><img src="https://img.shields.io/github/stars/sony/audio-visual-seld-dcase2023.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/record/7880637"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sony AI<br>
‚Ä¢ Dataset: STARSS23, Samples: 168, Modality: Multichannel audio, 360¬∞ video, MoCap-derived pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Emotional Speech-Driven Animation with Content-Emotion Disentanglement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.08990"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/radekdanecek/EMOTE"><img src="https://img.shields.io/github/stars/radekdanecek/EMOTE.svg?style=social&label=Star"></a><br><a href="https://emote.is.tue.mpg.de/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T ¬®ubingen, Germany<br>
‚Ä¢ Dataset: Pseudo ground-truth 3D (FLAME parameters) for the MEAD dataset, Samples: None, Modality: 3D facial motion parameters (FLAME)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Motion Capture Dataset for Practical Use of AI-based Motion Editing and Stylization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.08861"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://acesinc.co.jp/projects/project-1760"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ACES Inc., Japan<br>
‚Ä¢ Dataset: Bandai-Namco-Research-Motiondataset-1, Samples: None, Modality: MoCap joints<br>
‚Ä¢ Dataset: Bandai-Namco-Research-Motiondataset-2, Samples: None, Modality: MoCap joints<br>
‚Ä¢ Dataset: Emote motion dataset, Samples: 42, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Instant Multi-View Head Capture through Learnable Registration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.07437"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tempeh.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Intelligent Systems, T√ºbingen<br>
‚Ä¢ Dataset: FaMoS, Samples: 2660, Modality: multi-view gray-scale images + 3D head scans<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>4DHumanOutfit: a multi-subject 4D dataset of human motion sequences in varying outfits exhibiting large displacements</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.07399"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://kinovis.inria.fr/4dhumanoutfit/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NAVER LABS Europe<br>
‚Ä¢ Dataset: 4DHumanOutfit, Samples: 1540, Modality: RGB videos + 3D mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Anomaly Detection in Satellite Videos using Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.05376"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, Houston, TX 77004<br>
‚Ä¢ Dataset: GOES-16/17 Wildfire Anomaly Dataset, Samples: 520, Modality: Satellite videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Dance Generation by Sound Symbolic Words</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.03646"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/onomatopoeia-dance/home/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Tsukuba, Japan<br>
‚Ä¢ Dataset: Onomatopoeia-dance motion pairs, Samples: 44, Modality: 3D motion data (AIST++) and time-aligned onomatopoeia annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>VR.net: A Real-world Dataset for Virtual Reality Motion Sickness Research</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.03381"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vrhook.ahlab.org"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Auckland<br>
‚Ä¢ Dataset: VR.net, Samples: 12 hours of gameplay videos, Modality: RGB videos, depth maps, motion vectors, camera/object/headset pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.02850"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Arthur151/DynaCam"><img src="https://img.shields.io/github/stars/Arthur151/DynaCam.svg?style=social&label=Star"></a><br><a href="https://www.yusun.work/TRACE/TRACE.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Harbin Institute of Technology<br>
‚Ä¢ Dataset: DynaCam, Samples: more than 500 annotated DC-videos, Modality: RGB videos with camera poses and pseudo-ground-truth 3D human annotations (pose, shape, global trajectories)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>A Multi-Modal Transformer Network for Action Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.19624"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://aiaiproject.weebly.com/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA 22904<br>
‚Ä¢ Dataset: instructional activity dataset, Samples: None, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Large Car-following Data Based on Lyft level-5 Open Dataset: Following Autonomous Vehicles vs. Human-driven Vehicles</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.18921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/RomainLITUD/Car-Following-Dataset-HV-vs-AV"><img src="https://img.shields.io/github/stars/RomainLITUD/Car-Following-Dataset-HV-vs-AV.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Transport & Planning, Delft University of Technology, the Netherlands<br>
‚Ä¢ Dataset: Car-Following-Dataset-HV-vs-AV, Samples: 72341, Modality: Vehicle trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.18891"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XingqunQi-lab/EmotionGestures"><img src="https://img.shields.io/github/stars/XingqunQi-lab/EmotionGestures.svg?style=social&label=Star"></a><br><a href="https://xingqunqi-lab.github.io/Emotion-Gesture-Web/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: the Academy of Interdisciplinary Studies, The Hong Kong University of Science and Technology, Hong Kong, China<br>
‚Ä¢ Dataset: TED Emotion dataset, Samples: 78734, Modality: 3D upper body joints (pseudo ground truth), speech audio, text transcripts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Motion-Scenario Decoupling for Rat-Aware Video Position Prediction: Strategy and Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.18310"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dalian University of Technology, Liaoning, China<br>
‚Ä¢ Dataset: RatPose, Samples: 1023, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Alignment-free HDR Deghosting with Semantics Consistent Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.18135"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://steven-tel.github.io/sctnet/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Burgundy, ImViA<br>
‚Ä¢ Dataset: SCTNet HDR Deghosting Dataset, Samples: 144, Modality: LDR images of dynamic scenes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Z-GMOT: Zero-shot Generic Multiple Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.17648"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fsoft-aic.github.io/Z-GMOT"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: FPT Software AI Center, Vietnam<br>
‚Ä¢ Dataset: Referring GMOT dataset, Samples: 98, Modality: RGB videos + textual descriptions<br>
‚Ä¢ Dataset: Refer-GMOT40, Samples: 40, Modality: RGB videos + textual descriptions<br>
‚Ä¢ Dataset: Refer-Animal, Samples: 58, Modality: RGB videos + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>BASED: Benchmarking, Analysis, and Structural Estimation of Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.17477"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/illaitar/based"><img src="https://img.shields.io/github/stars/illaitar/based.svg?style=social&label=Star"></a><br><a href="https://videoprocessing.ai/benchmarks/deblurring.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Lomonosov Moscow State University<br>
‚Ä¢ Dataset: BASED, Samples: 23, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Comparison of Pedestrian Prediction Models from Trajectory and Appearance Data for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.15942"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Applied Research Team, Five AI (Bosch UK), Edinburgh, United Kingdom<br>
‚Ä¢ Dataset: NuScenes-Appearance, Samples: None, Modality: RGB images + 3D trajectories<br>
‚Ä¢ Dataset: motion-changes dataset, Samples: None, Modality: RGB images + 3D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Malicious or Benign? Towards Effective Content Moderation for Children's Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.15551"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/syedhammadahmed/mob"><img src="https://img.shields.io/github/stars/syedhammadahmed/mob.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of Central Florida, Orlando, FL USA<br>
‚Ä¢ Dataset: Malicious or Benign (MOB), Samples: 1565, Modality: RGB videos + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>EgoVSR: Towards High-Quality Egocentric Video Super-Resolution</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.14708"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chiyich/EGOVSR/"><img src="https://img.shields.io/github/stars/EGOVSR/.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen International Graduate School, Tsinghua University<br>
‚Ä¢ Dataset: EgoVSR, Samples: 46724, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>FEDORA: Flying Event Dataset fOr Reactive behAvior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.14392"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Purdue University, West Lafayette, IN 47907, USA<br>
‚Ä¢ Dataset: FEDORA, Samples: 5, Modality: RGB images, Event streams, IMU data, Depth, Pose, Optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Flare-Aware Cross-modal Enhancement Network for Multi-spectral Vehicle Re-identification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.13659"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Anonymous<br>
‚Ä¢ Dataset: WMVeID863, Samples: 4709, Modality: RGB, NI, and TI image triplets<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.13353"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://RenderMe-360.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory, SenseTime<br>
‚Ä¢ Dataset: RenderMe-360, Samples: 800000, Modality: Synchronized multi-view HD videos (60 cameras, 30FPS), audio, 3D scans, 2D/3D facial landmarks, FLAME models, text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>SIDAR: Synthetic Image Dataset for Alignment & Restoration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.12036"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Vision & Remote Sensing, Technische Universit√§t Berlin<br>
‚Ä¢ Dataset: SIDAR, Samples: None, Modality: Rendered RGB images + occlusion masks + homographies<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>ZeroFlow: Scalable Scene Flow via Distillation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.10424"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kylevedder/zeroflow"><img src="https://img.shields.io/github/stars/kylevedder/zeroflow.svg?style=social&label=Star"></a><br><a href="https://vedder.io/zeroflow"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Pennsylvania<br>
‚Ä¢ Dataset: Argoverse 2 Scene Flow Pseudo-Labels, Samples: 700 training sequences, Modality: LiDAR + scene flow pseudo-labels<br>
‚Ä¢ Dataset: Waymo Open Scene Flow Pseudo-Labels, Samples: 798 training sequences, Modality: LiDAR + scene flow pseudo-labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.09662"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://azadis.github.io/make-an-animation"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta AI<br>
‚Ä¢ Dataset: Text Pseudo-Pose (TPP) dataset, Samples: 35000000, Modality: text descriptions + 3D pseudo-pose SMPL annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>AMD: Autoregressive Motion Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.09381"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology, Zhejiang Univerisity<br>
‚Ä¢ Dataset: HumanLong3D, Samples: 43696, Modality: 3D human motions + text<br>
‚Ä¢ Dataset: HumanMusic, Samples: 137136, Modality: 3D human motions + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Motion Question Answering via Modular Motion Programs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.08953"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/markendo/HumanMotionQA/"><img src="https://img.shields.io/github/stars/HumanMotionQA/.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Stanford University<br>
‚Ä¢ Dataset: BABEL-QA, Samples: 1109, Modality: MoCap joints, rotations, body and hand meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Benchmarking UWB-Based Infrastructure-Free Positioning and Multi-Robot Relative Localization: Dataset and Characterization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.08532"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/TIERS/uwb-relative-localization-dataset"><img src="https://img.shields.io/github/stars/TIERS/uwb-relative-localization-dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Turku Intelligent Embedded and Robotic Systems (TIERS) Lab, University of Turku, Finland.<br>
‚Ä¢ Dataset: UWB-Based Infrastructure-Free Positioning and Multi-Robot Relative Localization Dataset, Samples: 24, Modality: ['UWB ranging', 'MOCAP trajectories', 'IMU', 'Odometry', 'VIO']<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Lightweight Delivery Detection on Doorbell Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.07812"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Qualcomm Technologies<br>
‚Ä¢ Dataset: Doorbell delivery detection dataset, Samples: 10873, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with Bird's Eye View based Appearance and Motion Features</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.07336"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xiekkki/motionbev"><img src="https://img.shields.io/github/stars/xiekkki/motionbev.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Automation, Southeast University<br>
‚Ä¢ Dataset: SipailouCampus, Samples: 26279 frames, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.07214"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/MMGEgo4D"><img src="https://img.shields.io/github/stars/facebookresearch/MMGEgo4D.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Texas at Austin<br>
‚Ä¢ Dataset: MMG-Ego4D, Samples: None, Modality: video, audio, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.06477"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC V6T 1Z4, Canada<br>
‚Ä¢ Dataset: IR-labelled tissue dataset, Samples: 1321, Modality: Stereo surgical videos with IR ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.06356"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="www.actors-hq.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Synthesia, Germany<br>
‚Ä¢ Dataset: ActorsHQ, Samples: 16, Modality: multi-view 12MP RGB videos + per-frame 3D meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.05301"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/clementinboittiaux/sfm-pipeline"><img src="https://img.shields.io/github/stars/clementinboittiaux/sfm-pipeline.svg?style=social&label=Star"></a><br><a href="https://seanoe.org/data/00810/92226/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ifremer, Zone Portuaire de Br¬¥egaillon, La Seyne-sur-Mer, France<br>
‚Ä¢ Dataset: Eiffel Tower, Samples: 18082, Modality: RGB images + 6DOF camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.03713"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://research.nvidia.com/labs/nxp/avatar-fingerprinting/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA<br>
‚Ä¢ Dataset: NVIDIA FacialReenactment (NVFAIR), Samples: 654726, Modality: RGB videos of talking heads<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.03187"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Georgia Institute of Technology<br>
‚Ä¢ Dataset: Virtual IMU data generated from virtual textual descriptions, Samples: 1600, Modality: Virtual accelerometer data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.03027"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tobias-kirschstein.github.io/nersemble"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich, Germany<br>
‚Ä¢ Dataset: NeRSemble, Samples: 4734, Modality: Multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.01618"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zehaozhu.github.io/ContactArt/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Texas at Austin<br>
‚Ä¢ Dataset: ContactArt, Samples: 552000, Modality: Hand poses, articulated object poses, contact regions, rendered RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.01241"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vimeo.com/823756031"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Social Cognitive Systems Group, Bielefeld University, Germany<br>
‚Ä¢ Dataset: BiGe dataset, Samples: 54360, Modality: 3D full-body joints, audio, text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.00767"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cao-cong/RViDeformer"><img src="https://img.shields.io/github/stars/cao-cong/RViDeformer.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical and Information Engineering, Tianjin University<br>
‚Ä¢ Dataset: ReCRVD, Samples: 120, Modality: paired noisy-clean raw videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Event-Free Moving Object Segmentation from Moving Ego Vehicle</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.00126"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZZY-Zhou/DSEC-MOS"><img src="https://img.shields.io/github/stars/ZZY-Zhou/DSEC-MOS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Burgundy, Dijon, France; University of Wurzburg, Wurzburg, Germany<br>
‚Ä¢ Dataset: DSEC-MOS, Samples: 13314, Modality: RGB videos, Event camera data, pixel-level moving object segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Density Invariant Contrast Maximization for Neuromorphic Earth Observations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.14125"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/neuromorphicsystems/event_warping"><img src="https://img.shields.io/github/stars/neuromorphicsystems/event_warping.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Western Sydney University<br>
‚Ä¢ Dataset: ISS event dataset, Samples: 10, Modality: Event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>What Happened 3 Seconds Ago? Inferring the Past with Thermal Imaging</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.13651"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZitianTang/Thermal-IM"><img src="https://img.shields.io/github/stars/ZitianTang/Thermal-IM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IIIS, Tsinghua University<br>
‚Ä¢ Dataset: Thermal-IM, Samples: 783, Modality: RGB-Thermal videos, RGB-Depth videos, 2D/3D human poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Dynamic Video Frame Interpolation with integrated Difficulty Pre-Assessment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.12664"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Samsung Electronics (China) R&D Center<br>
‚Ä¢ Dataset: VFI Difficulty Assessment dataset, Samples: 13030, Modality: RGB video frames + difficulty scores<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.12281"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/showlab/HOSNeRF"><img src="https://img.shields.io/github/stars/showlab/HOSNeRF.svg?style=social&label=Star"></a><br><a href="https://showlab.github.io/HOSNeRF"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Show Lab, National University of Singapore<br>
‚Ä¢ Dataset: HOSNeRF dataset, Samples: 6, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>MAMAF-Net: Motion-Aware and Multi-Attention Fusion Network for Stroke Diagnosis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.09466"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: VTT Technical Research Centre of Finland<br>
‚Ä¢ Dataset: Stroke-data, Samples: 148, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Text2Performer: Text-Driven Human Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.08483"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yumingj.github.io/projects/Text2Performer.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: Fashion-Text2Video Dataset, Samples: 600, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Learning How To Robustly Estimate Camera Pose in Endoscopic Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.08023"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/aimi-lab/robust-pose-estimator"><img src="https://img.shields.io/github/stars/aimi-lab/robust-pose-estimator.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.7727692"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ARTORG Center, University of Bern, Switzerland<br>
‚Ä¢ Dataset: StereoMIS, Samples: 16, Modality: Stereo videos + camera pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Text-Conditional Contextualized Avatars For Zero-Shot Personalization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.07410"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta AI<br>
‚Ä¢ Dataset: Image Text Pseudo-Pose (ITPP), Samples: 35000000, Modality: text + 3D SMPL poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Self-Supervised Scene Dynamic Recovery from Rolling Shutter Images and Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.06930"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/w3un/SelfUnroll"><img src="https://img.shields.io/github/stars/w3un/SelfUnroll.svg?style=social&label=Star"></a><br><a href="https://w3un.github.io/selfunroll/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University<br>
‚Ä¢ Dataset: DAVIS-RS-Event (DRE), Samples: 100, Modality: Rolling Shutter (RS) images + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>FollowMe: Vehicle Behaviour Prediction in Autonomous Vehicle Settings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.06121"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Texas at Austin<br>
‚Ä¢ Dataset: FollowMe, Samples: 384 sequences (32 participants x 12 scenarios), Modality: Vehicle trajectories (positions p=(x,y) over time)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.05684"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Tr3e/InterGen"><img src="https://img.shields.io/github/stars/Tr3e/InterGen.svg?style=social&label=Star"></a><br><a href="https://tr3e.github.io/intergen-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: InterHuman, Samples: 7779, Modality: skeletal motions + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>FIR-based Future Trajectory Prediction in Nighttime Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.05345"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/FordCVResearch/FIR-Trajectory-Prediction"><img src="https://img.shields.io/github/stars/FordCVResearch/FIR-Trajectory-Prediction.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ford Motor Company, GreenÔ¨Åeld Labs, Palo Alto, CA, USA<br>
‚Ä¢ Dataset: FIR-based Large Animal Detection and Trajectory Prediction Dataset, Samples: 26127, Modality: FIR videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.05170"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://deeperaction.github.io/datasets/sportsmot.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory for Novel Software Technology, Nanjing University, China<br>
‚Ä¢ Dataset: SportsMOT, Samples: 240, Modality: RGB videos + bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Multi-Object Tracking by Iteratively Associating Detections with Uniform Appearance for Trawl-Based Fishing Bycatch Monitoring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.04816"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical & Computer Engineering, University of Washington, United States<br>
‚Ä¢ Dataset: NIWA underwater fish dataset, Samples: 8, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>WOMD-LiDAR: Raw Sensor Dataset Benchmark for Motion Forecasting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.03834"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://waymo.com/open/data/motion/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Waymo LLC<br>
‚Ä¢ Dataset: WOMD-LiDAR, Samples: 104000, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Motion Capture Benchmark of Real Industrial Tasks and Traditional Crafts for Human Movement Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.03771"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/olivas-bre/GOM.git"><img src="https://img.shields.io/github/stars/olivas-bre/GOM.git.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.5356992"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Centre for Robotics, Mines Paris, Universit√© PSL, 75006 Paris, France<br>
‚Ä¢ Dataset: TV assembly (TVA), Samples: 479, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
‚Ä¢ Dataset: TV packaging (TVP), Samples: 54, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
‚Ä¢ Dataset: Airplane floater assembly (APA), Samples: 19, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
‚Ä¢ Dataset: Postures according to EAWS protocol (ERGD), Samples: 840, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
‚Ä¢ Dataset: Silk weaving (SLW), Samples: 308, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
‚Ä¢ Dataset: Glass blowing (GLB), Samples: 152, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
‚Ä¢ Dataset: Mastic cultivation (MSC), Samples: 83, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Automatic Detection of Reactions to Music via Earable Sensing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.03295"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KOREATECH, Republic of Korea<br>
‚Ä¢ Dataset: MusicReactionSet, Samples: 240, Modality: IMU and audio<br>
‚Ä¢ Dataset: controlled dataset, Samples: 30, Modality: IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>DEFLOW: Self-supervised 3D Motion Estimation of Debris Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.02569"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/liyzhu/DEFLOW"><img src="https://img.shields.io/github/stars/liyzhu/DEFLOW.svg?style=social&label=Star"></a><br><a href="https://liyzhu.github.io/DEFLOW/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Geodesy and Photogrammetry, ETH Zurich<br>
‚Ä¢ Dataset: Debris flow dataset, Samples: 6000, Modality: RGB images + LiDAR point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Re-Evaluating LiDAR Scene Flow for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.02150"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/n-chodosh/re-evaluating-lidar-flow"><img src="https://img.shields.io/github/stars/n-chodosh/re-evaluating-lidar-flow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: Argoverse 2.0 flow labels, Samples: , Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.01186"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://follow-your-pose.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China<br>
‚Ä¢ Dataset: LAION-Pose, Samples: , Modality: image-text-pose pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.01168"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: DeepAccident, Samples: 285000 samples, Modality: multi-view cameras, LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>CIMI4D: A Large Multimodal Climbing Motion Dataset under Human-scene Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.17948"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.lidarhumanmotion.net/cimi4d/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University<br>
‚Ä¢ Dataset: CIMI4D, Samples: 42, Modality: RGB videos, LiDAR point clouds, IMU measurements, static point cloud scenes, reconstructed scene meshes, annotated human poses, global trajectories, contact annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>CIRCLE: Capture In Rich Contextual Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.17912"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jp-araujo/CIRCLE"><img src="https://img.shields.io/github/stars/jp-araujo/CIRCLE.svg?style=social&label=Star"></a><br><a href="https://jp-araujo.github.io/CIRCLE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: CIRCLE, Samples: 7000, Modality: ['SMPL-X parameters', 'VR headset trajectory', 'Egocentric RGB-D video', 'MoCap joints']<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>CryoFormer: Continuous Heterogeneous Cryo-EM Reconstruction using Transformer-based Neural Representations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.16254"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cryoformer.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University, Cellverse, HKUST<br>
‚Ä¢ Dataset: PEDV Spike Protein Dataset, Samples: 50000, Modality: Atomic models (PDB), density maps (MRC), simulated 2D projection images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Multimodal video and IMU kinematic dataset on daily life activities using affordable devices (VIDIMU)</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.16150"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/twyncoder/vidimu-tools"><img src="https://img.shields.io/github/stars/twyncoder/vidimu-tools.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.7681316"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Valladolid, Valladolid, Spain<br>
‚Ä¢ Dataset: VIDIMU, Samples: 702, Modality: RGB video + IMU data + 3D body pose + 3D joint angles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Recovering 3D Hand Mesh Sequence from a Single Blurry Image: A New Dataset and Temporal Unfolding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.15417"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JaehaKim97/BlurHand_RELEASE"><img src="https://img.shields.io/github/stars/JaehaKim97/BlurHand_RELEASE.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of ECE&ASRI, Seoul National University, Korea<br>
‚Ä¢ Dataset: BlurHand, Samples: 155896, Modality: RGB images + 3D hand mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>NeuralPCI: Spatio-temporal Neural Field for 3D Point Cloud Multi-frame Non-linear Interpolation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.15126"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ispc-lab/NeuralPCI"><img src="https://img.shields.io/github/stars/ispc-lab/NeuralPCI.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tongji University<br>
‚Ä¢ Dataset: NL-Drive, Samples: None, Modality: LiDAR point cloud sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.14840"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Junggy/HAMMER-dataset"><img src="https://img.shields.io/github/stars/Junggy/HAMMER-dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich<br>
‚Ä¢ Dataset: HAMMER, Samples: 26 trajectories (~13k frames), Modality: Robot pose trajectories, RGB+Polarization, D-ToF, I-ToF, Stereo (passive/active), Dense ground truth depth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.14435"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JokerYan/NeRF-DS"><img src="https://img.shields.io/github/stars/JokerYan/NeRF-DS.svg?style=social&label=Star"></a><br><a href="https://github.com/JokerYan/NeRF-DS"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, National University of Singapore<br>
‚Ä¢ Dataset: Dynamic Specular Dataset, Samples: 8, Modality: monocular RGB videos + camera poses + object masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>OPDMulti: Openable Part Detection for Multiple Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.14087"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/3dlg-hcvc/OPDMulti"><img src="https://img.shields.io/github/stars/3dlg-hcvc/OPDMulti.svg?style=social&label=Star"></a><br><a href="https://3dlg-hcvc.github.io/OPDMulti/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Simon Fraser University<br>
‚Ä¢ Dataset: OPDMulti, Samples: 64213, Modality: RGB-D images + part masks + motion parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>MusicFace: Music-driven Expressive Singing Face Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.14044"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vcg.xmu.edu.cn/datasets/singingface/index.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Informatics, Xiamen University, Xiamen, 361000, China<br>
‚Ä¢ Dataset: SingingFace, Samples: 600, Modality: RGB videos with extracted 3D face pose and expression parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Progressively Optimized Local Radiance Fields for Robust View Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.13791"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/localrf"><img src="https://img.shields.io/github/stars/facebookresearch/localrf.svg?style=social&label=Star"></a><br><a href="https://localrf.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST<br>
‚Ä¢ Dataset: Static Hikes, Samples: 12, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.13767"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/flyLu/STIR-EG-VSR"><img src="https://img.shields.io/github/stars/flyLu/STIR-EG-VSR.svg?style=social&label=Star"></a><br><a href="https://vlis2022.github.io/cvpr23/egvsr"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AI Thrust, HKUST(GZ)<br>
‚Ä¢ Dataset: ALPIX-VSR, Samples: 26 video sequences, Modality: RGB videos + Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>3D-POP -- An automated annotation approach to facilitate markerless 2D-3D tracking of freely moving birds with marker-based motion capture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.13174"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/alexhang212/Dataset-3DPOP"><img src="https://img.shields.io/github/stars/alexhang212/Dataset-3DPOP.svg?style=social&label=Star"></a><br><a href="https://tinyurl.com/4ckbjcpx"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Collective Behavior and Dept. of Ecology of Animal Societies, Max Planck Institute of Animal Behavior<br>
‚Ä¢ Dataset: 3D-POP, Samples: 57, Modality: MoCap (6-DOF pose, 3D coordinates), RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Music-Driven Group Choreography</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.12337"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://aioz-ai.github.io/AIOZ-GDANCE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AIOZ, Singapore<br>
‚Ä¢ Dataset: AIOZ-GDANCE, Samples: 1808000, Modality: 3D Mesh + Music Audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Motion Matters: Neural Motion Transfer for Better Camera Physiological Measurement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.12059"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motion-matters.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UNC Chapel Hill<br>
‚Ä¢ Dataset: CDVS, Samples: 90, Modality: RGB videos<br>
‚Ä¢ Dataset: MAUBFC-rPPG, Samples: 42, Modality: RGB videos<br>
‚Ä¢ Dataset: MAPURE, Samples: 59, Modality: RGB videos<br>
‚Ä¢ Dataset: MASCAMPS-200, Samples: 200, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Propagate And Calibrate: Real-time Passive Non-line-of-sight Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.11791"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/againstentropy/NLOS-Track/"><img src="https://img.shields.io/github/stars/NLOS-Track/.svg?style=social&label=Star"></a><br><a href="https://againstentropy.github.io/NLOS-Track/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory<br>
‚Ä¢ Dataset: NLOS-Track, Samples: 1500, Modality: RGB videos + 2D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Learning Optical Flow from Event Camera with Rendered Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.11011"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: MDR, Samples: 80000, Modality: event streams + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>EarCough: Enabling Continuous Subject Cough Event Detection on Hearables</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.10445"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Technology, Tsinghua University<br>
‚Ä¢ Dataset: Synchronous Audio and Motion Dataset for Subject Cough Detection (name not explicitly given), Samples: None, Modality: 6-axis IMU (3-axis accelerometer, 3-axis gyroscope) and dual-channel audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Highly Efficient 3D Human Pose Tracking from Events with Spiking Spatiotemporal Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.09681"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JimmyZou/HumanPoseTrackingSNN"><img src="https://img.shields.io/github/stars/JimmyZou/HumanPoseTrackingSNN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: SynEventHPD, Samples: 9197, Modality: Synthetic event streams + SMPL annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Classification of Primitive Manufacturing Tasks from Filtered Event Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.09558"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Centre for Mechanical Engineering, Materials and Processes (CEMMPRE), University of Coimbra, 3030-788, Coimbra, Portugal<br>
‚Ä¢ Dataset: Dataset of Manufacturing Tasks (DMT22), Samples: 72, Modality: event camera data, RGB-D videos, electromagnetic pose data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.09095"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.lidarhumanmotion.net/sloper4d/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xiamen University, China<br>
‚Ä¢ Dataset: SLOPER4D, Samples: 15, Modality: LiDAR point clouds, RGB videos, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Evaluating gesture generation in a large-scale open challenge: The GENEA Challenge 2022</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.08737"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/GENEA-workshop/genea_challenge_2022"><img src="https://img.shields.io/github/stars/GENEA-workshop/genea_challenge_2022.svg?style=social&label=Star"></a><br><a href="https://youngwoo-yoon.github.io/GENEAchallenge2022"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SEED ‚Äì Electronic Arts (EA), Sweden<br>
‚Ä¢ Dataset: GENEA Challenge 2022 Dataset, Samples: 18 hours (training set), Modality: 3D full-body MoCap joints (including fingers), speech audio, text transcriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Unsupervised Contour Tracking of Live Cells by Mechanical and Cycle Consistency Losses</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.08364"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JunbongJang/contour-tracking/"><img src="https://img.shields.io/github/stars/contour-tracking/.svg?style=social&label=Star"></a><br><a href="https://junbongjang.github.io/projects/contour-tracking/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST<br>
‚Ä¢ Dataset: Live Cell Sparse Contour Tracking Labels, Samples: 13, Modality: Phase contrast & confocal fluorescence microscopy videos + sparse point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>A large-scale multimodal dataset of human speech recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.08295"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/G-Bob/Multimodal-dataset-for-human-speech-recognition"><img src="https://img.shields.io/github/stars/G-Bob/Multimodal-dataset-for-human-speech-recognition.svg?style=social&label=Star"></a><br><a href="https://nextcloud.gla.ac.uk/s/LJHKyBxLHXdk4xZ"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: James Watt School of Engineering, University of Glasgow, Glasgow, G12 8QQ,UK<br>
‚Ä¢ Dataset: A large-scale multimodal dataset of human speech recognition, Samples: Approx. 400 minutes from 20 participants, performing tasks including speaking 5 vowels, 15 words, and 16 sentences., Modality: UWB radar, mmWave radar, laser speckle patterns, audio, mouth video, mouth skeleton points<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>ForDigitStress: A multi-modal stress dataset employing a digital job interview scenario</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.07742"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hcai.eu/fordigitstress"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Lab for Human-Centered AI, Augsburg University, Augsburg, Germany<br>
‚Ä¢ Dataset: ForDigitStress, Samples: 40, Modality: 3D Skeleton data (Kinect), 2D pose trajectories (OpenPose), facial landmarks, action units, head pose, RGB video, audio, PPG, EDA, eye-tracking video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>BlinkFlow: A Dataset to Push the Limits of Event-based Optical Flow Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.07716"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zju3dv.github.io/blinkflow/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Lab of CAD&CG, Zhejiang University<br>
‚Ä¢ Dataset: BlinkFlow, Samples: 3587, Modality: Event stream + optical flow ground truth + RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>DisCoHead: Audio-and-Video-Driven Talking Head Generation by Disentangled Control of Head Pose and Facial Expressions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.07697"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/deepbrainai-research/koeba"><img src="https://img.shields.io/github/stars/deepbrainai-research/koeba.svg?style=social&label=Star"></a><br><a href="https://deepbrainai-research.github.io/discohead/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DeepBrain AI Inc., Seoul, Korea<br>
‚Ä¢ Dataset: Korean election broadcast addresses dataset (KoEBA), Samples: None, Modality: audio-video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>InsMOS: Instance-Aware Moving Object Segmentation in LiDAR Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.03909"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nubot-nudt/InsMOS"><img src="https://img.shields.io/github/stars/nubot-nudt/InsMOS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China<br>
‚Ä¢ Dataset: SemanticKITTI with 3D Bounding Box Instances, Samples: None, Modality: LiDAR<br>
‚Ä¢ Dataset: KITTI-road with 3D Bounding Box Instances, Samples: None, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Event Voxel Set Transformer for Spatiotemporal Representation Learning on Event Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.03856"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/bochenxie/NeuroHAR"><img src="https://img.shields.io/github/stars/bochenxie/NeuroHAR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering, City University of Hong Kong, Hong Kong SAR, China<br>
‚Ä¢ Dataset: Neuromorphic Human Action Recognition (NeuroHAR), Samples: 1584, Modality: event, RGB, and depth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>EvHandPose: Event-based 3D Hand Pose Estimation with Sparse Supervision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.02862"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Jianping-Jiang/EvHandPose"><img src="https://img.shields.io/github/stars/Jianping-Jiang/EvHandPose.svg?style=social&label=Star"></a><br><a href="https://www.pku-vcl.com/project/EvHandPose/main.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory for Multimedia Information Processing, National Engineering Research Center of Visual Technology, and AI Innovation Center, School of Computer Science, Peking University, Beijing 100871, China<br>
‚Ä¢ Dataset: EvRealHands, Samples: 102, Modality: Event streams, RGB images, 3D hand pose and shape annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.01943"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://spring-benchmark.org"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Visualization and Interactive Systems, University of Stuttgart<br>
‚Ä¢ Dataset: Spring, Samples: 23812, Modality: RGB videos + scene flow + optical flow + stereo disparity<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.01765"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XingqunQi/Diverse-3D-Hand-Gesture-Prediction"><img src="https://img.shields.io/github/stars/XingqunQi/Diverse-3D-Hand-Gesture-Prediction.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AAII, University of Technology Sydney; Netease Fuxi AI Lab<br>
‚Ä¢ Dataset: TED Hands, Samples: 134456, Modality: 3D axis-angle joint representations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.00938"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pku-epic.github.io/UniDexGrasp/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: UniDexGrasp's synthesized dexterous grasp dataset, Samples: 1120000, Modality: Robotic hand kinematics (root rotation, root translation, joint angles) for static grasps on 5519 object instances.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Tracking Fast by Learning Slow: An Event-based Speed Adaptive Hand Tracker Leveraging Knowledge in RGB Domain</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.14430"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ChuanlinLan/ESAHT"><img src="https://img.shields.io/github/stars/ChuanlinLan/ESAHT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: City University of Hong Kong<br>
‚Ä¢ Dataset: Event-based Speed Adaptive Hand Tracker (ESAHT) Dataset, Samples: 44, Modality: event streams + RGB videos + 3D hand pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Learning to Super-Resolve Blurry Images with Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.13766"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ShinyWang33/eSL-Net-Plusplus"><img src="https://img.shields.io/github/stars/ShinyWang33/eSL-Net-Plusplus.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University<br>
‚Ä¢ Dataset: synthetic GoPro dataset, Samples: 270 video sequences, Modality: HR clear images, LR blurry images, Event streams<br>
‚Ä¢ Dataset: RWS (Real-World Scenes), Samples: None, Modality: real-world events, real-world blurry APS frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>FLSea: Underwater Visual-Inertial and Stereo-Vision Forward-Looking Datasets</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.12772"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.kaggle.com/datasets/viseaonlab/flsea-vi"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ViSEAon Marine Imaging Lab, Department of Marine Technologies, University of Haifa, Haifa, Israel<br>
‚Ä¢ Dataset: FLSea, Samples: 17, Modality: underwater stereo videos, monocular videos + IMU data, ground truth depth maps and camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Causal Explanations for Sequential Decision-Making in Multi-Agent Systems</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.10809"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/uoe-agents/cema"><img src="https://img.shields.io/github/stars/uoe-agents/cema.svg?style=social&label=Star"></a><br><a href="https://datashare.ed.ac.uk/handle/10283/8714"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Edinburgh<br>
‚Ä¢ Dataset: HEADD, Samples: 1308, Modality: RGB videos + text explanations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Stable Motion Primitives via Imitation and Contrastive Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.10017"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rperezdattari/Stable-Motion-Primitives-via-Imitation-and-Contrastive-Learning"><img src="https://img.shields.io/github/stars/rperezdattari/Stable-Motion-Primitives-via-Imitation-and-Contrastive-Learning.svg?style=social&label=Star"></a><br><a href="https://youtu.be/OM-2edHBRfc"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Delft University of Technology<br>
‚Ä¢ Dataset: LAIR handwriting dataset, Samples: 10, Modality: mouse interface (2D position and velocity)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Anticipating Next Active Objects for Egocentric Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.06358"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sanket-thakur/ANACTO"><img src="https://img.shields.io/github/stars/sanket-thakur/ANACTO.svg?style=social&label=Star"></a><br><a href="https://sanket-thakur.github.io/ANACTO/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Pattern Analysis and Computer Vision (PA VIS), Istituto Italiano di Tecnologia (IIT), Department of Electrical, Electronics and Telecommunication Engineering and Naval Architecture (DITEN), University of Genoa, Italy<br>
‚Ä¢ Dataset: EpicKitchens-100 ANACTO Annotations, Samples: None, Modality: RGB videos + bounding box annotations<br>
‚Ä¢ Dataset: EGTEA+ ANACTO Annotations, Samples: None, Modality: RGB videos + bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>A Neuromorphic Dataset for Object Segmentation in Indoor Cluttered Environment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.06301"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yellow07200/ESD_labeling_tool"><img src="https://img.shields.io/github/stars/yellow07200/ESD_labeling_tool.svg?style=social&label=Star"></a><br><a href="https://figshare.com/s/7cf0e84fe8e7b9f7ae42"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Advanced Research and Innovation Center (ARIC), Khalifa University, Abu Dhabi, UAE<br>
‚Ä¢ Dataset: Event-based Segmentation Dataset (ESD), Samples: 145, Modality: Event streams, RGBD frames, Robot end-effector pose and velocity<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.05991"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/augcog/DTTDv1"><img src="https://img.shields.io/github/stars/augcog/DTTDv1.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Berkeley<br>
‚Ä¢ Dataset: Digital Twin Tracking Dataset (DTTD), Samples: 103, Modality: RGB-D videos + MoCap camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.05309"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ilaydayaman/LuViRA Dataset"><img src="https://img.shields.io/github/stars/ilaydayaman/LuViRA Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Lund University, Department of Electrical and Information Technology<br>
‚Ä¢ Dataset: LuViRA, Samples: 89, Modality: 6DOF pose ground truth, IMU, RGB-D images, 5G radio channel estimates, audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>MMPD: Multi-Domain Mobile Video Physiology Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.03840"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/THU-CS-PI/MMPD_rPPG_dataset"><img src="https://img.shields.io/github/stars/THU-CS-PI/MMPD_rPPG_dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: MMPD, Samples: 660, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Motion ID: Human Authentication Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.01751"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SamsungLabs/MotionID"><img src="https://img.shields.io/github/stars/SamsungLabs/MotionID.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Samsung R&D Institute Rus<br>
‚Ä¢ Dataset: MotionID: IMU all motions, Samples: None, Modality: IMU (accelerometer, magnetometer, gyroscope, rotation sensor)<br>
‚Ä¢ Dataset: MotionID: IMU specific motion, Samples: 30300, Modality: IMU (accelerometer, magnetometer, gyroscope, rotation sensor)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>GyroFlow+: Gyroscope-Guided Unsupervised Deep Homography and Optical Flow Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.10018"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information and Communication Engineering, University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: GHOF, Samples: 10000+ pairs, Modality: RGB videos + gyroscope readings + optical flow + homography<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Learning to View: Decision Transformers for Active Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.09544"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Amazon Lab126, Sunnyvale, CA 94098, USA; Carnegie Mellon University, Pittsburgh, PA 15213, USA<br>
‚Ä¢ Dataset: Unnamed Interactive Dataset (from AI2THOR), Samples: approx. 1000 initial poses in each of the 4 scene categories, Modality: Robot pose trajectories with RGB and depth images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.09489"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Sapienza University of Rome, Italy<br>
‚Ä¢ Dataset: HR-UBnormal, Samples: 234751, Modality: kinematic skeletons<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Dance2MIDI: Dance-driven multi-instruments music generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.09080"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZJUKG/Dance2MIDI"><img src="https://img.shields.io/github/stars/ZJUKG/Dance2MIDI.svg?style=social&label=Star"></a><br><a href="https://dance2midi.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology, Zhejiang University<br>
‚Ä¢ Dataset: D2MIDI, Samples: 71754, Modality: RGB videos + 3D keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.07652"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southeast University, China<br>
‚Ä¢ Dataset: HMDO (Hand Manipulation with Deformable Objects), Samples: 12, Modality: multi-view synchronized images, 3D meshes, contact deformation maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.06648"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="bit.ly/yelan-research"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California San Diego, USA<br>
‚Ä¢ Dataset: YeLan (Synthetic), Samples: 3958169, Modality: Event Camera + 3D joint coordinates<br>
‚Ä¢ Dataset: YeLan (Real-world), Samples: 446158, Modality: Event Camera + Motion Capture<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.05575"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Microelectromechanical Systems (CMEMS), University of Minho, Guimar√£es, Portugal<br>
‚Ä¢ Dataset: Custom Smart Walker Gait Dataset (not explicitly named), Samples: 360, Modality: RGB-D videos, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.05191"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AHupuJR/REFID"><img src="https://img.shields.io/github/stars/AHupuJR/REFID.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Research Center for Optical Instrumentation, Zhejiang University, 310027 Hangzhou, China, the Robotics and Perception Group, University of Zurich, 8050 Zurich, Switzerland<br>
‚Ä¢ Dataset: HighREV, Samples: 30, Modality: RGB videos + events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.03213"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/EGO4D/episodic-memory/tree/main/EgoTracks"><img src="https://img.shields.io/github/stars/main/EgoTracks.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta AI<br>
‚Ä¢ Dataset: EgoTracks, Samples: 22028, Modality: RGB videos + bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Augmenting Ego-Vehicle for Traffic Near-Miss and Accident Classification Dataset using Manipulating Conditional Style Translation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.02726"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jampang41/CST-S3D"><img src="https://img.shields.io/github/stars/jampang41/CST-S3D.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Big Data Integration Research Center, NICT, Tokyo, Japan<br>
‚Ä¢ Dataset: re-annotation DADA-2000 dataset, Samples: None, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.00493"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/argoverse/av2-api"><img src="https://img.shields.io/github/stars/argoverse/av2-api.svg?style=social&label=Star"></a><br><a href="https://www.argoverse.org/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Georgia Tech<br>
‚Ä¢ Dataset: Argoverse 2 Sensor Dataset, Samples: 1000, Modality: LiDAR point clouds, RGB cameras, stereo cameras, 6-DOF pose<br>
‚Ä¢ Dataset: Argoverse 2 Lidar Dataset, Samples: 20000, Modality: LiDAR point clouds, 6-DOF pose<br>
‚Ä¢ Dataset: Argoverse 2 Motion Forecasting Dataset, Samples: 250000, Modality: Object trajectories (2D position, velocity, heading), HD vector maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.00411"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Luciferbobo/D4NeRF"><img src="https://img.shields.io/github/stars/Luciferbobo/D4NeRF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: PhiGent Robotics<br>
‚Ä¢ Dataset: urban driving scenes, Samples: None, Modality: monocular RGB videos, depth, optical flow, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>X-MAS: Extremely Large-Scale Multi-Modal Sensor Dataset for Outdoor Surveillance in Real Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.14574"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lge-robot-navi"><img src="https://img.shields.io/github/stars/github.com/lge-robot-navi.svg?style=social&label=Star"></a><br><a href="http://gofile.me/6GfMG/eYjbJSjvF"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical Engineering at Korea Advanced Institute of Science and Technology (KAIST); Advanced Robotics Lab. at LG Electronics<br>
‚Ä¢ Dataset: X-MAS, Samples: 2624, Modality: RGB, thermal, IR, night vision, depth, LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>NeMo: 3D Neural Motion Fields from Multiple Video Instances of the Same Action</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.13660"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/nemo-neural-motion-field"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: Our MoCap Dataset, Samples: 40, Modality: MoCap + RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>ESVIO: Event-based Stereo Visual Inertial Odometry</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.13184"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arclab-hku/ESVIO"><img src="https://img.shields.io/github/stars/arclab-hku/ESVIO.svg?style=social&label=Star"></a><br><a href="https://github.com/arclab-hku/Event based VO-VIO-SLAM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: HKU Dataset, Samples: 9, Modality: Stereo event streams, stereo images, IMU, VICON ground truth poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Full-Body Articulated Human-Object Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.10621"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jnnan.github.io/project/chairs/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Intelligence Science and Technology, Peking University; Beijing Institute of General Artificial Intelligence (BIGAI)<br>
‚Ä¢ Dataset: CHAIRS, Samples: 1390, Modality: MoCap, multi-view RGB-D sequences, 3D meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.08914"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JeffWang987/ASAP"><img src="https://img.shields.io/github/stars/JeffWang987/ASAP.svg?style=social&label=Star"></a><br><a href="https://github.com/JeffWang987/ASAP"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CASIA<br>
‚Ä¢ Dataset: nuScenes-H, Samples: 1,200,000 annotated images (1M training, 0.2M validation), Modality: Surround-view RGB images + 3D bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Automatic vehicle trajectory data reconstruction at scale</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.07907"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="not available"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Civil and Environmental Engineering, Vanderbilt University, United States; Institute for Software Integrated Systems, Vanderbilt University, United States<br>
‚Ä¢ Dataset: Trajectory Reconciliation Benchmark Datasets, Samples: 5, Modality: vehicle trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.07626"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: HODome, Samples: 274, Modality: multi-view RGB videos + MoCap data (markers, SMPL-X parameters, keypoints)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Accidental Turntables: Learning 3D Pose by Watching Objects Turn</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.06300"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://people.cs.umass.edu/~zezhoucheng/acci-turn/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Massachusetts, Amherst<br>
‚Ä¢ Dataset: Accidental Turntables Dataset, Samples: 313, Modality: RGB videos + 3D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Ego-Body Pose Estimation via Ego-Head Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.04636"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: AMASS-Replica-Ego-Syn (ARES), Samples: 1664616, Modality: RGB videos + 3D human motions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>MIME: Human-Aware 3D Scene Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.04360"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mime.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T ¬®ubingen, Germany<br>
‚Ä¢ Dataset: 3D FRONT HUMAN, Samples: None, Modality: 3D scenes populated with SMPL-X human models<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.03741"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua Shenzhen International Graduate School, Tsinghua University<br>
‚Ä¢ Dataset: FineDance, Samples: 346, Modality: MoCap joints (52 joints), SMPL parameters, fbx files, multi-view videos, music<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Privacy-Preserving Visual Localization with Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.03177"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/82magnolia/event_localization"><img src="https://img.shields.io/github/stars/82magnolia/event_localization.svg?style=social&label=Star"></a><br><a href="https://82magnolia.github.io/event_localization/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Seoul National University<br>
‚Ä¢ Dataset: EvRooms, Samples: 23345, Modality: Event streams + 6DoF poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Muscles in Action</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.02978"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Columbia University<br>
‚Ä¢ Dataset: Muscles in Action (MIA), Samples: 15000, Modality: synchronized RGB video and surface electromyography (sEMG) signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Minimum Latency Deep Online Video Stabilization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.02073"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/liuzhen03/NNDVS"><img src="https://img.shields.io/github/stars/liuzhen03/NNDVS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: MotionStab, Samples: 65238, Modality: paired unstable/stable camera motion (meshflow)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>Reconstructing Hand-Held Objects from Monocular Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.16835"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dihuangdh.github.io/hhor"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Sydney<br>
‚Ä¢ Dataset: Hand-held Object Dataset (HOD), Samples: 35, Modality: Monocular 4K RGB videos + 3D ground truth meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>Comparison of Motion Encoding Frameworks on Human Manipulation Actions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.13024"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.7351664"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Third Institute of Physics, Dept. Computational Neuroscience, University of G ¬®ottingen, 37073 G ¬®ottingen, Germany<br>
‚Ä¢ Dataset: Human Manipulation Actions Dataset, Samples: 7652, Modality: 3D hand trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>Tensor4D : Efficient Neural 4D Decomposition for High-fidelity Dynamic Reconstruction and Rendering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.11610"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DSaurus/Tensor4D"><img src="https://img.shields.io/github/stars/DSaurus/Tensor4D.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Automation, Tsinghua University<br>
‚Ä¢ Dataset: Tensor4D multiview human motion dataset, Samples: 9, Modality: multi-view synchronized RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>Blur Interpolation Transformer for Real-World Motion from Blur</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.11423"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zzh-tech/BiT"><img src="https://img.shields.io/github/stars/zzh-tech/BiT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo, Japan; National Institute of Informatics, Japan<br>
‚Ä¢ Dataset: RBI, Samples: 55, Modality: low-frame-rate blurred videos and high-frame-rate sharp videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>H-VFI: Hierarchical Frame Interpolation for Videos with Large Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.11309"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kuaishou Technology<br>
‚Ä¢ Dataset: YouTube200K, Samples: 200000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>Leveraging Multi-stream Information Fusion for Trajectory Prediction in Low-illumination Scenarios: A Multi-channel Graph Convolutional Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.10226"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/TommyGong08/MSIF"><img src="https://img.shields.io/github/stars/TommyGong08/MSIF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Mechanical Engineering, Beijing Institute of Technology<br>
‚Ä¢ Dataset: Dark-HEV-I, Samples: 230, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>3d human motion generation from the text via gesture action classification and the autoregressive model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.10003"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/GT-KIM/motion_generation_from_text"><img src="https://img.shields.io/github/stars/GT-KIM/motion_generation_from_text.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical Engineering, Korea University, Seoul, South Korea<br>
‚Ä¢ Dataset: action-based gesture dataset, Samples: 1200, Modality: MoCap joints<br>
‚Ä¢ Dataset: Gesture Action Classification dataset, Samples: 1309, Modality: text sentences with labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>HARDVS: Revisiting Human Activity Recognition with Dynamic Vision Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.09648"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/HARDVS"><img src="https://img.shields.io/github/stars/Event-AHU/HARDVS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: HARDVS, Samples: 107646, Modality: DVS event streams + RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>Detecting Line Segments in Motion-blurred Images with Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.07365"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/levenberg/FE-LSD"><img src="https://img.shields.io/github/stars/levenberg/FE-LSD.svg?style=social&label=Star"></a><br><a href="https://levenberg.github.io/FE-LSD"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University<br>
‚Ä¢ Dataset: FE-Wireframe, Samples: 5462, Modality: motion-blurred RGB images + event data + line annotations<br>
‚Ä¢ Dataset: FE-Blurframe, Samples: 800, Modality: motion-blurred RGB images + event streams + line annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.05709"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lisiyao21/AnimeRun"><img src="https://img.shields.io/github/stars/lisiyao21/AnimeRun.svg?style=social&label=Star"></a><br><a href="https://lisiyao21.github.io/projects/AnimeRun"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: AnimeRun, Samples: 30, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>CRIPP-VQA: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.03779"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Maitreyapatel/CRIPP-VQA/"><img src="https://img.shields.io/github/stars/CRIPP-VQA/.svg?style=social&label=Star"></a><br><a href="https://maitreyapatel.com/CRIPP-VQA/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Arizona State University<br>
‚Ä¢ Dataset: CRIPP-VQA, Samples: 7000, Modality: Synthetic RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>TAP-Vid: A Benchmark for Tracking Any Point in a Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.03726"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/deepmind/tapnet"><img src="https://img.shields.io/github/stars/deepmind/tapnet.svg?style=social&label=Star"></a><br><a href="https://github.com/deepmind/tapnet"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DeepMind<br>
‚Ä¢ Dataset: TAP-Vid-Kinetics, Samples: 1189, Modality: RGB videos + point tracks<br>
‚Ä¢ Dataset: TAP-Vid-DAVIS, Samples: 30, Modality: RGB videos + point tracks<br>
‚Ä¢ Dataset: TAP-Vid-Kubric, Samples: 799, Modality: Synthetic RGB videos + ground-truth point tracks<br>
‚Ä¢ Dataset: TAP-Vid-RGB-Stacking, Samples: 50, Modality: Synthetic RGB videos + ground-truth point tracks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>UmeTrack: Unified multi-view end-to-end hand tracking for VR</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.00099"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta Reality Labs, USA<br>
‚Ä¢ Dataset: Large-scale egocentric hand tracking dataset, Samples: 1397 real sequences and 1397 synthetic sequences, Modality: Multi-view monochrome video from headset-mounted cameras, 3D hand poses (from motion capture)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>InGVIO: A Consistent Invariant Filter for Fast and High-Accuracy GNSS-Visual-Inertial Odometry</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.15145"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ChangwuLiu/InGVIO"><img src="https://img.shields.io/github/stars/ChangwuLiu/InGVIO.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Aerospace Engineering, Tsinghua University<br>
‚Ä¢ Dataset: fixed-wing datasets (fwgvieasy, fwgvimedium, fwgvihard), Samples: 3, Modality: Stereo images + IMU + raw GNSS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>Monocular Dynamic View Synthesis: A Reality Check</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.13445"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hangg7/dycheck"><img src="https://img.shields.io/github/stars/hangg7/dycheck.svg?style=social&label=Star"></a><br><a href="https://hangg7.com/dycheck"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: iPhone dataset, Samples: 14, Modality: RGB videos + Lidar depth + annotated keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>BlanketGen - A synthetic blanket occlusion augmentation pipeline for MoCap datasets</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.12035"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.inesctec.pt/brain-lab/brain-lab-public/blanket-gen-releases"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Biomedical Engineering Research, INESC TEC, Porto, Portugal<br>
‚Ä¢ Dataset: BlanketGen-3DPW, Samples: 1037, Modality: RGB videos + 3D pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>JRDB-Pose: A Large-scale Dataset for Multi-Person Pose Estimation and Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.11940"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jrdb.erc.monash.edu/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: JRDB-Pose, Samples: 5022 pose tracks, Modality: RGB videos, LiDAR point clouds, human pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>A Clinical Dataset for the Evaluation of Motion Planners in Medical Applications</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.10834"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UNC-Robotics/Med-MPD"><img src="https://img.shields.io/github/stars/UNC-Robotics/Med-MPD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of North Carolina at Chapel Hill<br>
‚Ä¢ Dataset: Medical Motion Planning Dataset (Med-MPD), Samples: 15, Modality: 3D binary maps (from medical images) + start/target poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.09729"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://silverster98.github.io/HUMANISE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science & Technology, Beijing Institute of Technology<br>
‚Ä¢ Dataset: HUMANISE, Samples: 19600, Modality: SMPL-X motion sequences in 3D scene point clouds with language descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>Neural Contact Fields: Tracking Extrinsic Contact with Tactile Sensing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.09297"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/carolinahiguera/NCF"><img src="https://img.shields.io/github/stars/carolinahiguera/NCF.svg?style=social&label=Star"></a><br><a href="https://github.com/carolinahiguera/NCF"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Washington<br>
‚Ä¢ Dataset: YCB-Extrinsic-Contact, Samples: 4500, Modality: Tactile images (DIGIT), end-effector poses, object point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>INSANE: Cross-Domain UAV Data Sets with Increased Number of Sensors for developing Advanced and Novel Estimators</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.09114"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sst.aau.at/cns/datasets"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Control of Networked Systems Group of the University of Klagenfurt, Austria<br>
‚Ä¢ Dataset: INSANE, Samples: 27, Modality: 6-DoF trajectories, multiple IMUs, mono and stereo cameras, RTK GNSS, UWB, Laser Range Finder, Magnetometer, Motion Capture<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>Self-Improving SLAM in Dynamic Environments: Learning When to Mask</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.08350"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universit√© Paris-Saclay, CEA, List F-91120, Palaiseau, France<br>
‚Ä¢ Dataset: ConsInv, Samples: None, Modality: Monocular and Stereo RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>Weakly-Supervised Optical Flow Estimation for Time-of-Flight</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.05298"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/schellmi42/WFlowToF"><img src="https://img.shields.io/github/stars/schellmi42/WFlowToF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ulm University<br>
‚Ä¢ Dataset: CB-dataset extension, Samples: 14, Modality: Simulated raw iToF measurements<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>Robustness Certification of Visual Perception Models via Camera Motion Smoothing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.04625"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HanjiangHu/camera-motion-smoothing"><img src="https://img.shields.io/github/stars/HanjiangHu/camera-motion-smoothing.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: MetaRoom, Samples: 500 training camera poses and 120 testing camera poses per object (20 objects total), Modality: RGB images, dense point clouds, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>OGC: Unsupervised 3D Object Segmentation from Rigid Dynamics of Point Clouds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.04458"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vLAR-group/OGC"><img src="https://img.shields.io/github/stars/vLAR-group/OGC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: vLAR Group, The Hong Kong Polytechnic University<br>
‚Ä¢ Dataset: OGC-DR, Samples: 5000, Modality: Point Cloud<br>
‚Ä¢ Dataset: OGC-DRSV, Samples: 5000, Modality: Point Cloud<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>BlanketSet -- A clinical real-world in-bed action recognition and qualitative semi-synchronised MoCap dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.03600"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://rdm.inesctec.pt/dataset/nis-2022-004"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Biomedical Engineering Research, INESC TEC, Porto, Portugal; Faculty of Engineering (FEUP), University of Porto, Porto, Portugal<br>
‚Ä¢ Dataset: BlanketSet, Samples: 405, Modality: RGB-IR-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>GLAD: Grounded Layered Autonomous Driving for Complex Service Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.02302"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, SUNY Binghamton<br>
‚Ä¢ Dataset: IMDataset, Samples: 13800, Modality: Sequences of four RGB images from four simulated cameras (front, back, left, right) with safety labels for lane merging behaviors.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>A Benchmark for Multi-Modal Lidar SLAM with Ground Truth in GNSS-Denied Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.00812"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/TIERS/tiers-lidars-dataset-enhanced"><img src="https://img.shields.io/github/stars/TIERS/tiers-lidars-dataset-enhanced.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Turku Intelligent Embedded and Robotic Systems (TIERS) Lab, University of Turku, Finland.<br>
‚Ä¢ Dataset: TIERS Lidar Dataset Enhanced, Samples: 17, Modality: Multi-modal LiDAR, IMU, cameras, with 6-DOF pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>WorldGen: A Large Scale Generative Simulator</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.00715"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://prg.cs.umd.edu/WorldGen"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Perception and Robotics Group, University of Maryland Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, USA<br>
‚Ä¢ Dataset: WorldGen, Samples: None, Modality: RGB videos + optical flow, depth, surface normals, semantic segmentation, event data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Motion and Appearance Adaptation for Cross-Domain Motion Transfer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.14529"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: Mixamo-Video Dataset, Samples: 690, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>NEURAL MARIONETTE: A Transformer-based Multi-action Human Motion Synthesis System</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.13204"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wjohnnyw/NeuralMarionette"><img src="https://img.shields.io/github/stars/wjohnnyw/NeuralMarionette.svg?style=social&label=Star"></a><br><a href="https://wjohnnyw.github.io/blog/tag2motion/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Information Technology, Monash University, Melbourne 3168, Australia<br>
‚Ä¢ Dataset: BABEL-MAG (BABEL for Multi-Action Generation), Samples: 7643, Modality: MoCap joint rotations and root translation<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Assessing the Role of Datasets in the Generalization of Motion Deblurring Methods to Real Images</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.12675"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/GuillermoCarbajal/SBDD"><img src="https://img.shields.io/github/stars/GuillermoCarbajal/SBDD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IIE, Facultad de Ingenier¬¥ ƒ±a, Universidad de la Rep¬¥ ublica, Herrera y Reissig 565, Montevideo, 11500, Uruguay.<br>
‚Ä¢ Dataset: SBDD (Segmentation-Based Deblurring Dataset), Samples: None, Modality: RGB images (sharp/blurred pairs)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Real-RawVSR: Real-World Raw Video Super-Resolution with a Benchmark Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.12475"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zmzhang1998/Real-RawVSR"><img src="https://img.shields.io/github/stars/zmzhang1998/Real-RawVSR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical and Information Engineering, Tianjin University, Tianjin, China<br>
‚Ä¢ Dataset: Real-RawVSR, Samples: 450, Modality: raw videos + sRGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.12354"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://intercap.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T ¬®ubingen, Germany<br>
‚Ä¢ Dataset: InterCap, Samples: 223, Modality: multi-view RGB-D videos + 3D meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>PL-EVIO: Robust Monocular Event-based Visual Inertial Odometry with Point and Line Features</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.12160"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arclab-hku/Event_based_VO-VIO-SLAM_dataset"><img src="https://img.shields.io/github/stars/arclab-hku/Event_based_VO-VIO-SLAM_dataset.svg?style=social&label=Star"></a><br><a href="https://youtu.be/KnWZ4anBMK4"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Adaptive Robotic Controls Lab (ArcLab), Department of Mechanical Engineering, Faculty of Engineering, The University of Hong Kong, Hong Kong SAR, China<br>
‚Ä¢ Dataset: Event based VO-VIO-SLAM dataset, Samples: 11, Modality: event camera data, standard camera images, IMU measurements, VICON ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.12009"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CFCS, Peking University; Beijing Institute for General AI<br>
‚Ä¢ Dataset: SimGrasp, Samples: 1810, Modality: depth point cloud sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>T3VIP: Transformation-based 3D Video Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.11693"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://t3vip.cs.uni-freiburg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Freiburg<br>
‚Ä¢ Dataset: DexHand, Samples: 10000, Modality: RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Learning Interpretable Dynamics from Images of a Freely Rotating 3D Rigid Body</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.11355"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jjmason687/LearningSO3fromImages"><img src="https://img.shields.io/github/stars/jjmason687/LearningSO3fromImages.svg?style=social&label=Star"></a><br><a href="https://www.dropbox.com/sh/menv3lu9mquu1wh/AABovQ53udtryDC24xPLGw17a?dl=0"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical and Aerospace Engineering, Princeton University<br>
‚Ä¢ Dataset: Uniform density cube, Samples: 1000, Modality: Rendered RGB videos<br>
‚Ä¢ Dataset: Uniform density prism, Samples: 1000, Modality: Rendered RGB videos<br>
‚Ä¢ Dataset: Non-uniform density cube, Samples: 1000, Modality: Rendered RGB videos<br>
‚Ä¢ Dataset: Non-uniform density prism, Samples: 1000, Modality: Rendered RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>T2FPV: Dataset and Method for Correcting First-Person View Errors in Pedestrian Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.11294"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cmubig/T2FPV"><img src="https://img.shields.io/github/stars/cmubig/T2FPV.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Science Dept., Carnegie Mellon University<br>
‚Ä¢ Dataset: T2FPV-ETH, Samples: 49115, Modality: Synthetic first-person view RGB videos, instance segmentation masks, 2D ground-plane trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Visual Localization and Mapping in Dynamic and Changing Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.10710"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering, University of Illinois at Urbana-Champaign<br>
‚Ä¢ Dataset: PUC-USP dataset, Samples: 6, Modality: RGB-D videos + MoCap trajectory<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>FT-HID: A Large Scale RGB-D Dataset for First and Third Person Human Interaction Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.10155"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/guozih/FT-HID-Dataset"><img src="https://img.shields.io/github/stars/guozih/FT-HID-Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical and Automation Engineering, Tianjin University, Tianjin, China<br>
‚Ä¢ Dataset: FT-HID dataset, Samples: 38364, Modality: RGB videos, depth maps, 3D skeleton sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Decentralized Vehicle Coordination: The Berkeley DeepDrive Drone Dataset and Consensus-Based Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.08763"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/b3d-project/b3d"><img src="https://img.shields.io/github/stars/b3d-project/b3d.svg?style=social&label=Star"></a><br><a href="https://github.com/b3d-project/b3d"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering at Cornell University, Department of Electrical Engineering and Computer Sciences at the University of California, Berkeley<br>
‚Ä¢ Dataset: Berkeley DeepDrive Drone (B3D) dataset, Samples: 20, Modality: aerial drone videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Imitrob: Imitation Learning Dataset for Training and Evaluating 6D Object Pose Estimators</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.07976"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/imitrob/imitrob_dataset_code"><img src="https://img.shields.io/github/stars/imitrob/imitrob_dataset_code.svg?style=social&label=Star"></a><br><a href="http://imitrob.ciirc.cvut.cz/imitrobdataset.php"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague, Czech Republic<br>
‚Ä¢ Dataset: Imitrob, Samples: 352, Modality: RGB-D videos + 6D object poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.07556"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS"><img src="https://img.shields.io/github/stars/ubisoft/ubisoft-laforge-ZeroEGGS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ubisoft, Canada; York University, Canada<br>
‚Ä¢ Dataset: unnamed, Samples: 67, Modality: MoCap joints, Audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>A Temporal Densely Connected Recurrent Network for Event-based Human Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.07034"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xavier-zw/tDenseRNN pose"><img src="https://img.shields.io/github/stars/xavier-zw/tDenseRNN pose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Information Science and Engineering, Hunan Normal University, 36 Lushan Road, Changsha, China<br>
‚Ä¢ Dataset: CDEHP, Samples: 500, Modality: event streams, RGB color frames, depth frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>TEAM: a parameter-free algorithm to teach collaborative robots motions from user demonstrations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.06940"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SchindlerReGIS/team"><img src="https://img.shields.io/github/stars/SchindlerReGIS/team.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ¬¥Ecole polytechnique f¬¥ ed¬¥ erale de Lausanne (EPFL), Lausanne, Switzerland<br>
‚Ä¢ Dataset: Door maintenance dataset, Samples: 21, Modality: robot joint angles<br>
‚Ä¢ Dataset: Factory worker dataset, Samples: , Modality: robot joint angles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>COMPASS: A Formal Framework and Aggregate Dataset for Generalized Surgical Procedure Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.06424"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UVA-DSA/COMPASS"><img src="https://img.shields.io/github/stars/UVA-DSA/COMPASS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, 22903, VA, USA<br>
‚Ä¢ Dataset: COMPASS, Samples: 286, Modality: Robotic kinematics + Videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Picking Up Speed: Continuous-Time Lidar-Only Odometry using Doppler Velocity Measurements</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.03304"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/utiasASRL/steam_icp"><img src="https://img.shields.io/github/stars/utiasASRL/steam_icp.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto Institute for Aerospace Studies (UTIAS), University of Toronto<br>
‚Ä¢ Dataset: Aeva dataset, Samples: 8, Modality: FMCW LiDAR with Doppler velocity<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>SIND: A Drone Dataset at Signalized Intersection in China</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.02297"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SOTIF-AVLab/SinD"><img src="https://img.shields.io/github/stars/SOTIF-AVLab/SinD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China<br>
‚Ä¢ Dataset: SIND, Samples: 13248, Modality: Drone-captured videos, trajectory data (position, velocity, acceleration), HD map, traffic light states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Visual Odometry with Neuromorphic Resonator Networks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.02000"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Neuroinformatics, University of Zurich and ETH Zurich, Switzerland<br>
‚Ä¢ Dataset: Robotics Arm Dataset, Samples: None, Modality: Event camera data + robot arm trajectory<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>A Benchmark for Unsupervised Anomaly Detection in Multi-Agent Trajectories</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.01838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/againerju/r_u_maad"><img src="https://img.shields.io/github/stars/againerju/r_u_maad.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Mercedes-Benz Group AG, Institute of Measurement, Control and Microtechnology, University Ulm<br>
‚Ä¢ Dataset: R-U-MAAD, Samples: 160, Modality: 2D multi-agent trajectories, HD-maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>The Magni Human Motion Dataset: Accurate, Complex, Multi-Modal, Natural, Semantically-Rich and Contextualized</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.14925"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ¬®Orebro University, Sweden<br>
‚Ä¢ Dataset: Magni, Samples: None, Modality: Motion capture, eye-gaze, LiDAR, RGB videos, RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Stabilize, Decompose, and Denoise: Self-Supervised Fluoroscopy Denoising</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.14022"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Z2Sky Technologies Inc.<br>
‚Ä¢ Dataset: Fluoroscopy dataset, Samples: 27, Modality: Fluoroscopy videos<br>
‚Ä¢ Dataset: Clinical dataset, Samples: 60, Modality: Low-dose X-ray videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Robust and Efficient Depth-based Obstacle Avoidance for Autonomous Miniaturized UAVs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.12624"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ETH-PBL/Matrix-ToF-Drones"><img src="https://img.shields.io/github/stars/ETH-PBL/Matrix-ToF-Drones.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Zurich<br>
‚Ä¢ Dataset: Robust and Efficient Depth-based Obstacle Avoidance for Autonomous Miniaturized UAVs Dataset, Samples: 43, Modality: Grayscale video, 8x8 ToF depth matrix, UAV internal state (attitude, velocity, position), MoCap (attitude, position)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Unsupervised Spike Depth Estimation via Cross-modality Cross-domain Knowledge Transfer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.12527"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Theia-4869/BiCross"><img src="https://img.shields.io/github/stars/Theia-4869/BiCross.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory for Multimedia Information Processing, School of CS, Peking University<br>
‚Ä¢ Dataset: Virtual KITTI spike, Samples: None, Modality: spike streams<br>
‚Ä¢ Dataset: KITTI spike, Samples: None, Modality: spike streams<br>
‚Ä¢ Dataset: Driving Stereo spike, Samples: None, Modality: spike streams<br>
‚Ä¢ Dataset: NYUv2 spike, Samples: None, Modality: spike streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Event-based Image Deblurring with Dynamic Motion Awareness</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.11398"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huawei Technologies, Zurich Research Center<br>
‚Ä¢ Dataset: RGBlur+E, Samples: 61, Modality: RGB images + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>The GENEA Challenge 2022: A large evaluation of data-driven co-speech gesture generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.10441"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/genea-workshop/genea_challenge_2022"><img src="https://img.shields.io/github/stars/genea-workshop/genea_challenge_2022.svg?style=social&label=Star"></a><br><a href="https://youngwoo-yoon.github.io/GENEAchallenge2022/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETRI<br>
‚Ä¢ Dataset: GENEA Challenge 2022 Dataset, Samples: 18 hours, Modality: MoCap joints (BVH format) + speech<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Temporal View Synthesis of Dynamic Scenes through 3D Object Motion Estimation with Multi-Plane Images</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.09463"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nagabhushansn95.github.io/publications/2022/DeCOMPnet.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Communication Engineering, Indian Institute of Science<br>
‚Ä¢ Dataset: Indian Institute of Science Virtual Environment Exploration Dataset - Dynamic Scenes (IISc VEED-Dynamic), Samples: 800, Modality: RGB videos + depth + camera pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>MoCapDeform: Monocular 3D Human Motion Capture in Deformable Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.08439"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Malefikus/MoCapDeform"><img src="https://img.shields.io/github/stars/Malefikus/MoCapDeform.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Saarland University, SIC<br>
‚Ä¢ Dataset: MoCapDeform (MCD) dataset, Samples: 4 video sequences, Modality: RGB videos + ground-truth 3D human meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>WatchPed: Pedestrian Crossing Intention Prediction Using Embedded Sensors of Smartwatch</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.07441"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tinyurl.com/pedestrian2023"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AÔ¨Åniti, Virginia, United States<br>
‚Ä¢ Dataset: WatchPed Pedestrian Crossing Intention Dataset, Samples: 255, Modality: RGB videos, smartwatch IMU (accelerometer, gyroscope), bounding boxes, pose information, vehicle speed<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.07363"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Robotics and Intelligent Machines, Georgia Institute of Technology<br>
‚Ä¢ Dataset: MoCapAct, Samples: 517800, Modality: Simulated humanoid rollouts (proprioceptive observations + actions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Heart rate estimation in intense exercise videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.02509"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ynapolean/IBIS-CNN"><img src="https://img.shields.io/github/stars/ynapolean/IBIS-CNN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Delft<br>
‚Ä¢ Dataset: IntensePhysio, Samples: 15, Modality: RGB videos + heart rate<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Learning Modal-Invariant and Temporal-Memory for Video-based Visible-Infrared Person Re-Identification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.02450"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VCM-project233/MITML"><img src="https://img.shields.io/github/stars/VCM-project233/MITML.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Harbin Institute of Technology, Shenzhen<br>
‚Ä¢ Dataset: HITSZ-VCM, Samples: 21863, Modality: RGB videos + IR videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>AutoLaparo: A New Dataset of Integrated Multi-tasks for Image-guided Surgical Automation in Laparoscopic Hysterectomy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.02049"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WANG-Ziyi/AutoLaparo"><img src="https://img.shields.io/github/stars/WANG-Ziyi/AutoLaparo.svg?style=social&label=Star"></a><br><a href="https://autolaparo.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong; T Stone Robotics Institute, The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: AutoLaparo, Samples: 300, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Mates2Motion: Learning How Mechanical CAD Assemblies Work</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.01779"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Washington<br>
‚Ä¢ Dataset: Mates2Motion Processed Dataset, Samples: 7328, Modality: CAD B-Reps with mate annotations<br>
‚Ä¢ Dataset: Mates2Motion User-Annotated Validation Set, Samples: 100, Modality: CAD B-Reps with expert-annotated mates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.01633"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/UnrealEgo/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, SIC, Keio University<br>
‚Ä¢ Dataset: UnrealEgo, Samples: 45520, Modality: Synthetic stereo fisheye images, depth maps, and 3D joint positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Interaction Mix and Match: Synthesizing Close Interaction using Conditional Hierarchical GAN with Multi-Hot Class Embedding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.00774"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Aman-Goel1/IMM"><img src="https://img.shields.io/github/stars/Aman-Goel1/IMM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: International Institute of Information Technology Hyderabad, India<br>
‚Ä¢ Dataset: New synthetic 2-character close interactions dataset, Samples: 300, Modality: 3D skeletal motions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Learning Pseudo Front Depth for 2D Forward-Looking Sonar-based Multi-view Stereo</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.00233"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sollynoay/EPSSN"><img src="https://img.shields.io/github/stars/sollynoay/EPSSN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Precision Engineering, Graduate School of Engineering, The University of Tokyo, Japan<br>
‚Ä¢ Dataset: EPSSN Synthetic Datasets, Samples: 8000, Modality: Acoustic images + relative poses<br>
‚Ä¢ Dataset: EPSSN Real Dataset, Samples: 1493, Modality: Acoustic images + relative poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Efficient Video Deblurring Guided by Motion Magnitude</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.13374"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sollynoay/MMP-RNN"><img src="https://img.shields.io/github/stars/sollynoay/MMP-RNN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo<br>
‚Ä¢ Dataset: MMP dataset, Samples: 22,499 training samples, Modality: RGB videos + motion magnitude maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>CelebV-HQ: A Large-Scale Video Facial Attributes Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.12393"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/CelebV-HQ/CelebV-HQ"><img src="https://img.shields.io/github/stars/CelebV-HQ/CelebV-HQ.svg?style=social&label=Star"></a><br><a href="https://celebv-hq.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SenseTime Research<br>
‚Ä¢ Dataset: CelebV-HQ, Samples: 35666, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Hybrid Classifiers for Spatio-temporal Real-time Abnormal Behaviors Detection, Tracking, and Recognition in Massive Hajj Crowds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.11931"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Jamoum University College, Umm Al-Qura University<br>
‚Ä¢ Dataset: HAJJv2, Samples: 18, Modality: RGB videos + annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Combining Internal and External Constraints for Unrolling Shutter in Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.11725"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="www.wisdom.weizmann.ac.il/~vision/VideoRS"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Computer Science and Applied Math, The Weizmann Institute of Science<br>
‚Ä¢ Dataset: In-the-wild-RS, Samples: 15, Modality: RS/GS video pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>My View is the Best View: Procedure Learning from Egocentric Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.10883"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Sid2697/EgoProceL"><img src="https://img.shields.io/github/stars/Sid2697/EgoProceL.svg?style=social&label=Star"></a><br><a href="https://sid2697.github.io/egoprocel/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Visual Information Technology, IIIT, Hyderabad<br>
‚Ä¢ Dataset: EgoProceL, Samples: 349, Modality: egocentric videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>D2-TPred: Discontinuous Dependency for Trajectory Prediction under Traffic Lights</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.10398"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VTP-TL/D2-TPred"><img src="https://img.shields.io/github/stars/VTP-TL/D2-TPred.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer and Artificial Intelligence, Zhengzhou University, Zhengzhou, China.<br>
‚Ä¢ Dataset: VTP-TL, Samples: 5073, Modality: 2D vehicle trajectories and traffic light states from drone videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Region Aware Video Object Segmentation with Deep Motion Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.10258"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://ieee-dataport.org/9608"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Software Engineering, The University of Western Australia<br>
‚Ä¢ Dataset: OVOS, Samples: 607, Modality: RGB videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Spotting Temporally Precise, Fine-Grained Events in Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.10213"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SoccerNet/sn-spotting"><img src="https://img.shields.io/github/stars/SoccerNet/sn-spotting.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: Tennis, Samples: 3345, Modality: RGB videos<br>
‚Ä¢ Dataset: Figure Skating, Samples: 371, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.10123"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zzh-tech/Animation-from-Blur"><img src="https://img.shields.io/github/stars/zzh-tech/Animation-from-Blur.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo<br>
‚Ä¢ Dataset: GenBlur, Samples: 192, Modality: Synthesized blurry RGB images + sharp RGB video sequences + motion guidance<br>
‚Ä¢ Dataset: B-Aist++, Samples: 105, Modality: Synthesized blurry RGB images + sharp RGB video sequences + motion guidance<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.10120"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dmoltisanti/brace/"><img src="https://img.shields.io/github/stars/brace/.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Edinburgh<br>
‚Ä¢ Dataset: BRACE, Samples: 465, Modality: 2D keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Human keypoint detection for close proximity human-robot interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.07742"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://osf.io/qfkvt/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic<br>
‚Ä¢ Dataset: Human in Close Proximity (HiCP) dataset, Samples: 3232, Modality: RGB images + 2D keypoint annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>A Dual-Masked Auto-Encoder for Robust Motion Capture with Spatial-Temporal Skeletal Token Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.07381"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HKBU-VSComputing/2022_MM_DMAE-Mocap"><img src="https://img.shields.io/github/stars/HKBU-VSComputing/2022_MM_DMAE-Mocap.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China<br>
‚Ä¢ Dataset: BU-Mocap, Samples: 5, Modality: RGB videos, depth maps, IMU sensor data, point cloud data, 3D skeletal trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>SHREC 2022 Track on Online Detection of Heterogeneous Gestures</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.06706"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Verona, Department of Computer Science<br>
‚Ä¢ Dataset: SHREC 2022 Heterogeneous Gestures, Samples: 288, Modality: 3D hand joint positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Is Appearance Free Action Recognition Possible?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.06261"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://f-ilic.github.io/AppearanceFreeActionRecognition"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Graz, Austria<br>
‚Ä¢ Dataset: Appearance Free Dataset (AFD101), Samples: 13320, Modality: Synthetic noise videos animated by optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Entry-Flipped Transformer for Inference and Prediction of Participant Behavior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.06235"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Singtel Cognitive and Artificial Intelligence Lab (SCALE@NTU), Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: Tennis Doubles Dataset, Samples: 4905, Modality: RGB videos with bounding box, action, and trajectory annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Learning to Estimate External Forces of Human Motion in Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.05845"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MichiganCOG/ForcePose"><img src="https://img.shields.io/github/stars/MichiganCOG/ForcePose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Michigan<br>
‚Ä¢ Dataset: ForcePose, Samples: 1344, Modality: RGB videos + MoCap markers + force plate data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Occluded Human Body Capture with Self-Supervised Spatial-Temporal Motion Prior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.05375"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/boycehbz/CHOMP"><img src="https://img.shields.io/github/stars/boycehbz/CHOMP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southeast University, China<br>
‚Ä¢ Dataset: OcMotion, Samples: 43 sequences, Modality: RGB videos + 3D motion annotations (SMPL), 2D poses, camera parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Fine-grained Activities of People Worldwide</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.05182"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/visym/heyvi"><img src="https://img.shields.io/github/stars/visym/heyvi.svg?style=social&label=Star"></a><br><a href="https://visym.github.io/cap"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visym Labs, Cambridge MA, USA<br>
‚Ä¢ Dataset: Consented Activities of People (CAP), Samples: 1450000, Modality: RGB videos with annotations (bounding box tracks, temporal localization)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>CausalAgents: A Robustness Benchmark for Motion Forecasting using Causal Relationships</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.03586"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/google-research/causal-agents"><img src="https://img.shields.io/github/stars/google-research/causal-agents.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google Research, Brain Team<br>
‚Ä¢ Dataset: CausalAgents, Samples: None, Modality: Causal agent labels for agent trajectories from the Waymo Open Motion Dataset (WOMD)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>MoRPI: Mobile Robot Pure Inertial Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.02982"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ansfl/MoRPI"><img src="https://img.shields.io/github/stars/ansfl/MoRPI.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: the Hatter Department of Marine Technologies, University of Haifa, Israel<br>
‚Ä¢ Dataset: MoRPI, Samples: 143, Modality: IMU data (accelerometer, gyroscope) from a mobile robot<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Robustness Analysis of Video-Language Models Against Visual and Language Perturbations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.02159"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Madeline-Schiappa/Robustness-of-Video-Language-Models"><img src="https://img.shields.io/github/stars/Madeline-Schiappa/Robustness-of-Video-Language-Models.svg?style=social&label=Star"></a><br><a href="https://bit.ly/3CNOly4"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Central Florida<br>
‚Ä¢ Dataset: MSRVTT-P, Samples: 90000, Modality: RGB videos + text captions<br>
‚Ä¢ Dataset: YouCook2-P, Samples: 301500, Modality: RGB videos + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>VECtor: A Versatile Event-Centric Benchmark for Multi-Sensor SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.01404"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://star-datasets.github.io/vector/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Mobile Perception Lab of the School of Information Science and Technology, ShanghaiTech University<br>
‚Ä¢ Dataset: VECtor, Samples: 18, Modality: event stereo camera, regular stereo camera, RGB-D, LiDAR, IMU, Motion Capture (MoCap) ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Segmentation Guided Deep HDR Deghosting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.01229"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://val.serc.iisc.ernet.in/HDR/shdr/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computational and Data Sciences, Indian Institute of Science, Bengaluru, KA, 560012, INDIA<br>
‚Ä¢ Dataset: MEDS (Multi-Exposure Dynamic motion Segmentation dataset), Samples: 3683, Modality: Multi-exposure RGB images with human-annotated motion segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>Spatial Transformer Network with Transfer Learning for Small-scale Fine-grained Skeleton-based Tai Chi Action Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.15002"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cloud.hit605.org/s/taichi"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Control Science and Engineering Harbin Institute of Technology, Harbin, China<br>
‚Ä¢ Dataset: Tai Chi dataset, Samples: 200, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.13673"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Tobias-Fischer/sparse-event-vpr"><img src="https://img.shields.io/github/stars/Tobias-Fischer/sparse-event-vpr.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: QUT Centre for Robotics, Queensland University of Technology<br>
‚Ä¢ Dataset: QCR-Event-VPR dataset, Samples: 16, Modality: Event camera stream, Robot kinematics/trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>IBISCape: A Simulated Benchmark for multi-modal SLAM Systems Evaluation in Large-scale Dynamic Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.13455"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AbanobSoliman/IBISCape.git"><img src="https://img.shields.io/github/stars/AbanobSoliman/IBISCape.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universit√© Paris-Saclay, Univ Evry, IBISC Laboratory, 34 Rue du Pelvoux, Evry, 91020, Essonne, France.<br>
‚Ä¢ Dataset: IBISCape, Samples: 34, Modality: stereo-RGB, stereo-DVS, Depth, IMU, GPS, ground truth scene segmentation, vehicle ego-motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>Learn to Predict How Humans Manipulate Large-sized Objects from Interactive Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.12612"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, The University of Hong Kong, Hong Kong SAR<br>
‚Ä¢ Dataset: None, Samples: 18343, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>EventNeRF: Neural Radiance Fields from a Single Colour Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.11896"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/4DV-MPII/EventNeRF"><img src="https://img.shields.io/github/stars/4DV-MPII/EventNeRF.svg?style=social&label=Star"></a><br><a href="https://4dqv.mpi-inf.mpg.de/EventNeRF"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, SIC<br>
‚Ä¢ Dataset: EventNeRF Dataset, Samples: 17, Modality: Colour event streams + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>A High Resolution Multi-exposure Stereoscopic Image & Video Database of Natural Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.11095"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/multi-exposure-stereo-data/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Indian Institute of Technology Madras, Tamil Nadu, 600036, India<br>
‚Ä¢ Dataset: A High Resolution Multi-exposure Stereoscopic Image & Video Database of Natural Scenes, Samples: 18, Modality: Stereoscopic RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>SJ-HD^2R: Selective Joint High Dynamic Range and Denoising Imaging for Dynamic Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.09611"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huawei Noah‚Äôs Ark Lab<br>
‚Ä¢ Dataset: SJ-HD2R RAW-HDR dataset, Samples: 207, Modality: RAW image sequences<br>
‚Ä¢ Dataset: SJ-HD2R RGB-HDR dataset, Samples: 207, Modality: RGB image sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>Capturing and Inferring Dense Full-Body Human-Scene Contact</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.09553"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rich-dataset/RICH"><img src="https://img.shields.io/github/stars/rich-dataset/RICH.svg?style=social&label=Star"></a><br><a href="https://rich.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T ¬®ubingen, Germany<br>
‚Ä¢ Dataset: RICH, Samples: 142, Modality: Multiview 4K RGB videos, 3D body meshes (SMPL-X), 3D laser scene scans, human-scene contact labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>Recurrent Transformer Variational Autoencoders for Multi-Action Motion Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.06741"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Bonn<br>
‚Ä¢ Dataset: PROX (augmented), Samples: 100000 frames, Modality: SMPL fittings + GT action labels<br>
‚Ä¢ Dataset: Charades (augmented), Samples: 1918 sequences, Modality: SMPL fittings + action labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>SCAMPS: Synthetics for Camera Measurement of Physiological Signals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.04197"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/danmcduff/scampsdataset"><img src="https://img.shields.io/github/stars/danmcduff/scampsdataset.svg?style=social&label=Star"></a><br><a href="https://github.com/danmcduff/scampsdataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Microsoft<br>
‚Ä¢ Dataset: SCAMPS, Samples: 2800, Modality: RGB videos + head pose + facial actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>Generating Long Videos of Dynamic Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.03429"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.timothybrooks.com/tech/long-videos"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA, UC Berkeley<br>
‚Ä¢ Dataset: mountain biking dataset, Samples: 1202, Modality: RGB videos<br>
‚Ä¢ Dataset: horseback riding dataset, Samples: 66, Modality: RGB videos<br>
</td></tr>
</table>

## üí™ How to Contribute

If you have a paper or are aware of relevant research that should be incorporated, please contribute via pull requests, issues, email, or other suitable methods.
