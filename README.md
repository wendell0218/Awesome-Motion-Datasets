# Awesome-Motion-Datasets

## üîç Related Papers

‚ö†Ô∏è Automated analysis may be inaccurate.

<table style="width: 100%;">
<tr><td><strong>Date</strong></td><td><strong>Paper</strong></td><td><strong>Contribution</strong></td><td><strong>Links</strong></td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Learning to Track Any Points from Human Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.06233"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST AI<br>
‚Ä¢ Dataset: Anthro-LD, Samples: 1400, Modality: RGB videos + 2D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.05698"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mohsij/space-event-rgb-fusion"><img src="https://img.shields.io/github/stars/mohsij/space-event-rgb-fusion.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AI for Space Group, The University of Adelaide, Australia<br>
‚Ä¢ Dataset: FRESH, Samples: 24, Modality: RGB frames + event data + 6DoF poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Neural-Driven Image Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.05397"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://loongx1.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NUS<br>
‚Ä¢ Dataset: L-Mind, Samples: 23928, Modality: EEG, fNIRS, PPG, head motion (6-axis IMU), speech, image pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Beyond Features: How Dataset Design Influences Multi-Agent Trajectory Prediction Performance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.05098"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robert Bosch GmbH, Stuttgart, Germany<br>
‚Ä¢ Dataset: L4 Motion Forecasting dataset, Samples: 90k, Modality: LiDAR, cameras, radars<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.04955"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xinyueli2896/Expotion.git"><img src="https://img.shields.io/github/stars/xinyueli2896/Expotion.git.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates<br>
‚Ä¢ Dataset: Expotion Dataset, Samples: 7 hours of video, Modality: RGB videos of facial expressions and upper-body gestures<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for Particle Image Velocimetry</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.04750"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: International School, Beijing University of Posts and Telecommunications<br>
‚Ä¢ Dataset: Comprehensive PIV Benchmark Dataset, Samples: 19500, Modality: Synthetic particle image pairs + ground-truth velocity fields<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Grounded Gesture Generation: Language, Motion, and Space</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.04522"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://groundedgestures.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: Grounded Gestures, Samples: 6250, Modality: MoCap (HumanML3D format), Speech, 3D Scene Info<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.02948"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Westlake University, Zhejiang University<br>
‚Ä¢ Dataset: DriveMRP-10K, Samples: 10000, Modality: multimodal dataset comprising scene images (front-view, BEV), motion trajectories (waypoints), and textual annotations (VQA pairs, risk labels)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.02565"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.buzhenhuang.com/works/CloseApp.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southeast University, National University of Singapore<br>
‚Ä¢ Dataset: WildCHI, Samples: 100, Modality: RGB videos + pseudo ground-truth SMPL<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.02479"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/loseevaya/CrowdTrack"><img src="https://img.shields.io/github/stars/loseevaya/CrowdTrack.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: CrowdTrack, Samples: 33, Modality: RGB videos + bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.02200"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/ESTR-CoT"><img src="https://img.shields.io/github/stars/Event-AHU/ESTR-CoT.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei 230601, China<br>
‚Ä¢ Dataset: CoT_ESTR, Samples: 16222, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.00660"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/crs524/MTCNet"><img src="https://img.shields.io/github/stars/crs524/MTCNet.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Medical Ultrasound Image Computing (MUSIC) Lab, School of Biomedical Engineering, Medical School, Shenzhen University, Shenzhen, China<br>
‚Ä¢ Dataset: 4D MV dataset, Samples: 160, Modality: 4D Ultrasound<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.00525"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://djamahl99.github.io/qaymo-pages/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Queensland, Brisbane, Australia<br>
‚Ä¢ Dataset: Box-QAymo, Samples: 13714, Modality: Camera images with box-referenced Q&A pairs derived from 3D object trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.00339"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/Amar-S/MOVi-MC-AC"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Lawrence Livermore National Laboratory<br>
‚Ä¢ Dataset: MOVi-MC-AC, Samples: 2041, Modality: Multi-camera RGB videos, Depth masks, Modal/Amodal segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.24074"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DurrLab/C3VD"><img src="https://img.shields.io/github/stars/DurrLab/C3VD.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Biomedical Engineering, Johns Hopkins University<br>
‚Ä¢ Dataset: C3VDv2, Samples: 192, Modality: RGB videos + depth + surface normals + optical flow + 6-DoF camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23957"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sinoyou.github.io/gavs"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich, Switzerland<br>
‚Ä¢ Dataset: Repurposed DeepFused dataset, Samples: 15, Modality: RGB videos + 3D camera poses + dynamic object masks + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23759"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, NUS, Singapore<br>
‚Ä¢ Dataset: Hyst-YT, Samples: 1980, Modality: RGB surgical videos with part-level segmentation masks<br>
‚Ä¢ Dataset: Lob-YT, Samples: 203, Modality: RGB surgical videos with part-level segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23690"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lucaria-academy.github.io/SynMotion/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ant Group<br>
‚Ä¢ Dataset: MotionBench, Samples: 96-160, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Event-based Tiny Object Detection: A Benchmark Dataset and Baseline</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23575"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ChenYichen9527/Ev-UAV"><img src="https://img.shields.io/github/stars/ChenYichen9527/Ev-UAV.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not found in the provided document<br>
‚Ä¢ Dataset: EV-UAV, Samples: 147, Modality: Event camera stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>DexH2R: A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23152"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dexh2r.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: DexH2R, Samples: 4282, Modality: multi-view RGB-D streams, 3D annotations, robot kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.22718"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of Minnesota<br>
‚Ä¢ Dataset: Partial-RoboArt, Samples: None, Modality: 4D point clouds<br>
‚Ä¢ Dataset: Occluded-RoboArt, Samples: None, Modality: 4D point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.22554"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/seamless_interaction"><img src="https://img.shields.io/github/stars/facebookresearch/seamless_interaction.svg?style=social&label=Star"></a><br><a href="https://huggingface.co/datasets/facebook/seamless-interaction"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta<br>
‚Ä¢ Dataset: Seamless Interaction Dataset, Samples: 64739, Modality: RGB videos, Audio, SMPL-H poses, facial expression codes, text transcripts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Generating Attribute-Aware Human Motions from Textual Prompt</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.21912"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: HumanAttr, Samples: 18199, Modality: SMPL parameters from MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.21765"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/QiLi111/tus-rec-challenge_baseline"><img src="https://img.shields.io/github/stars/QiLi111/tus-rec-challenge_baseline.svg?style=social&label=Star"></a><br><a href="https://github-pages.ucl.ac.uk/tus-rec-challenge/TUS-REC2024/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UCL Hawkes Institute, Department of Medical Physics and Biomedical Engineering, University College London, London, WC1E 6BT, U.K.<br>
‚Ä¢ Dataset: TUS-REC2024, Samples: 2040, Modality: 2D ultrasound videos + 6-DoF pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.21680"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vinayak-vg/PhotonSplat"><img src="https://img.shields.io/github/stars/vinayak-vg/PhotonSplat.svg?style=social&label=Star"></a><br><a href="https://vinayak-vg.github.io/PhotonSplat/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Indian Institute of Technology, Madras, India<br>
‚Ä¢ Dataset: PhotonScenes, Samples: 9, Modality: multi-view SPAD binary images + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.20936"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://physrig.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Illinois Urbana Champaign<br>
‚Ä¢ Dataset: PhysRig Synthetic Dataset, Samples: 120, Modality: Simulated 3D mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.20550"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Tuebingen<br>
‚Ä¢ Dataset: BOAT360, Samples: None, Modality: Fisheye RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.20103"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: BrokenVideos, Samples: 3254, Modality: RGB videos + pixel-level masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.19851"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anima-x.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beihang University, China<br>
‚Ä¢ Dataset: None, Samples: 161023, Modality: Rigged 3D animation sequences (processed into multi-view videos and corresponding 2D pose maps)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>The Starlink Robot: A Platform and Dataset for Mobile Satellite Communication</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.19781"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/StarlinkRobot"><img src="https://img.shields.io/github/stars/github.com/StarlinkRobot.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University College London<br>
‚Ä¢ Dataset: Starlink Robot Dataset, Samples: None, Modality: robot kinematics, pose trajectories, LiDAR point clouds, fisheye camera images, IMU, GPS, communication metrics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Deblurring in the Wild: A Real-World Dataset from Smartphone High-Speed Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.19445"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Dhaka<br>
‚Ä¢ Dataset: SloMoDeblur, Samples: 42045, Modality: blur-sharp image pairs from high-speed videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EvDetMAV: Generalized MAV Detection from Moving Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.19416"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WindyLab/EvDetMAV"><img src="https://img.shields.io/github/stars/WindyLab/EvDetMAV.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology, Zhejiang University<br>
‚Ä¢ Dataset: EventMAV, Samples: 25335, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Matrix-Game: Interactive World Foundation Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.18701"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SkyworkAI/Matrix-Game"><img src="https://img.shields.io/github/stars/SkyworkAI/Matrix-Game.svg?style=social&label=Star"></a><br><a href="https://matrix-game-homepage.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Skywork AI<br>
‚Ä¢ Dataset: Matrix-Game-MC, Samples: 2,700 hours of unlabeled video clips, >1,000 hours of labeled video clips, Modality: RGB videos + action labels (keyboard, mouse) + agent kinematics (position, velocity, orientation)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.18443"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZzhYgwh/TwistEstimator"><img src="https://img.shields.io/github/stars/ZzhYgwh/TwistEstimator.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Automation, Northwestern Polytechnical University, Xi‚Äôan, Shaanxi, 710129 P.R. China.<br>
‚Ä¢ Dataset: None, Samples: 10, Modality: Event camera data, 4D mmWave radar data, IMU data, RTK-GPS ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.17590"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/taco-group/DRAMA-X"><img src="https://img.shields.io/github/stars/taco-group/DRAMA-X.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Texas A&M University<br>
‚Ä¢ Dataset: DRAMA-X, Samples: 5686, Modality: RGB video clips + object trajectories + intent labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.17332"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="P2MFDS Github Repository"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Northwestern Polytechnical University<br>
‚Ä¢ Dataset: None, Samples: 18000, Modality: mmWave 3D point cloud + 3D vibration data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.16856"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/little-snail-f/ParkFormer"><img src="https://img.shields.io/github/stars/little-snail-f/ParkFormer.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Automation, Chinese Academy of Sciences,Beijing 100190, China<br>
‚Ä¢ Dataset: ParkFormer Dataset, Samples: 272, Modality: RGB images, depth maps, ego-motion states (velocity, acceleration), pedestrian trajectories, control commands<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EchoShot: Multi-Shot Portrait Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.15838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://johnneywang.github.io/EchoShot-webpage"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xi‚Äôan Jiaotong University<br>
‚Ä¢ Dataset: PortraitGala, Samples: 650000, Modality: RGB videos + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Toward Rich Video Human-Motion2D Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.14428"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/FooAuto/Toward-Rich-Video-Human-Motion2D-Generation"><img src="https://img.shields.io/github/stars/FooAuto/Toward-Rich-Video-Human-Motion2D-Generation.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Technology, Tongji University, China<br>
‚Ä¢ Dataset: Motion2D-Video-150K, Samples: 150000, Modality: 2D skeleton sequences with textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Action Dubber: Timing Audible Actions via Inflectional Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.13320"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WenlongWan/Audible623"><img src="https://img.shields.io/github/stars/WenlongWan/Audible623.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Engineering, South China University of Technology<br>
‚Ä¢ Dataset: Audible 623, Samples: 623, Modality: RGB videos + frame-level annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>MAMMA: Markerless & Automatic Multi-Person Motion Action Capture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.13040"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, Germany<br>
‚Ä¢ Dataset: MAMMASyn, Samples: 2.8k sequences / 2.5M samples, Modality: Synthetic multi-view RGB video + SMPL-X annotations + dense 2D landmarks + segmentation masks<br>
‚Ä¢ Dataset: Latin-Dance, Samples: 10 sequences, Modality: Vicon MoCap data<br>
‚Ä¢ Dataset: Interacting Couples, Samples: 48 sequences, Modality: Vicon MoCap data<br>
‚Ä¢ Dataset: MAMMAEval-Singles, Samples: 22 sequences, Modality: Multi-view RGB video + Vicon MoCap data + SMPL-X annotations<br>
‚Ä¢ Dataset: MAMMAEval-Dance, Samples: 17 sequences, Modality: Multi-view RGB video + Vicon MoCap data + SMPL-X annotations<br>
‚Ä¢ Dataset: MAMMAEval-Extra, Samples: 16 sequences, Modality: Multi-view RGB video + Vicon MoCap data (standard and extra markers) + SMPL-X annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Multiple Object Tracking in Video SAR: A Benchmark and Tracking Baseline</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.12105"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/softwarePupil/VSMB"><img src="https://img.shields.io/github/stars/softwarePupil/VSMB.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Information Engineering, Beihang University<br>
‚Ä¢ Dataset: Video SAR MOT Benchmark (VSMB), Samples: 45, Modality: Video SAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.11774"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Indian Institute of Technology Kanpur<br>
‚Ä¢ Dataset: Isometric-Multiclass Dataset (IMCD), Samples: 4339, Modality: 2D pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>RationalVLA: A Rational Vision-Language-Action Model with Dual System</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.10826"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://irpn-eai.github.io/RationalVLA/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: RAtional MAnipulation (RAMA), Samples: 14412, Modality: Language instructions + RGB images + robot arm kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial Motion Capture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.10580"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZuoCX1996/TIC"><img src="https://img.shields.io/github/stars/ZuoCX1996/TIC.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xiamen University, China<br>
‚Ä¢ Dataset: DSTIC, Samples: 1.04M sequences, Modality: Optical motion capture (body pose, absolute IMU orientation and acceleration), raw IMU readings<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.10353"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://Motion-R1.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: GigaAI<br>
‚Ä¢ Dataset: MotionCoT Data Engine, Samples: None, Modality: Chain-of-Thought (CoT) annotations paired with existing motion descriptions and motion sequences (MoCap)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.09997"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta, UC Merced<br>
‚Ä¢ Dataset: Customized Kubric dataset, Samples: 40000, Modality: Synthetic multi-view videos + 3D scene flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.09663"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Anhui University<br>
‚Ä¢ Dataset: RS-Art, Samples: 18 objects (6 categories, 3 instances each), each with 7 distinct articulation states and over 400 observations per instance., Modality: RGB-D captures, camera poses (intrinsics/extrinsics), reverse-engineered 3D models (USD, URDF, PLY) with part meshes, textures, joint definitions, and physics properties.<br>
‚Ä¢ Dataset: PartNet-Mobility (extended), Samples: Extended with 2-3 new instances per category across 8 categories, plus 4 objects with three movable parts., Modality: Synthetic 3D models with articulation data.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Synthetic Human Action Video Data Generation with Pose Transfer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.09411"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="synthetic-human-action.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SSPS<br>
‚Ä¢ Dataset: RANDOM People, Samples: 3600, Modality: Synthetic RGB videos + 3D Gaussian avatars + source identity videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Audio-Sync Video Generation with Multi-Stream Temporal Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.08003"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hjzheng.net/projects/MTV/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing Academy of Artificial Intelligence<br>
‚Ä¢ Dataset: DEMIX, Samples: 392000, Modality: RGB videos + demixed audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07865"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vLAR-group/FreeGave"><img src="https://img.shields.io/github/stars/vLAR-group/FreeGave.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: vLAR Group, The Hong Kong Polytechnic University<br>
‚Ä¢ Dataset: FreeGave-GoPro Dataset, Samples: 6, Modality: multi-view RGB videos with camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07860"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/uzh-rpg/event_based_ping_pong_ball_trajectory_prediction"><img src="https://img.shields.io/github/stars/uzh-rpg/event_based_ping_pong_ball_trajectory_prediction.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics and Perception Group, University of Zurich, Switzerland<br>
‚Ä¢ Dataset: Egocentric Event-Based Ping Pong Trajectories, Samples: 30, Modality: 3D ground-truth ball trajectories, event streams, RGB videos, eye-tracking, IMU, audio, SLAM poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07603"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: SurgBench, Samples: 225250, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>OpenDance: Multimodal Controllable 3D Dance Generation Using Large-scale Internet Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07565"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://open-dance.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: OpenDance5D, Samples: 41000, Modality: RGB video, audio waveform, 2D skeletal keypoints, 3D SMPL motion, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Improving Traffic Signal Data Quality for the Waymo Open Motion Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07150"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/michigan-traffic-lab/WOMD-Traffic-Signal-Data-Improvement"><img src="https://img.shields.io/github/stars/michigan-traffic-lab/WOMD-Traffic-Signal-Data-Improvement.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Civil and Environmental Engineering, University of Michigan, Ann Arbor, MI, USA<br>
‚Ä¢ Dataset: Waymo Open Motion Dataset - Improved Traffic Signal Data, Samples: over 360,000 scenarios, Modality: Vehicle trajectories with imputed traffic signal states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.06596"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://fulmen67.github.io/EV-LayerSegNet"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Advanced Research and Innovation Center, Khalifa University<br>
‚Ä¢ Dataset: Not explicitly named in the paper, Samples: 1025, Modality: simulated event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>(LiFT) Lightweight Fitness Transformer: A language-vision model for Remote Monitoring of Physical Training</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.06480"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Electrical and Computer Engineering University of California, San Diego<br>
‚Ä¢ Dataset: Olympia, Samples: 7618, Modality: RGB videos + 3D skeletal keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.06318"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southeast University<br>
‚Ä¢ Dataset: GyroPeak-100, Samples: None, Modality: IMU signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>ChronoTailor: Harnessing Attention Guidance for Fine-Grained Video Virtual Try-On</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.05858"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Communication University of China<br>
‚Ä¢ Dataset: StyleDress, Samples: 12500, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Gen4D: Synthesizing Humans and Scenes in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.05397"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Vision and Image Processing Lab, University of Waterloo, Canada<br>
‚Ä¢ Dataset: SportPAL, Samples: 2012, Modality: Synthetic images with 2D/3D human pose annotations and SMPLX parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>FreeTimeGS: Free Gaussian Primitives at Anytime and Anywhere for Dynamic Scene Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.05348"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zju3dv.github.io/freetimegs/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: SelfCap, Samples: 8, Modality: multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>FRED: The Florence RGB-Event Drone Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.05163"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://miccunifi.github.io/FRED/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Florence<br>
‚Ä¢ Dataset: FRED (Florence RGB-Event Drone dataset), Samples: 4620, Modality: RGB videos and event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.04224"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://oxdan.active.vision/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Oxford<br>
‚Ä¢ Dataset: Oxford Day-and-Night, Samples: 491750, Modality: Egocentric RGB/grayscale video + IMU + 6DoF camera poses + 3D point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EV-Flying: an Event-based Dataset for In-The-Wild Recognition of Flying Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.04048"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Florence<br>
‚Ä¢ Dataset: EV-Flying, Samples: 3206, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Controllable Human-centric Keyframe Interpolation with Generative Prior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.03119"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gseancdat.github.io/projects/PoseFuse3D_KI"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: CHKI-Video, Samples: 2614, Modality: RGB videos + 2D poses + 3D SMPL-X parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.03107"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ByteDance-Seed/BM-code"><img src="https://img.shields.io/github/stars/ByteDance-Seed/BM-code.svg?style=social&label=Star"></a><br><a href="https://boese0601.github.io/bytemorph"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ByteDance Seed<br>
‚Ä¢ Dataset: ByteMorph-6M, Samples: 6450000, Modality: image pairs + text instructions<br>
‚Ä¢ Dataset: ByteMorph-Bench, Samples: 613, Modality: image pairs + text instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.02845"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LEI-QI-233/HAR-in-Space"><img src="https://img.shields.io/github/stars/LEI-QI-233/HAR-in-Space.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Karlsruhe Institute of Technology<br>
‚Ä¢ Dataset: MicroG-4M, Samples: 4759, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.02733"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/LecterF/LinkTo-Anime"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Macau University of Science and Technology, Macau, China<br>
‚Ä¢ Dataset: LinkTo-Anime, Samples: 395, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>InterRVOS: Interaction-aware Referring Video Object Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.02356"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cvlab-kaist.github.io/InterRVOS"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST AI<br>
‚Ä¢ Dataset: InterRVOS-8K, Samples: 8738, Modality: RGB videos + language expressions + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.01674"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nju-pcalab.github.io/projects/MotionSight"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: MotionVid-QA, Samples: 40000, Modality: RGB videos + QA pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.01608"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/amathislab/EPFL-Smart-Kitchen"><img src="https://img.shields.io/github/stars/amathislab/EPFL-Smart-Kitchen.svg?style=social&label=Star"></a><br><a href="https://github.com/amathislab/EPFL-Smart-Kitchen"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: √âcole Polytechnique F√©d√©rale de Lausanne (EPFL), Lausanne<br>
‚Ä¢ Dataset: EPFL-Smart-Kitchen-30, Samples: 60189, Modality: RGB-D videos, IMU, eye gaze, 3D hand poses, 3D body poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.00411"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: LoHoSet, Samples: 23000, Modality: RGB-D images, language instructions, robot actions (end-effector Cartesian positions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Sequence-Based Identification of First-Person Camera Wearers in Third-Person Views</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.00394"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Indiana University Bloomington<br>
‚Ä¢ Dataset: TF2025, Samples: None, Modality: Synchronized first- and third-person videos with segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>MotionPersona: Characteristics-aware Locomotion Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.00173"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motionpersona25.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: MotionPersona, Samples: 3150, Modality: MoCap joints (SMPL-X)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>From Motion to Behavior: Hierarchical Modeling of Humanoid Generative Behavior Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.00043"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sun Yat-sen University<br>
‚Ä¢ Dataset: GBC-100K, Samples: 123700, Modality: RGB videos + SMPL parameters + hierarchical textual annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Spatiotemporal Analysis of Forest Machine Operations Using 3D Video Classification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.24375"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.nibio.no/maciekwielgosz/forest_video_classification"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Norwegian Institute of Bioeconomy Research (NIBIO)<br>
‚Ä¢ Dataset: Forest Machine Operations Video Dataset, Samples: 349, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.24249"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China<br>
‚Ä¢ Dataset: bronchoscopy localization dataset, Samples: 66, Modality: Bronchoscopy videos + annotated 6-DOF camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Autoregressive Meta-Actions for Unified Controllable Trajectory Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.23612"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://arma-traj.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: Waymo Motion Dataset with frame-level meta-action annotations, Samples: None, Modality: Agent trajectories and map data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.23525"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xyz123xyz456/hallo4"><img src="https://img.shields.io/github/stars/xyz123xyz456/hallo4.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: Audio-Driven Portrait DPO Dataset, Samples: 20000, Modality: RGB video pairs with human preference labels (motion-video alignment, portrait fidelity)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Semantics-Aware Human Motion Generation from Audio Instructions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.23465"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Chinese Academy of Sciences, China<br>
‚Ä¢ Dataset: Oral Dataset (from HumanML3D), Samples: 87384, Modality: audio instructions + MoCap-based pose sequences<br>
‚Ä¢ Dataset: Oral Dataset (from KIT-ML), Samples: 12696, Modality: audio instructions + MoCap-based pose sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.22977"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vivocameraresearch.github.io/hypermotion/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Centre for Computer Animation, Bournemouth University, UK<br>
‚Ä¢ Dataset: Open-HyperMotionX Dataset, Samples: 19597, Modality: RGB videos + 2D pose annotations<br>
‚Ä¢ Dataset: HyperMotionX Bench, Samples: 100, Modality: RGB videos + 2D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.22859"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://muskie82.github.io/4dtam/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dyson Robotics Laboratory, Imperial College London<br>
‚Ä¢ Dataset: Sim4D, Samples: 50, Modality: RGB images, depth, surface normals, foreground masks, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.22769"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of St Andrews<br>
‚Ä¢ Dataset: MotionGaze, Samples: 803K+ IMU readings, Modality: IMU (accelerometer, gyroscope), RGB images, Gaze coordinates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.22335"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://aczheng-cai.github.io/up_slam.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Chinese Academy of Sciences<br>
‚Ä¢ Dataset: Bonn RGB-D Dynamic Dataset masks, Samples: None, Modality: Dynamic object masks for RGB-D sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.21653"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bwgzk-keke.github.io/DiffPhy/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Johns Hopkins University<br>
‚Ä¢ Dataset: PhyHQ, Samples: 8000, Modality: RGB videos + text captions + physical phenomena labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Frame In-N-Out: Unbounded Controllable Image-to-Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.21491"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://uva-computer-vision-lab.github.io/Frame-In-N-Out/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Virginia<br>
‚Ä¢ Dataset: Frame In-N-Out Training Dataset, Samples: 72300, Modality: RGB videos with processed metadata including motion trajectories, text prompts, bounding boxes, and identity reference images<br>
‚Ä¢ Dataset: Frame In-N-Out Evaluation Benchmark, Samples: 372, Modality: RGB videos with processed metadata including motion trajectories, text prompts, bounding boxes, and identity reference images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>RefAV: Towards Planning-Centric Scenario Mining</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.20981"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="cainand.github.io/RefAV/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: RefAV, Samples: 1000, Modality: LiDAR, 360¬∞ ring cameras, HD maps, 3D track annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.20460"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: VCIP, CS, Nankai University, Horizon Robotics<br>
‚Ä¢ Dataset: PM-X, Samples: 600, Modality: Dual-state rendered images, URDF annotations, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>HAODiff: Human-Aware One-Step Diffusion via Dual-Prompt Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.19742"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gobunu/HAODiff"><img src="https://img.shields.io/github/stars/gobunu/HAODiff.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: MPII-Test, Samples: 5427, Modality: RGB images with human motion blur<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.19386"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nate-gillman/force-prompting"><img src="https://img.shields.io/github/stars/nate-gillman/force-prompting.svg?style=social&label=Star"></a><br><a href="https://force-prompting.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: Global wind force dataset, Samples: 15000, Modality: Synthetic RGB videos + Force vectors<br>
‚Ä¢ Dataset: Local point force dataset, Samples: 23000, Modality: Synthetic RGB videos + Force vectors + Application points<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>PosePilot: An Edge-AI Solution for Posture Correction in Physical Exercises</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.19186"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gadhvirushiraj/PosePilot"><img src="https://img.shields.io/github/stars/gadhvirushiraj/PosePilot.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: HTI Lab, Plaksha University, Mohali, India<br>
‚Ä¢ Dataset: In-house Dataset, Samples: 336, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>EventEgoHands: Event-based Egocentric 3D Hand Mesh Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.19169"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Keio University<br>
‚Ä¢ Dataset: N-HOT3D, Samples: 447704, Modality: synthetic event streams + 3D hand mesh annotations (MANO parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>BiomechGPT: Towards a Biomechanically Fluent Multimodal Foundation Model for Clinically Relevant Motion Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.18465"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Neuroscience, Northwestern University; Shirley Ryan AbilityLab<br>
‚Ä¢ Dataset: Not explicitly named, Samples: 27000 motion-question-answer pairs, Modality: Biomechanical trajectories (joint angles and velocities)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.17677"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ophnet-3d.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Monash University<br>
‚Ä¢ Dataset: OphNet-3D, Samples: 41, Modality: Multi-view RGB-D videos with annotated MANO hand meshes and 6-DoF instrument poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.17201"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Informatics Institute, University of Amsterdam<br>
‚Ä¢ Dataset: Fish Data, Samples: 3796, Modality: RGB videos + MOT annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.17114"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/BASHLab/RAVEN"><img src="https://img.shields.io/github/stars/BASHLab/RAVEN.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical & Computer Engineering
Worcester Polytechnic Institute<br>
‚Ä¢ Dataset: AVS-QA, Samples: 300000, Modality: RGB video + audio + IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.16602"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Peking University<br>
‚Ä¢ Dataset: MEgoHand Unified Dataset, Samples: 24000, Modality: RGB-D frames + MANO hand parameters + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.16249"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China.<br>
‚Ä¢ Dataset: DOFS (Deformable Object with Full Spatial information), Samples: 120 episodes (70 simulation, 50 real-world), Modality: multi-view RGB-D images, robot gripper poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.15287"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://intothemild.github.io/GS2E.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Computer Engineering, Peking University<br>
‚Ä¢ Dataset: GS2E, Samples: 1150, Modality: RGB frames, event streams, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Intentional Gesture: Deliver Your Intentions with Gestures for Speech</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.15197"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://andypinxinliu.github.io/Intentional-Gesture"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Rochester<br>
‚Ä¢ Dataset: InG (Intention-Grounded Gestures), Samples: 47913, Modality: SMPL-based gesture data + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.14866"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nisarganc.github.io/UPTor-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Bosch Corporate Research, Robert Bosch GmbH, Stuttgart, Germany, University of Technology Nuremberg (UTN), Germany<br>
‚Ä¢ Dataset: DARKO, Samples: 508, Modality: 3D Poses from RGB<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Learning collision risk proactively from naturalistic driving data at scale</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.13556"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yiru-Jiao/GSSM"><img src="https://img.shields.io/github/stars/Yiru-Jiao/GSSM.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.15787/VTT1/EFYEJR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Delft University of Technology<br>
‚Ä¢ Dataset: Bird‚Äôs eye view trajectory reconstruction of naturalistic crashes and near-crashes in the SHRP2 NDS, Samples: 6664, Modality: Bird's eye view trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>eStonefish-scenes: A synthetically generated dataset for underwater event-based optical flow prediction tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.13309"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/CIRS-Girona/estonefish-scenes"><img src="https://img.shields.io/github/stars/CIRS-Girona/estonefish-scenes.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/15130453"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Vision and Robotics Research Institute (ViCOROB), Universitat de Girona (UdG), Spain<br>
‚Ä¢ Dataset: eStonefish-scenes, Samples: 8, Modality: event streams, grayscale images, ground truth optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Event-Driven Dynamic Scene Depth Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.13279"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NUS<br>
‚Ä¢ Dataset: EventDC-Real, Samples: 15845, Modality: Color images, LiDAR, Event streams<br>
‚Ä¢ Dataset: EventDC-SemiSyn, Samples: 9307, Modality: Color images, LiDAR, Event streams<br>
‚Ä¢ Dataset: EventDC-FullSyn, Samples: 21500, Modality: Color images, Depth data, Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>FlowCut: Unsupervised Video Instance Segmentation via Temporal Mask Matching</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.13174"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Vision Group, University of Bern, Switzerland<br>
‚Ä¢ Dataset: FlowCut pseudo-labeled video dataset, Samples: 167365, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.12774"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Western Australia<br>
‚Ä¢ Dataset: UniHM Dataset, Samples: 44962, Modality: SMPL motion + text descriptions + scene voxels + object point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Event-based Star Tracking under Spacecraft Jitter: the e-STURT Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.12588"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zenodo.org/records/14031911"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Australian Institute for Machine Learning<br>
‚Ä¢ Dataset: e-STURT (Event-based Star Tracking Under Jitter), Samples: 200, Modality: event streams + piezoelectric actuator telemetry<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.11920"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University<br>
‚Ä¢ Dataset: H2R-UR5-SSv2-1M-Gripper, Samples: 62500, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-SSv2-1M-Leaphand, Samples: 62500, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-SSv2-1M-Mix, Samples: 62500, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-Franka-SSv2-1M, Samples: 62500, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-Ego4D-1M-Gripper, Samples: 117624, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-Ego4D-1M-Leaphand, Samples: 117624, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-Ego4D-1M-Mix, Samples: 117624, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-Franka-Ego4D-1M, Samples: 117624, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.11868"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National University of Defense Technology<br>
‚Ä¢ Dataset: MonoMobility Dataset, Samples: 26, Modality: monocular videos, 3D point clouds, motion part segmentation, motion attribute annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ElderFallGuard: Real-Time IoT and Computer Vision-Based Fall Detection System for Elderly Safety</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.11845"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information and Communicaton Technology, Bangladesh University of Engineering Technology, Dhaka, bangladesh<br>
‚Ä¢ Dataset: ElderFallGuard dataset, Samples: 7200, Modality: RGB images + 2D pose landmarks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.11282"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="github.com/shrutarv/MTevent_toolkit"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Dortmund<br>
‚Ä¢ Dataset: MTevent, Samples: 75, Modality: Stereo event camera streams, RGB videos, MoCap poses, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Robust 2D lidar-based SLAM in arboreal environments without IMU/GNSS</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.10847"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Pontificia Universidad Cat ¬¥olica de Chile<br>
‚Ä¢ Dataset: Pullally, Samples: None, Modality: LiDAR, RTK-GNSS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.10696"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tartanair.org/tartanground"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotic Systems Lab, ETH Zurich, Zurich, Switzerland<br>
‚Ä¢ Dataset: TartanGround, Samples: 910 trajectories / 1.5 million samples, Modality: RGB stereo video, depth, optical flow, stereo disparity, LiDAR point clouds, ground truth poses, semantic segmented images, occupancy maps, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.10205"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universitat de Barcelona, Spain<br>
‚Ä¢ Dataset: Foodkit, Samples: 21, Modality: RGB videos, image sequences, camera pose trajectories, segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.09694"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AgibotTech/EWMBench"><img src="https://img.shields.io/github/stars/AgibotTech/EWMBench.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AgiBot, HIT<br>
‚Ä¢ Dataset: EWMBENCH Dataset, Samples: 100, Modality: RGB videos, robot end-effector trajectories, textual instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.08986"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Arkansas, Fayetteville, AR, USA<br>
‚Ä¢ Dataset: ChicGrasp, Samples: 50, Modality: RGB video, robot proprioception (joint positions, joint velocities), binary gripper states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>RDD: Robust Feature Detector and Descriptor using Deformable Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.08013"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://xtcpete.github.io/rdd/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Creative Technologies, University of Southern California<br>
‚Ä¢ Dataset: MegaDepth-View, Samples: 1487, Modality: RGB images + camera poses + depth maps<br>
‚Ä¢ Dataset: Air-to-Ground, Samples: 1500, Modality: RGB images + camera poses + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>BETTY Dataset: A Multi-modal Dataset for Full-Stack Autonomy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.07266"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pitt-mit-iac.github.io/betty-dataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics Institute, Carnegie Mellon University<br>
‚Ä¢ Dataset: BETTY Dataset, Samples: None, Modality: Camera, LiDAR, Radar, IMU, GNSS, Odometry, Planned Trajectories, Actuation Commands (Throttle, Brake, Steering), Tire State (temperature, pressure, speed, torque, slip angle), Suspension<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.07007"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zyzhangUstc/MELLM"><img src="https://img.shields.io/github/stars/zyzhangUstc/MELLM.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, University of Science and Technology of China<br>
‚Ä¢ Dataset: Instruction-following MEU Dataset, Samples: 4793, Modality: Instruction-description pairs and motion-enhanced color maps derived from optical flow of video frames (based on the DFME dataset).<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.06537"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence, Beihang University; The Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: MRFashion-7K, Samples: 7335, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.05001"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nie-lang/StabStitch2"><img src="https://img.shields.io/github/stars/nie-lang/StabStitch2.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Science, Beijing Jiaotong University<br>
‚Ä¢ Dataset: StabStitch-D, Samples: over 100 video pairs, Modality: RGB video pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.04974"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wengwanjiang.github.io/ReAlign-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Engineering, Southeast University, Nanjing, China<br>
‚Ä¢ Dataset: BiHumanML3D, Samples: 13312, Modality: 3D motion sequences + bilingual (English/Chinese) text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.04203"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Qzping/ELGAR"><img src="https://img.shields.io/github/stars/Qzping/ELGAR.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Central Conservatory of Music, China and Tsinghua University, China<br>
‚Ä¢ Dataset: SPD-GEN, Samples: 81, Modality: MoCap joint rotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>A Dataset and Toolkit for Multiparameter Cardiovascular Physiology Sensing on Rings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.04172"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/thuhci/RingTool"><img src="https://img.shields.io/github/stars/thuhci/RingTool.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: ùúè-Ring, Samples: 28.21 hours of raw data from 34 subjects, Modality: PPG (infrared and red channels), 3-axis accelerometer data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.04002"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mshoe/PARC"><img src="https://img.shields.io/github/stars/mshoe/PARC.svg?style=social&label=Star"></a><br><a href="https://github.com/mshoe/PARC"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Simon Fraser University, Canada<br>
‚Ä¢ Dataset: Initial Parkour Dataset, Samples: None, Modality: MoCap joints with contact labels<br>
‚Ä¢ Dataset: PARC Generated Dataset, Samples: approximately 3000 sequences, Modality: Physics-corrected kinematic motions with contact labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03738"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://amo-humanoid.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC San Diego<br>
‚Ä¢ Dataset: AMO dataset, Samples: None, Modality: Dynamically-feasible whole-body reference joint angles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>A Unit Enhancement and Guidance Framework for Audio-Driven Avatar Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03603"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: CNAS (Chinese News Anchor Speech Dataset), Samples: 1473, Modality: RGB videos + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>AquaticVision: Benchmarking Visual SLAM in Underwater Environment with Events and Frames</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03448"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/aquaticvision-lias"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Data Science, The Chinese University of Hong Kong, Shenzhen, P. R. China<br>
‚Ä¢ Dataset: AquaticVision, Samples: 9, Modality: Stereo event streams, stereo grayscale frames, IMU, motion capture trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03154"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Simon Fraser University, Canada<br>
‚Ä¢ Dataset: BrokenAMASS, Samples: over 40 hours, Modality: MoCap skeleton<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03116"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Lab of Multispectral Information Intelligent Processing Technology School of Artificial Intelligence and Automation, Huazhong University of Science and Technology<br>
‚Ä¢ Dataset: Complex, High-speed Motion (CHMD), Samples: None, Modality: RGB videos + Events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.01838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://kevinlee09.github.io/research/MVHumanNet++/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen<br>
‚Ä¢ Dataset: MVHumanNet++, Samples: 60000, Modality: Multi-view RGB videos, human masks, camera parameters, 2D/3D keypoints, SMPL/SMPLX parameters, normal maps, depth maps, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.01746"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mattie-e.github.io/Co3/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: GES-Inter, Samples: 27390, Modality: 3D human postures (SMPL-X) + separated audio + text transcripts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.00866"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kocurvik/rdnet"><img src="https://img.shields.io/github/stars/kocurvik/rdnet.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Mathematics, Physics and Informatics, Comenius University Bratislava, Bratislava, Slovakia.<br>
‚Ä¢ Dataset: ROTUNDA and CATHEDRAL, Samples: 2891, Modality: RGB images + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>SOTA: Spike-Navigated Optimal TrAnsport Saliency Region Detection in Composite-bias Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.00394"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lwxfight/sota"><img src="https://img.shields.io/github/stars/lwxfight/sota.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory for Multimedia Information Processing, Peking University; Hubei Key Laboratory of Transportation Internet of Things, Wuhan University of Technology<br>
‚Ä¢ Dataset: SPIKE-DAVIS, Samples: None, Modality: Synthetic spike streams from interpolated videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Fine-grained spatial-temporal perception for gas leak segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.00295"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of British Columbia - Okanagan<br>
‚Ä¢ Dataset: GasVid, Samples: 187, Modality: infrared videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.21718"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing University of Posts and Telecommunications, Beijing, China<br>
‚Ä¢ Dataset: ListenerX, Samples: 6683, Modality: 3D parametric head model (FLAME) sequences, speaker audio, text descriptions, emotion intensity tags<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.20808"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bit-bots.github.io/SoccerDiffusion"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Hamburg<br>
‚Ä¢ Dataset: SoccerDiffusion Dataset, Samples: 88, Modality: RGB images, IMU rotations, joint states, joint commands, game state<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.20234"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UP-COUNT/tracking"><img src="https://img.shields.io/github/stars/UP-COUNT/tracking.svg?style=social&label=Star"></a><br><a href="https://up-count.github.io/tracking"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Robotics and Machine Intelligence, Pozna ¬¥n University of Technology<br>
‚Ä¢ Dataset: UP-COUNT-TRACK, Samples: 31, Modality: RGB videos + trajectory annotations + sensor metadata<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.19654"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Loughborough University, Epinal Way, Loughborough, LE11 3TU, Leicestershire, United Kingdom<br>
‚Ä¢ Dataset: DRL-generated 2D SLAM error dataset (unnamed in paper), Samples: 75000, Modality: 2D Occupancy Grid Maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.19514"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Lappeenranta-Lahti University of Technology LUT, Finland<br>
‚Ä¢ Dataset: FSAnno, Samples: 783, Modality: Motion data, skeleton data, RGB video, audio, text annotations<br>
‚Ä¢ Dataset: FSBench, Samples: 783, Modality: Motion data, skeleton data, RGB video, audio, text annotations (QA pairs, comments, scores)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.18864"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: Particle Scenes with Spike and Displacement (PSSD), Samples: 30000 sample sequences (10,000 for each of 3 sub-datasets), Modality: Spike streams + ground truth displacement fields + images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>E-VLC: A Real-World Dataset for Event-based Visible Light Communication And Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.18521"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://woven-visionai.github.io/evlc-dataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Woven by Toyota<br>
‚Ä¢ Dataset: E-VLC, Samples: 110, Modality: Event camera streams, Frame-based videos, Ground-truth camera poses, LED marker positions, Bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Dynamic Camera Poses and Where to Find Them</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17788"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://research.nvidia.com/labs/dir/dynpose-100k"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA, University of Michigan<br>
‚Ä¢ Dataset: DynPose-100K, Samples: 100131, Modality: RGB videos + camera poses<br>
‚Ä¢ Dataset: Lightspeed, Samples: 36, Modality: Synthetic videos + ground truth camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17414"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://2y7c3.github.io/3DV-TON/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DAMO Academy, Alibaba Group; Hupan Lab<br>
‚Ä¢ Dataset: HR-VVT, Samples: 130, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17371"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://deepscenario.github.io/DSC3D/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DeepScenario, TU Munich, Munich Center for Machine Learning<br>
‚Ä¢ Dataset: DeepScenario Open 3D Dataset (DSC3D), Samples: 175000, Modality: 6DoF bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Contrastive Learning for Continuous Touch-Based Authentication</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17271"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: North China University of Technology<br>
‚Ä¢ Dataset: Ffinger, Samples: interaction data from 29 participants, Modality: Multi-channel time series of finger trajectories (coordinates, timestamp, pressure, area, velocity, direction)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Advancing Radar Hand Gesture Recognition: A Hybrid Spectrum Synthetic Framework Merging Simulation with Neural Networks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.16423"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Institute of Health Data Science, Peking University<br>
‚Ä¢ Dataset: unnamed radar-vision HGR dataset, Samples: None, Modality: mmWave radar signals, 3D hand joint coordinates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>SILM: A Subjective Intent Based Low-Latency Framework for Multiple Traffic Participants Joint Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.16377"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory of General Artificial Intelligence, Key Laboratory of Machine Perception (MoE), School of Intelligence Science and Technology, Peking University, Beijing 100871, China.<br>
‚Ä¢ Dataset: CAIC-TP, Samples: more than 25000 sequences, Modality: Trajectory sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.15786"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gdaosu.github.io/sat2groundscape"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Ohio State University<br>
‚Ä¢ Dataset: Sat2GroundScape, Samples: 45000 sequences (estimated from 90 scenes * ~500 sequences/scene); containing 25,000+ panoramic pairs and 100,000+ perspective pairs, Modality: Satellite images, ground-view panoramic/perspective images, depth maps, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Towards Understanding Camera Motions in Any Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.15376"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/linzhiqiu/camerabench"><img src="https://img.shields.io/github/stars/linzhiqiu/camerabench.svg?style=social&label=Star"></a><br><a href="https://linzhiqiu.github.io/papers/camerabench"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CMU<br>
‚Ä¢ Dataset: CameraBench, Samples: 3381, Modality: RGB videos + camera motion labels + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.14602"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://k2muse.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; University of Chinese Academy of Sciences, Beijing, China<br>
‚Ä¢ Dataset: K2MUSE, Samples: None, Modality: kinematics (MoCap markers), kinetics (force plates), amplitude-mode ultrasound (AUS), surface electromyography (sEMG)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>MILUV: A Multi-UAV Indoor Localization dataset with UWB and Vision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.14376"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/decargroup/miluv"><img src="https://img.shields.io/github/stars/decargroup/miluv.svg?style=social&label=Star"></a><br><a href="https://decargroup.github.io/miluv/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: McGill University<br>
‚Ä¢ Dataset: MILUV, Samples: 36, Modality: UWB, stereo vision, monocular vision, IMU, laser rangefinder, magnetometer, MoCap poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Adversarial Locomotion and Motion Imitation for Humanoid Policy Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.14305"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://almi-humanoid.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Artificial Intelligence (TeleAI), China Telecom<br>
‚Ä¢ Dataset: ALMI-X, Samples: 81,549 trajectories, Modality: Robot kinematic trajectories (states, actions, joint angles, etc.) with language descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.12284"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bit.ly/LatentAct"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Illinois Urbana-Champaign<br>
‚Ä¢ Dataset: HoloAssist with 3D hand motion and contact annotations, Samples: 15000, Modality: 3D hand pose trajectories + contact maps (from egocentric RGB videos)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Coding-Prior Guided Diffusion Network for Video Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.12222"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="Not provided in the paper."><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the paper.<br>
‚Ä¢ Dataset: GoPro dataset augmented with coding priors, Samples: 3214 image pairs, Modality: RGB videos + Motion Vectors + Coding Residuals<br>
‚Ä¢ Dataset: DVD dataset augmented with coding priors, Samples: 71 videos, Modality: RGB videos + Motion Vectors + Coding Residuals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>CodingHomo: Bootstrapping Deep Homography With Video Coding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.12165"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/liuyike422/CodingHomo"><img src="https://img.shields.io/github/stars/liuyike422/CodingHomo.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information and Communication Engineering, University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: CA-unsup, Samples: 446200, Modality: Image pairs + motion vectors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Action Anticipation from SoccerNet Football Video Broadcasts</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.12021"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MohamadDalal/FAANTRA"><img src="https://img.shields.io/github/stars/MohamadDalal/FAANTRA.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Aalborg University<br>
‚Ä¢ Dataset: SoccerNet Ball Action Anticipation dataset, Samples: 12433, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>RESPLE: Recursive Spline Estimation for LiDAR-Based Odometry</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.11580"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ASIG-X/RESPLE"><img src="https://img.shields.io/github/stars/ASIG-X/RESPLE.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Link√∂ping University, Sweden<br>
‚Ä¢ Dataset: HelmDyn, Samples: 10, Modality: LiDAR, IMU, MoCap<br>
‚Ä¢ Dataset: R-Campus, Samples: 1, Modality: LiDAR, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.11521"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: INTER DRIVE, Samples: 150000, Modality: Agent trajectories + natural language annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.11326"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pvuw.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Big Data, Fudan University<br>
‚Ä¢ Dataset: MOSE (extended test set), Samples: None, Modality: RGB videos + segmentation masks<br>
‚Ä¢ Dataset: MeViS (extended test set), Samples: None, Modality: RGB videos + language expressions + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.10905"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: InterHF, Samples: 90000, Modality: annotated video clips<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>SeeTree -- A modular, open-source system for tree detection and orchard localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.10764"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Jostan86/pf_orchard_localization"><img src="https://img.shields.io/github/stars/Jostan86/pf_orchard_localization.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Collaborative Robotics and Intelligent Systems (CoRIS) Institute, Oregon State University<br>
‚Ä¢ Dataset: SeeTree Dataset, Samples: 55, Modality: IMU, GNSS, RGB-D, wheel odometry<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>HUMOTO: A 4D Dataset of Mocap Human Object Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.10414"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jiaxin-lu.github.io/humoto/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Texas at Austin, Adobe Research<br>
‚Ä¢ Dataset: HUMOTO, Samples: 736, Modality: MoCap (full-body and hands), RGB-D videos, 3D object models, text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>RGB-Event based Pedestrian Attribute Recognition: A Benchmark Dataset and An Asymmetric RWKV Fusion Framework</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.10018"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/OpenPAR"><img src="https://img.shields.io/github/stars/Event-AHU/OpenPAR.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: EventPAR, Samples: 100000, Modality: RGB frames and event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-wave Point Cloud Sequence</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.09862"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University, Bytedance Research<br>
‚Ä¢ Dataset: virtual radar-text dataset, Samples: 13308, Modality: mmWave radar point cloud sequence + text<br>
‚Ä¢ Dataset: real test dataset, Samples: 375, Modality: mmWave radar point cloud sequence + text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>CamMimic: Zero-Shot Image To Camera Motion Personalized Video Generation Using Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.09472"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cammimic.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland College Park<br>
‚Ä¢ Dataset: None, Samples: 680, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Low-Light Image Enhancement using Event-Based Illumination Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.09379"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: INSAIT, Sofia University<br>
‚Ä¢ Dataset: EvLowLight, Samples: 60, Modality: RGB images + temporal-mapping events + motion events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.09129"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: dConstruct Robotics<br>
‚Ä¢ Dataset: Multi-camera SLAM Dataset, Samples: 4, Modality: RGB images, IMU, Lidar, Pose Trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.08222"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/F3Set/F3Set"><img src="https://img.shields.io/github/stars/F3Set/F3Set.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ningbo University<br>
‚Ä¢ Dataset: F3Set (tennis), Samples: 11584, Modality: RGB videos + event annotations<br>
‚Ä¢ Dataset: F3Set (badminton), Samples: 112, Modality: RGB videos + event annotations<br>
‚Ä¢ Dataset: F3Set (table tennis), Samples: 42, Modality: RGB videos + event annotations<br>
‚Ä¢ Dataset: F3Set (tennis doubles), Samples: 78, Modality: RGB videos + event annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>RealCam-Vid: High-resolution Video Dataset with Dynamic Scenes and Metric-scale Camera Movements</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.08212"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZGCTroy/RealCam-Vid"><img src="https://img.shields.io/github/stars/ZGCTroy/RealCam-Vid.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science Zhejiang University<br>
‚Ä¢ Dataset: RealCam-Vid, Samples: , Modality: High-resolution videos + metric-scale camera parameters + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Multi-person Physics-based Pose Estimation for Combat Sports</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.08175"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ¬¥Ecole de technologie sup ¬¥erieure, Montreal, Canada<br>
‚Ä¢ Dataset: Elite Boxing Footage, Samples: Over 20 minutes of video footage, Modality: Multi-view RGB videos<br>
‚Ä¢ Dataset: Supplementary Dataset, Samples: None, Modality: Multi-view RGB videos + MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Towards Unconstrained 2D Pose Estimation of the Human Spine</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.08110"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://saifkhichi96.github.io/research/spinepose/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: German Research Center for Artificial Intelligence (DFKI)<br>
‚Ä¢ Dataset: SpineTrack, Samples: 58766, Modality: RGB images + 2D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>FMNV: A Dataset of Media-Published News Videos for Fake News Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.07687"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DennisIW/FMNV"><img src="https://img.shields.io/github/stars/DennisIW/FMNV.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the paper<br>
‚Ä¢ Dataset: FMNV, Samples: 2393, Modality: RGB videos, audio, text titles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>S2R-HDR: A Large-Scale Rendered Dataset for HDR Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.07667"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://openimaginglab.github.io/S2R-HDR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory<br>
‚Ä¢ Dataset: S2R-HDR, Samples: 1000, Modality: HDR image sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.07375"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/IRMVLab/MMTwin"><img src="https://img.shields.io/github/stars/IRMVLab/MMTwin.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IRMV Lab, the Department of Automation, Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: Self-recorded HTP benchmark (unnamed in paper), Samples: 1200, Modality: egocentric RGB videos + point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.07083"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://kszpxxzmc.github.io/GenDoP/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: DataDoP, Samples: 29000, Modality: camera trajectories, RGBD images, textual captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Uncertainty-Aware Hybrid Machine Learning in Virtual Sensors for Vehicle Sideslip Angle Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.06105"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MB-Team-THI/UAHL-RevStED"><img src="https://img.shields.io/github/stars/MB-Team-THI/UAHL-RevStED.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CARISSMA Institute of Automated Driving, Technische Hochschule Ingolstadt, Germany<br>
‚Ä¢ Dataset: ReV-StED (Real-world Vehicle State Estimation Dataset), Samples: 900000, Modality: Vehicle dynamics sensor data from onboard sensors (CAN), GNSS/Inertial System, and optical speed sensor.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>UVG-VPC: Voxelized Point Cloud Dataset for Visual Volumetric Video-based Coding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.05888"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ultravideo.fi/UVG-VPC/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ultra Video Group, Tampere University, Tampere, Finland<br>
‚Ä¢ Dataset: UVG-VPC, Samples: 12, Modality: Voxelized point cloud sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Event-based Civil Infrastructure Visual Defect Detection: ev-CIVIL Dataset and Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.05679"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gwgknudayanga/evCIVIL"><img src="https://img.shields.io/github/stars/gwgknudayanga/evCIVIL.svg?style=social&label=Star"></a><br><a href="https://figshare.com/s/825aec2714266fa40d29"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Photonics Engineering, Technical University of Denmark, Denmark<br>
‚Ä¢ Dataset: ev-CIVIL, Samples: 680, Modality: DVS event streams + Grayscale frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>From Sparse Signal to Smooth Motion: Real-Time Motion Generation with Rolling Prediction Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.05265"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/barquerogerman/RPM"><img src="https://img.shields.io/github/stars/barquerogerman/RPM.svg?style=social&label=Star"></a><br><a href="https://barquerogerman.github.io/RPM/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta Reality Labs, Universitat de Barcelona, Computer Vision Center<br>
‚Ä¢ Dataset: GORP, Samples: >14 hours from 28 participants, Modality: VR tracking signals (head, hands/controllers) + MoCap ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.05046"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nju-cite-mocaphumanoid.github.io/MotionPRO/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Science and Engineering, Nanjing University, Nanjing, China<br>
‚Ä¢ Dataset: MotionPRO, Samples: 729, Modality: Pressure data, RGB videos, Optical MoCap data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Inter-event Interval Microscopy for Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.04924"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory for Multimedia Information Processing, Peking University, Beijing 100871, China<br>
‚Ä¢ Dataset: IEIMat, Samples: None, Modality: Event streams and fluorescence microscopy images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>OmniCam: Unified Multimodal Video Generation via Camera Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.02312"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: OmniTr, Samples: 1000, Modality: RGB videos, camera trajectories, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Beyond Static Scenes: Camera-controllable Background Generation for Human Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.02004"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yaomingshuai.github.io/Beyond-Static-Scenes.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Harbin Institute of Technology, Harbin, China<br>
‚Ä¢ Dataset: DynaScene, Samples: 200000, Modality: RGB videos + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Dynamic Initialization for LiDAR-inertial SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.01451"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lian-yue0515/D-LI-Init"><img src="https://img.shields.io/github/stars/lian-yue0515/D-LI-Init.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: school of Mechanical Engineering, Shandong University, Jinan 250061, China and Key Laboratory of High Efficiency and Clean Mechanical Manufacture, Ministry of Education, Jinan 250061, China<br>
‚Ä¢ Dataset: D-LI-Init test datasets, Samples: 49, Modality: LiDAR, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.00438"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LannnSun/a-real-life-flexiwear-bodynet-dataset"><img src="https://img.shields.io/github/stars/LannnSun/a-real-life-flexiwear-bodynet-dataset.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Key Laboratory of Navigation and Location-based Services, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, 200240<br>
‚Ä¢ Dataset: a real-life flexiwear-bodynet-dataset, Samples: 429, Modality: IMU data from smartphone, smartwatch, and headphones; Ground truth trajectories from VIO<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.24379"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sqwu.top/Any2Cap/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kuaishou Technology, National University of Singapore<br>
‚Ä¢ Dataset: Any2CapIns, Samples: 44644, Modality: RGB videos + human pose sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR) Challenge</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.24306"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/athaddius/STIRMetrics"><img src="https://img.shields.io/github/stars/athaddius/STIRMetrics.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/14803158"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Intuitive Surgical Inc.<br>
‚Ä¢ Dataset: STIR Challenge 2024 dataset (STIRC2024), Samples: 60, Modality: Stereo RGB videos + point trajectories from IR tattoos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.24026"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://humandreamer.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: GigaAI<br>
‚Ä¢ Dataset: MotionVid, Samples: 1270321, Modality: text + 2D pose sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Can Visuo-motor Policies Benefit from Random Exploration Data? A Case Study on Stacking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.23571"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cloudgripper.org"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: CloudGripper-Stack-750, Samples: 12400, Modality: RGB videos + proprioception states + actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>FRAME: Floor-aligned Representation for Avatar Motion from Egocentric Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.23094"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="vcai.mpi-inf.mpg.de/projects/FRAME"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: FRAME, Samples: 1600000, Modality: Stereo fisheye video, 6D head pose, Ground truth 3D joint positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>SocialGen: Modeling Multi-Human Social Interaction with Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.22906"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://socialgenx.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: SocialX, Samples: >40K, Modality: XH3D (SMPL-compatible pose data) + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.22397"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vadeli.github.io/GAITGen/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto, Computer Science Department, Vector Institute, KITE Research Institute, UHN<br>
‚Ä¢ Dataset: PD-GaM, Samples: 1701, Modality: 3D mesh dataset (SMPL parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.22394"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/Endo-TTAP-36E5"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Mechatronics and Engineering, Shenzhen University, Shenzhen, China<br>
‚Ä¢ Dataset: Endo-TAPC5, Samples: 40, Modality: RGB videos + point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.21860"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://maniptrans.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of General Artificial Intelligence, BIGAI<br>
‚Ä¢ Dataset: DEXMANIP NET, Samples: 3.3K episodes, Modality: simulated robotic manipulation trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.21268"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.lidarhumanmotion.net/climbingcap/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University; National Institute for Data Science in Health and Medicine, Xiamen University; Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University<br>
‚Ä¢ Dataset: AscendMotion, Samples: 412000, Modality: RGB, LiDAR, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>SpikeDerain: Unveiling Clear Videos from Rainy Sequences Using Color Spike Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.20315"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Wuhan University of Technology<br>
‚Ä¢ Dataset: RAIN100C, Samples: 100, Modality: RGB videos + color spike streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EGVD: Event-Guided Video Diffusion Model for Physically Realistic Large-Motion Frame Interpolation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.20268"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OpenImagingLab/EGVD"><img src="https://img.shields.io/github/stars/OpenImagingLab/EGVD.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University, Shanghai AI Laboratory<br>
‚Ä¢ Dataset: DJI 30fps, Samples: 191 sequences, Modality: RGB videos + simulated event data<br>
‚Ä¢ Dataset: Comprehensive Training Dataset (Prophesee, BS-ERGB, DJI 30fps, GOPRO 240fps), Samples: 400 sequences, Modality: RGB videos + real/simulated event data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EBS-EKF: Accurate and High Frequency Event-based Star Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.20101"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.kitware.com/nest-public/kwebsstartracking"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kitware<br>
‚Ä¢ Dataset: EBS-EKF Star Tracking Dataset, Samples: 14, Modality: Event streams + 3D rotational trajectories (quaternions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.19913"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/c7w/PartRM"><img src="https://img.shields.io/github/stars/c7w/PartRM.svg?style=social&label=Star"></a><br><a href="https://PartRM.c7w.tech/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: PartDrag-4D, Samples: 20548, Modality: Multi-view images, animated meshes, point clouds, and 2D drag vectors for articulated objects<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.19355"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ikodoh.github.io/ST-VLM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea University<br>
‚Ä¢ Dataset: STKit, Samples: 116000, Modality: RGB videos with kinematic instruction tuning data (QA pairs)<br>
‚Ä¢ Dataset: STKit-Bench, Samples: 1400, Modality: RGB videos with kinematic instruction tuning data (QA pairs)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>TrackRAD2025 challenge dataset: Real-time tumor tracking for MRI-guided radiotherapy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.19119"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LMUK-RADONC-PHYS-RES/trackrad2025/"><img src="https://img.shields.io/github/stars/trackrad2025/.svg?style=social&label=Star"></a><br><a href="https://trackrad2025.grand-challenge.org/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Radiation Oncology, Radiation Oncology Key Laboratory of Sichuan Province, Sichuan Clinical Research Center for Cancer, Sichuan Cancer Hospital & Institute, Sichuan Cancer Center, University of Electronic Science and Technology of China, Chengdu, China<br>
‚Ä¢ Dataset: TrackRAD2025, Samples: 585, Modality: 2D cine MRIs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Target-Aware Video Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18950"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://taeksuu.github.io/tavid/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Seoul National University<br>
‚Ä¢ Dataset: unnamed, Samples: 1290, Modality: RGB videos + segmentation masks + text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Predicting the Road Ahead: A Knowledge Graph based Foundation Model for Scene Understanding in Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18730"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/boschresearch/fm4su"><img src="https://img.shields.io/github/stars/boschresearch/fm4su.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Bosch Corporate Research, Renningen, Germany, University of Stuttgart, Stuttgart, Germany<br>
‚Ä¢ Dataset: BEV symbolic scene representation dataset from nuScenesKG, Samples: 30000, Modality: Symbolic Bird's-Eye-View (BEV) representation from Knowledge Graph<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18552"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://potentialming.github.io/projects/EvAnimate"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Sydney<br>
‚Ä¢ Dataset: EvTikTok, Samples: 350, Modality: simulated event streams<br>
‚Ä¢ Dataset: EvHumanMotion, Samples: 113, Modality: RGB videos + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Human-Object Interaction via Automatically Designed VLM-Guided Motion Policy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18349"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vlm-rmd.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Interplay, Samples: 1210, Modality: Interaction plans, 3D scene layouts, text instructions, and top-view images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>AGIR: Assessing 3D Gait Impairment with Reasoning based on LLMs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18141"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/w/AGIR-7BF7/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ICube laboratory, University of Strasbourg, CNRS, France<br>
‚Ä¢ Dataset: Enhanced Parkinson's Disease (PD) gait dataset, Samples: 883, Modality: 3D joint data + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>TransAnimate: Taming Layer Diffusion to Generate RGBA Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17934"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Clemson University<br>
‚Ä¢ Dataset: Animate Dataset, Samples: 3000, Modality: RGBA videos<br>
‚Ä¢ Dataset: Foreground Object Videos Dataset, Samples: 7000, Modality: RGBA videos<br>
‚Ä¢ Dataset: Synthesized Transparent Motion Videos, Samples: 20000, Modality: RGBA videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Event-Based Crossing Dataset (EBCD)</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17499"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/joeduman/Thresholded event-based-crossing-dataset"><img src="https://img.shields.io/github/stars/joeduman/Thresholded event-based-crossing-dataset.svg?style=social&label=Star"></a><br><a href="https://ieee-dataport.org/documents/event-based-crossing-dataset-ebcd"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UMBC<br>
‚Ä¢ Dataset: Event-Based Crossing Dataset (EBCD), Samples: 33, Modality: Event-based images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17358"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Oxford, Department of Computer Science<br>
‚Ä¢ Dataset: Synthetic Motion-Blurred Image Dataset (from ScanNet++v2), Samples: 121200, Modality: Synthesized motion-blurred RGB images, depth maps, optical flow fields, camera poses<br>
‚Ä¢ Dataset: Real-world Motion-Blurred Image Dataset, Samples: 10000, Modality: Real-world motion-blurred RGB images, ARKit poses, IMU measurements<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17132"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: FallingDetection-CeleX, Samples: 875, Modality: Event-camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>ColabSfM: Collaborative Structure-from-Motion by Point Cloud Registration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17093"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/EricssonResearch/ColabSfM"><img src="https://img.shields.io/github/stars/EricssonResearch/ColabSfM.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Link√∂ping University<br>
‚Ä¢ Dataset: ColabSfM SfM Registration Dataset, Samples: 22000, Modality: Pairs of 3D SfM point clouds with normals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Pedestrians and Robots: A Novel Dataset for Learning Distinct Social Navigation Forces</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.16481"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Humanoid Robots Lab, University of Bonn, Germany<br>
‚Ä¢ Dataset: robot-pedestrian influence (RPI) dataset, Samples: 18669, Modality: 2D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.16421"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://quanhaol.github.io/magicmotion-site/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: MagicData, Samples: 51000, Modality: RGB videos with text and trajectory (mask, bounding box) annotations<br>
‚Ä¢ Dataset: MagicBench, Samples: 600, Modality: RGB videos with trajectory (mask, bounding box) annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>PoseTraj: Pose-Aware Trajectory Control in Video Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.16068"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://robingg1.github.io/Pose-Traj/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Edinburgh<br>
‚Ä¢ Dataset: PoseTraj-10K, Samples: 10000, Modality: RGB videos + trajectories + 3D bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.15835"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vulab-ai.github.io/BARD-GS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Case Western Reserve University<br>
‚Ä¢ Dataset: Real-world Captured Blurry Dataset, Samples: 12, Modality: Paired blurry (24 FPS) and sharp (240 FPS) RGB videos of dynamic scenes from synchronized cameras<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Controlling Avatar Diffusion with Learnable Gaussian Embedding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.15809"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ustc3dv.github.io/Learn2Control/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: Synthetic Head Dataset, Samples: 10000, Modality: Synthetic images + FLAME parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14935"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://favor-bench.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: FAVOR-Bench, Samples: 1776, Modality: RGB videos<br>
‚Ä¢ Dataset: FAVOR-Train, Samples: 17152, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14919"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: Unified and Extended Motion Dataset, Samples: 48251, Modality: MoCap joints + Text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Matching Skeleton-based Activity Representations with Heterogeneous Signals for HAR</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14547"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Shuheng-Li/SKELAR"><img src="https://img.shields.io/github/stars/Shuheng-Li/SKELAR.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, San Diego<br>
‚Ä¢ Dataset: MASD (Multimodal Activity Sensing Dataset), Samples: 540, Modality: IMU, WiFi, skeleton<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14247"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/NSN-Hello/GeoFlow-SLAM"><img src="https://img.shields.io/github/stars/NSN-Hello/GeoFlow-SLAM.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Horizon Robotics<br>
‚Ä¢ Dataset: Go2 D435i dataset, Samples: 4, Modality: RGB-D, IMU, LiDAR, Legged Odometry<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14229"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ha-vln-project.vercel.app/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Washington<br>
‚Ä¢ Dataset: HAPS 2.0, Samples: 486, Modality: SMPL mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>8-Calves Image dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.13777"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/tonyFang04/8-calves"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Bristol<br>
‚Ä¢ Dataset: 8-Calves dataset, Samples: 1, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Multi-Platform Teach-and-Repeat Navigation by Visual Place Recognition Based on Deep-Learned Local Features</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.13090"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://imr.ciirc.cvut.cz/Datasets/TaR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Czech Institute of Informatics, Robotics and Cybernetics, Czech technical University in Prague; Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague<br>
‚Ä¢ Dataset: TaR, Samples: 3, Modality: RGB images + pose transformations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Beyond Role-Based Surgical Domain Modeling: Generalizable Re-Identification in the Operating Room</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.13028"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wngTn/orreid"><img src="https://img.shields.io/github/stars/wngTn/orreid.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Chair for Computer Aided Medical Procedures, Technical University of Munich, Boltzmannstra√üe 3, 85748, Garching, Germany<br>
‚Ä¢ Dataset: ORReID13, Samples: 6358, Modality: 3D point clouds + multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>GIFT: Generated Indoor video frames for Texture-less point tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12944"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/GIFT-6D02/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southern University of Science and Technology<br>
‚Ä¢ Dataset: GIFT, Samples: 1800, Modality: RGB videos + point trajectories + optical flow + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>AUTV: Creating Underwater Video Datasets with Pixel-wise Annotations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12828"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: UTV, Samples: 2000, Modality: RGB videos + fine-grained text annotations (including motion/behavior)<br>
‚Ä¢ Dataset: SUTV, Samples: 10000, Modality: RGB videos + pixel-wise segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Stereo Event-based, 6-DOF Pose Tracking for Uncooperative Spacecraft</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12732"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Zibin6/SE6PT"><img src="https://img.shields.io/github/stars/Zibin6/SE6PT.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Aerospace Science and Engineering, National University of Defense Technology<br>
‚Ä¢ Dataset: Stereo Event-based Uncooperative Spacecraft Motion Dataset, Samples: 17, Modality: Stereo event streams + 6-DOF pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12419"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/3190105222/EgoEv_Gesture"><img src="https://img.shields.io/github/stars/3190105222/EgoEv_Gesture.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang University, China<br>
‚Ä¢ Dataset: EgoEvGesture, Samples: 5419, Modality: event camera streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>M2UD: A Multi-model, Multi-scenario, Uneven-terrain Dataset for Ground Robot with Localization and Mapping Evaluation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12387"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yaepiii.github.io/M2UD/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Robotics at Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China<br>
‚Ä¢ Dataset: M2UD, Samples: 58, Modality: LiDAR, RGB-D Camera, IMU, GNSS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Bring Your Rear Cameras for Egocentric 3D Human Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.11652"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/EgoRear/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, SIC<br>
‚Ä¢ Dataset: Ego4View-Syn, Samples: 8372, Modality: synthetic RGB videos (fisheye) + SMPL parameters<br>
‚Ä¢ Dataset: Ego4View-RW, Samples: 478, Modality: real-world RGB videos (fisheye) + MoCap joints + SMPL parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EMoTive: Event-guided Trajectory Modeling for 3D Motion Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.11371"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: USTC<br>
‚Ä¢ Dataset: CarlaEvent3D, Samples: 22125, Modality: Events + Images + 3D Motion Labels (Optical Flow, Motion in Depth, Scene Flow)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Enhancing Hand Palm Motion Gesture Recognition by Eliminating Reference Frame Bias via Frame-Invariant Similarity Measures</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.11352"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.15020057"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering and Flanders Make at KU Leuven<br>
‚Ä¢ Dataset: Hand Palm Motion (HPM) dataset, Samples: 420, Modality: Motion capture trajectories (3D position + quaternion)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>6D Object Pose Tracking in Internet Videos for Robotic Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.10307"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague<br>
‚Ä¢ Dataset: New dataset of instructional videos, Samples: 32, Modality: RGB videos + 6D object pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>RMG: Real-Time Expressive Motion Generation with Self-collision Avoidance for 6-DOF Companion Robotic Arms</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.09959"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: robotic arm expressive motion dataset, Samples: over 10,000, Modality: Robotic arm joint trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Unified Dense Prediction of Video Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.09344"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Virginia<br>
‚Ä¢ Dataset: Panda-Dense, Samples: 300000, Modality: RGB videos + entity segmentation + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>SwapAnyone: Consistent and Realistic Video Synthesis for Swapping Any Person into Any Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.09154"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/PKU-YuanGroup/SwapAnyone"><img src="https://img.shields.io/github/stars/PKU-YuanGroup/SwapAnyone.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: HumanAction-32K, Samples: 32000, Modality: RGB videos + pose keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Ev-Layout: A Large-scale Event-based Multi-modal Dataset for Indoor Layout Estimation and Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.08370"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software, Shandong University, China<br>
‚Ä¢ Dataset: Ev-Layout, Samples: 2500, Modality: RGB images, event streams, IMU data, illuminance sensor data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HERO: Human Reaction Generation from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.08270"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JackYu6/HERO"><img src="https://img.shields.io/github/stars/JackYu6/HERO.svg?style=social&label=Star"></a><br><a href="https://jackyu6.github.io/HERO"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: ViMo, Samples: 3500, Modality: RGB videos + 3D human motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Depth-Assisted Network for Indiscernible Marine Object Counting with Adaptive Motion-Differentiated Feature Encoding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.08152"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OUCVisionGroup/VIMOC-Net"><img src="https://img.shields.io/github/stars/OUCVisionGroup/VIMOC-Net.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Engineering, Ocean University of China<br>
‚Ä¢ Dataset: VIMOC Dataset, Samples: 50, Modality: RGB videos + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>AG-VPReID: A Challenging Large-Scale Benchmark for Aerial-Ground Video-based Person Re-Identification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.08121"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Vi-Sion/AG-VPReID-Net"><img src="https://img.shields.io/github/stars/Vi-Sion/AG-VPReID-Net.svg?style=social&label=Star"></a><br><a href="https://github.com/Vi-Sion/AG-VPReID-Net"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical Engineering and Robotics, Queensland University of Technology<br>
‚Ä¢ Dataset: AG-VPReID, Samples: 32321, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HumanMM: Global Human Motion Recovery from Multi-shot Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07597"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zhangyuhong01.github.io/HumanMM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University, IDEA Research<br>
‚Ä¢ Dataset: ms-Motion, Samples: 600, Modality: RGB videos + 3D human motion (SMPL) + camera parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>AthletePose3D: A Benchmark Dataset for 3D Human Pose Estimation and Kinematic Validation in Athletic Movements</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07499"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/calvinyeungck/AthletePose3D"><img src="https://img.shields.io/github/stars/calvinyeungck/AthletePose3D.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Graduate School of Informatics, Nagoya University, Nagoya, Japan<br>
‚Ä¢ Dataset: AthletePose3D, Samples: 165000, Modality: RGB videos + 3D poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>PersonaBooth: Personalized Text-to-Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07390"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://boeun-kim.github.io/page-PersonaBooth"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Birmingham, Korea Electronics Technology Institute, Dankook University<br>
‚Ä¢ Dataset: PerMo, Samples: 6,610 clips, Modality: MoCap (optical markers, skeleton, SMPL-H mesh), text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs and Chain-of-Thought Prompting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07234"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau<br>
‚Ä¢ Dataset: Highway-Text, Samples: 6606, Modality: Text descriptions of traffic scenarios<br>
‚Ä¢ Dataset: Urban-Text, Samples: 5431, Modality: Text descriptions of traffic scenarios<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>YOLOMG: Vision-based Drone-to-Drone Detection with Appearance and Pixel-Level Motion Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07115"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Irisky123/YOLOMG"><img src="https://img.shields.io/github/stars/Irisky123/YOLOMG.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Artificial Intelligence, Westlake University, Hangzhou, China<br>
‚Ä¢ Dataset: ARD100, Samples: 100, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07020"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University at Buffalo, SUNY<br>
‚Ä¢ Dataset: DriveLM-Deficit, Samples: 53895, Modality: RGB video clips<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HybridReg: Robust 3D Point Cloud Registration with Hybrid Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07019"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hxwork/HybridReg"><img src="https://img.shields.io/github/stars/hxwork/HybridReg.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: HybridMatch, Samples: 50600, Modality: Point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Motion Anything: Any to Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.06955"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://steve-zeyu-zhang.github.io/MotionAnything"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ANU<br>
‚Ä¢ Dataset: Text-Music-Dance (TMD), Samples: 2153, Modality: text, music, and dance motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Sign Language Translation using Frame and Event Stream: Benchmark Dataset and Algorithms</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.06484"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/OpenESL"><img src="https://img.shields.io/github/stars/Event-AHU/OpenESL.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: VECSL, Samples: 15676, Modality: RGB frames + Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.06053"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dropletx.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IEIT System Co., Ltd.<br>
‚Ä¢ Dataset: DropletVideo-10M, Samples: 10000000, Modality: RGB videos + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>A Helping (Human) Hand in Kinematic Structure Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.05301"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics and Biology Laboratory, Technische Universit√§t Berlin; Science of Intelligence, Research Cluster of Excellence, Berlin<br>
‚Ä¢ Dataset: unnamed, Samples: 30, Modality: RGB-D videos + MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.05231"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="Search for 'Kaiwu' on ScienceDB"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the text<br>
‚Ä¢ Dataset: Kaiwu, Samples: 11664, Modality: Motion Capture (3D skeleton ground truth), Multi-view RGB-D videos, Audio, IMU, EMG, Eye Gaze (first-person video), Hand Pose (data glove), Tactile/Force (data glove)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Combined Physics and Event Camera Simulator for Slip Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tub-rip/event-slip"><img src="https://img.shields.io/github/stars/tub-rip/event-slip.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit ¬®at Berlin, and Robotics Institute Germany<br>
‚Ä¢ Dataset: Simple Set, Samples: 192, Modality: Event camera data, RGB frames, Object/Gripper orientations (quaternions)<br>
‚Ä¢ Dataset: Complex Set, Samples: 1200, Modality: Event camera data, RGB frames, Object/Gripper orientations (quaternions)<br>
‚Ä¢ Dataset: Real Set, Samples: 5, Modality: Event camera data, RGB frames, Robot kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>What Are You Doing? A Closer Look at Controllable Human Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04666"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/google-deepmind/wyd-benchmark"><img src="https://img.shields.io/github/stars/google-deepmind/wyd-benchmark.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google DeepMind<br>
‚Ä¢ Dataset: What Are You Doing? (WYD), Samples: 1544, Modality: RGB videos + captions + video segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>3HANDS Dataset: Learning from Humans for Generating Naturalistic Handovers with Supernumerary Robotic Limbs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04635"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hci.cs.uni-saarland.de/projects/3hands/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Saarland University<br>
‚Ä¢ Dataset: 3HANDS, Samples: 946, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Omnidirectional Multi-Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04565"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xifen523/OmniTrack"><img src="https://img.shields.io/github/stars/xifen523/OmniTrack.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hunan University<br>
‚Ä¢ Dataset: QuadTrack, Samples: 32, Modality: panoramic image sequences + 2D bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04257"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Seoul National University<br>
‚Ä¢ Dataset: Truebones Zoo dataset (annotated), Samples: 1000+, Modality: skeletal motion + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Supervised Visual Docking Network for Unmanned Surface Vehicles Using Auto-labeling in Real-world Water Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.03282"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/usv-docking/home"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information Science and Engineering, Hebei University of Science and Technology, 26 Yuxiang Street, Yuhua District, Shijiazhuang, Heibei Province, 050018, P.R. China<br>
‚Ä¢ Dataset: USV Visual Docking Dataset, Samples: 2000, Modality: Fisheye images + relative pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Monocular Person Localization under Camera Ego-motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.02916"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MEDLAR-T-Rex/rpf_quadruped"><img src="https://img.shields.io/github/stars/MEDLAR-T-Rex/rpf_quadruped.svg?style=social&label=Star"></a><br><a href="https://medlartea.github.io/rpf-quadruped/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Key Laboratory of Robotics and Computer Vision, Southern University of Science and Technology (SUSTech), and the Department of Electronic and Electrical Engineering, SUSTech.<br>
‚Ä¢ Dataset: Our Dataset, Samples: None, Modality: RGB videos (pin-hole, fisheye, equirectangular), UWB distance measurements, motion capture poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.02572"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://racevla.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Intelligent Space Robotics Laboratory, Skolkovo Institute of Science and Technology<br>
‚Ä¢ Dataset: RaceVLA dataset, Samples: 200, Modality: Vicon MoCap (position, velocity, yaw), RGB images, natural language instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>BdSLW401: Transformer-Based Word-Level Bangla Sign Language Recognition Using Relative Quantization Encoding (RQE)</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.02360"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the paper<br>
‚Ä¢ Dataset: BdSLW401, Samples: 102176, Modality: Pose landmarks (MediaPipe)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>One-Step Event-Driven High-Speed Autofocus</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.01214"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang University, China<br>
‚Ä¢ Dataset: PSF-based Focus Event Synthetic Dataset, Samples: 84, Modality: Synthetic focus event stacks with simulated motion<br>
‚Ä¢ Dataset: DAVIS346 Autofocus Dataset, Samples: 28, Modality: Time-synchronized event streams and grayscale images<br>
‚Ä¢ Dataset: Prophesee EVK4 Autofocus Dataset, Samples: 28, Modality: Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HiMo: High-Speed Objects Motion Compensation in Point Clouds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.00803"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/KTH-RPL/HiMo"><img src="https://img.shields.io/github/stars/KTH-RPL/HiMo.svg?style=social&label=Star"></a><br><a href="https://kin-zhang.github.io/HiMo"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Division of Robotics, Perception, and Learning (RPL), KTH Royal Institute of Technology; Autonomous Transport Solutions Lab, Scania Group<br>
‚Ä¢ Dataset: Scania, Samples: 500 sequences, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.00495"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://xuanchenli.github.io/TexTalk/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: TexTalk4D, Samples: 100, Modality: audio-synced 3D mesh sequences with 8K dynamic textures<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>High Dynamic Range Video Compression: A Large-Scale Benchmark Dataset and A Learned Bit-depth Scalable Compression Algorithm</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.00410"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sdkinda/HDR-Learned-Video-Coding"><img src="https://img.shields.io/github/stars/sdkinda/HDR-Learned-Video-Coding.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shangha University<br>
‚Ä¢ Dataset: HDRVD2K, Samples: 2200, Modality: HDR videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.00389"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Keio University<br>
‚Ä¢ Dataset: AMPL (Acoustic Music-based PoseLearning dataset), Samples: None, Modality: Mocap joints + Acoustic signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Towards long-term player tracking with graph hierarchies and domain-specific features</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.21242"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mkoshkina/sports-SUSHI"><img src="https://img.shields.io/github/stars/mkoshkina/sports-SUSHI.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: York University<br>
‚Ä¢ Dataset: Hockey Tracking Dataset, Samples: 20, Modality: RGB videos with MOT annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>egoPPG: Heart Rate Estimation from Eye-Tracking Cameras in Egocentric Systems to Benefit Downstream Vision Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.20879"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, ETH Zurich<br>
‚Ä¢ Dataset: egoPPG-DB, Samples: 150, Modality: Eye-tracking videos + POV RGB videos + IMU + PPG + ECG<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>EyEar: Learning Audio Synchronized Human Gaze Trajectory Based on Physics-Informed Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.20858"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XiaochuanLiu-ruc/EyEar"><img src="https://img.shields.io/github/stars/XiaochuanLiu-ruc/EyEar.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Gaoling School of Artifcial Intelligence, Renmin University of China, Beijing, China<br>
‚Ä¢ Dataset: EyEar-20k, Samples: 20000, Modality: Gaze trajectories + images + synchronized audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.20824"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Texas at Austin<br>
‚Ä¢ Dataset: Handheld Burst Motion Dataset, Samples: 102, Modality: Homography matrices from multi-frame captures<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>FuseGrasp: Radar-Camera Fusion for Robotic Grasping of Transparent Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.20037"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Information Engineering, The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: self-built RGB-D-Radar transparent object dataset, Samples: 600, Modality: RGB images + depth images + mmWave radar images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>C-Drag: Chain-of-Thought Driven Motion Controller for Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.19868"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WesLee88524/C-Drag-Official-Repo"><img src="https://img.shields.io/github/stars/WesLee88524/C-Drag-Official-Repo.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northwestern Polytechnical University<br>
‚Ä¢ Dataset: VOI, Samples: 72, Modality: RGB videos with bounding box and trajectory annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>TransVDM: Motion-Constrained Video Diffusion Model for Transparent Video Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.19454"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Alibaba Group<br>
‚Ä¢ Dataset: None, Samples: 10000, Modality: RGB-alpha video clips + bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.18373"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://siplab.org/projects/EgoSim"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich<br>
‚Ä¢ Dataset: MultiEgoView, Samples: 119.4 hours (synthetic) + 5 hours (real), Modality: RGB videos (from 6 body-worn cameras), ground-truth 3D body poses, activity annotations, simulated IMU data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Impact of Object Weight in Handovers: Inspiring Robotic Grip Release and Motion from Human Handovers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.17834"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/paragkhanna1/RPL Khanna Human Handover Datasets"><img src="https://img.shields.io/github/stars/paragkhanna1/RPL Khanna Human Handover Datasets.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Division of Robotics, Perception and Learning (RPL), EECS, KTH Royal Institute of Technology, Sweden<br>
‚Ä¢ Dataset: Handovers@RPL-2.0, Samples: 3235, Modality: MoCap joints, Forces<br>
‚Ä¢ Dataset: YCB-Handovers, Samples: 2771, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>V-HOP: Visuo-Haptic 6D Object Pose Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.17434"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lhy.xyz/projects/v-hop/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: V-HOP Multi-embodied Dataset, Samples: 1550000, Modality: RGB-D videos, robot kinematics (joint positions), tactile sensor data, 6D object pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>A dataset of high-resolution plantar pressures for gait analysis across varying footwear and walking speeds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.17244"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UNB-StepUP/StepUP-P150"><img src="https://img.shields.io/github/stars/UNB-StepUP/StepUP-P150.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.20383/103.01285"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of New Brunswick, Institute of Biomedical Engineering<br>
‚Ä¢ Dataset: UNB StepUP-P150, Samples: over 200,000 footsteps, Modality: Plantar pressure<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>DeProPose: Deficiency-Proof 3D Human Pose Estimation via Adaptive Multi-View Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.16419"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WUJINHUAN/DeProPose"><img src="https://img.shields.io/github/stars/WUJINHUAN/DeProPose.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence, Xidian University, China<br>
‚Ä¢ Dataset: Deficiency-Aware 3D Pose Estimation (DA-3DPE) dataset, Samples: 575689, Modality: multi-view RGB videos + 3D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.14917"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not specified in text<br>
‚Ä¢ Dataset: VQA driving instruction dataset, Samples: None, Modality: multi-view scene videos, BEV map images, QA annotations, vehicle motion trajectories, and control signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.14795"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Milab, Westlake University<br>
‚Ä¢ Dataset: Humanoid-VLA motion-language interleaved dataset, Samples: 929000 clips, Modality: motion sequences + text annotations<br>
‚Ä¢ Dataset: Humanoid-S, Samples: 4646 video clips, Modality: human pose + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Exploiting Deblurring Networks for Radiance Fields</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.14454"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KT<br>
‚Ä¢ Dataset: BlurRF-Synth, Samples: 150, Modality: Synthesized RGB images (motion & defocus blurred)<br>
‚Ä¢ Dataset: BlurRF-Real, Samples: 5, Modality: Real-world RGB images (motion blurred)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.14004"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen University<br>
‚Ä¢ Dataset: Inter3D, Samples: 4, Modality: RGB images + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.13859"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: MSVCOD, Samples: 162, Modality: RGB videos with pixel-level annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Event-Based Video Frame Interpolation With Cross-Modal Asymmetric Bidirectional Motion Fields</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.13716"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/intelpro/CBMNet"><img src="https://img.shields.io/github/stars/intelpro/CBMNet.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea Advanced Institute of Science and Technology<br>
‚Ä¢ Dataset: ERF-X170FPS, Samples: 140, Modality: RGB videos + Event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>MoVer: Motion Verification for Motion Graphics Animations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.13372"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mover-dsl.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University, USA<br>
‚Ä¢ Dataset: MoVer Test Dataset, Samples: 5600, Modality: Text prompts paired with ground truth MoVer verification programs for 2D SVG motion graphics animations.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>iMOVE: Instance-Motion-Aware Video Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.11594"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kuaishou Technology, Zhejiang University<br>
‚Ä¢ Dataset: iMOVE-IT, Samples: 114000, Modality: RGB videos with bounding box motion trajectories and dynamic captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>AdaManip: Adaptive Articulated Object Manipulation Environments and Policy Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.11124"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://adamanip.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center on Frontiers of Computing Studies, School of Computer Science, Peking University<br>
‚Ä¢ Dataset: AdaManip Adaptive Demonstrations, Samples: 5540 simulated sequences, Modality: Robot end-effector pose trajectories, 3D point clouds, robot proprioception states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>E-3DGS: Event-Based Novel View Rendering of Large-Scale Scenes Using 3D Gaussian Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.10827"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/E3DGS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Saarland University, MPI for Informatics, SIC<br>
‚Ä¢ Dataset: E-3DGS-Real, Samples: None, Modality: color event stream, RGB images, camera poses<br>
‚Ä¢ Dataset: E-3DGS-Synthetic, Samples: 3, Modality: simulated color event stream, rendered RGB frames, camera poses<br>
‚Ä¢ Dataset: E-3DGS-Synthetic-Hard, Samples: 3, Modality: simulated color event stream, rendered RGB frames, noisy camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.09533"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University of Science and Technology<br>
‚Ä¢ Dataset: TalkingFace-Wild, Samples: 31300, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>A Deep Inverse-Mapping Model for a Flapping Robotic Wing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.09378"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Hadar933/AdaptiveSpectrumLayer"><img src="https://img.shields.io/github/stars/Hadar933/AdaptiveSpectrumLayer.svg?style=social&label=Star"></a><br><a href="https://www.beatus-lab.org"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Engineering, The Institute of Life Sciences, Center for Bioengineering, The Hebrew University of Jerusalem<br>
‚Ä¢ Dataset: None, Samples: 153, Modality: 3D wing kinematics (Euler angles) from stereo high-speed cameras, synchronized with aerodynamic force sensor data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>EventSTR: A Benchmark Dataset and Baselines for Event Stream based Scene Text Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.09020"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/EventSTR"><img src="https://img.shields.io/github/stars/Event-AHU/EventSTR.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University<br>
‚Ä¢ Dataset: EventSTR, Samples: 9928, Modality: Event stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Measuring Anxiety Levels with Head Motion Patterns in Severe Depression Population</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.08813"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Univ. Lille, CNRS, Centrale Lille, Institut Mines-T ¬¥el¬¥ecom, UMR 9189 CRIStAL, F-59000 Lille, France; Univ. Lille, Inserm, CHU Lille, U1172 - LilNCog - Lille Neuroscience & Cognition, F-59000 Lille, France<br>
‚Ä¢ Dataset: CALYPSO Depression Dataset, Samples: 32, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.08639"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cinemaster-dev.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dalian University of Technology<br>
‚Ä¢ Dataset: Unnamed 3D box and camera pose video dataset, Samples: 156000, Modality: RGB videos + 3D bounding boxes + 3D camera trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.07869"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://eventego3d.mpi-inf.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Computing and Artificial Intelligence, Max Planck Institute for Informatics, SIC, Saarbr√ºcken, Germany; Augmented Vision, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany.<br>
‚Ä¢ Dataset: EE3D-R, Samples: 12 sequences, ~4.64e5 poses, Modality: egocentric event streams, 3D body joints, allocentric RGB streams, SMPL body parameters<br>
‚Ä¢ Dataset: EE3D-W, Samples: 9 sequences, ~4.18e5 poses, Modality: egocentric event streams, 3D body joints, allocentric RGB streams, SMPL body parameters<br>
‚Ä¢ Dataset: EE3D-S, Samples: 946 motion sequences, ~6.34e6 3D human poses, Modality: synthetic egocentric event streams, 3D body joints, SMPL body models<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.07531"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: Camera Motion Control Dataset, Samples: 62000, Modality: RGB videos + camera trajectories<br>
‚Ä¢ Dataset: Object Motion Control Dataset, Samples: 60000, Modality: RGB videos + dense/sparse object trajectories + optical flow<br>
‚Ä¢ Dataset: VideoLightingDirection (VLD) Dataset, Samples: 57600, Modality: Synthetic RGB videos + camera trajectories + lighting direction<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>CT-UIO: Continuous-Time UWB-Inertial-Odometer Localization Using Non-Uniform B-spline with Fewer Anchors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.06287"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JasonSun623/CT-UIO"><img src="https://img.shields.io/github/stars/JasonSun623/CT-UIO.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Engineering Research Center of Robot Visual Perception and Control Technology, Hunan University, Changsha 410012, China<br>
‚Ä¢ Dataset: Corridor Dataset, Samples: 6, Modality: UWB, IMU, Odometer, LIDAR<br>
‚Ä¢ Dataset: Exhibition Hall Dataset, Samples: 6, Modality: UWB, IMU, Odometer, LIDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.05979"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology, China<br>
‚Ä¢ Dataset: Open-VFX, Samples: 675, Modality: RGB videos + text prompts + instance segmentation masks + start/end timestamps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.04847"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://agnjason.github.io/HumanDiT-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University, ByteDance<br>
‚Ä¢ Dataset: Unnamed HumanDiT dataset, Samples: 4500000, Modality: RGB videos + body pose sequences + background keypoint sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>High-Speed Dynamic 3D Imaging with Sensor Fusion Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.04630"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of North Carolina, Chapel Hill, USA<br>
‚Ä¢ Dataset: Synthetic High-Speed Dynamic Scenes, Samples: 3, Modality: RGB videos + Depth maps + Events<br>
‚Ä¢ Dataset: Real-World High-Speed Dynamic Scenes, Samples: 3, Modality: RGB videos + Depth maps + Events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>HD-EPIC: A Highly-Detailed Egocentric Video Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.04144"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://hd-epic.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Uni. of Bristol<br>
‚Ä¢ Dataset: HD-EPIC, Samples: 19900, Modality: Egocentric RGB videos, 6DoF camera trajectories, 3D digital twins, 3D object/hand masks, gaze, audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.03639"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://snap-research.github.io/PointVidGen/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Los Angeles<br>
‚Ä¢ Dataset: PointVid, Samples: 70000, Modality: RGB videos + 3D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Swarm Characteristic Classification using Robust Neural Networks with Optimized Controllable Inputs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.03619"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DWPeltier3/Swarm-NN-TSC"><img src="https://img.shields.io/github/stars/DWPeltier3/Swarm-NN-TSC.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Naval Postgraduate School<br>
‚Ä¢ Dataset: Combined ND, Samples: 72000, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
‚Ä¢ Dataset: Combined DM, Samples: 24000, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
‚Ä¢ Dataset: Combined Noise, Samples: 244800, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
‚Ä¢ Dataset: Combined DM+, Samples: 200000, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
‚Ä¢ Dataset: Combined ND & DM, Samples: 240000, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>SKI Models: Skeleton Induced Vision-Language Embeddings for Understanding Activities of Daily Living</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.03459"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/thearkaprava/SKI-Models"><img src="https://img.shields.io/github/stars/thearkaprava/SKI-Models.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of North Carolina at Charlotte<br>
‚Ä¢ Dataset: NTU120 video-instruction pairs, Samples: 100K question-answer pairs, Modality: RGB videos + text instruction pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal Structures from Multi-view Joint Cloud</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.02936"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China<br>
‚Ä¢ Dataset: BUMocap-X, Samples: 1 sequence (120 seconds), Modality: MoCap joints from multi-view video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Event-aided Semantic Scene Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.02334"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Pandapan01/EvSSC"><img src="https://img.shields.io/github/stars/Pandapan01/EvSSC.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang University<br>
‚Ä¢ Dataset: DSEC-SSC, Samples: 12 sequences (3,488 frames), Modality: Event camera data + RGB images + LiDAR<br>
‚Ä¢ Dataset: SemanticKITTI-E, Samples: 4649 frames (3834 train, 815 val), Modality: RGB images + simulated event data + LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Every Image Listens, Every Image Dances: Music-Driven Image Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.18801"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stony Brook University<br>
‚Ä¢ Dataset: MuseDance, Samples: 2904, Modality: RGB videos + audio + text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>UDC-VIT: A Real-World Video Dataset for Under-Display Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.18545"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mcrl/UDC-VIT"><img src="https://img.shields.io/github/stars/mcrl/UDC-VIT.svg?style=social&label=Star"></a><br><a href="https://kyusuahn.github.io/UDC-VIT.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Data Science, Seoul National University, Seoul, Republic of Korea; Research Center, Samsung Display Co., Ltd., Yongin, Republic of Korea<br>
‚Ä¢ Dataset: UDC-VIT, Samples: 647, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via Multimodal Visual Feature Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.18124"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="remote-bmxs.netlify.app"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Academy for Engineering & Technology, Fudan University<br>
‚Ä¢ Dataset: NEPose, Samples: 50 videos, Modality: Binocular 4K endoscopic videos + pose trajectories from optical tracking system<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>VidSole: A Multimodal Dataset for Joint Kinetics Quantification and Disease Detection with Deep Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.17890"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: VidSole, Samples: 2632, Modality: instrumented insole forces and moments, 2-viewpoint RGB video, 3D motion capture, force plate data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Extending Information Bottleneck Attribution to Video Sequences</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.16889"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/anonrep/IBA-for-Video-Sequences"><img src="https://img.shields.io/github/stars/anonrep/IBA-for-Video-Sequences.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit ¬®at Berlin<br>
‚Ä¢ Dataset: deepfake detection dataset, Samples: 378, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.14319"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Xiaohao-Xu/SLAM-under-Perturbation"><img src="https://img.shields.io/github/stars/Xiaohao-Xu/SLAM-under-Perturbation.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Michigan, Ann Arbor<br>
‚Ä¢ Dataset: Robust-Ego3D, Samples: 1000, Modality: RGB-D videos + pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Improving Video Generation with Human Feedback</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.13918"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gongyeliu.github.io/videoalign"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: Human-labeled video generation preference dataset, Samples: 108000, Modality: RGB videos + human preference annotations<br>
‚Ä¢ Dataset: VideoGen-RewardBench, Samples: 26500, Modality: RGB videos + human preference annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Multimodal Sensor Dataset for Monitoring Older Adults Post Lower-Limb Fractures in Community Settings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.13888"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/abedidev/maison-llf"><img src="https://img.shields.io/github/stars/abedidev/maison-llf.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/14597613"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KITE Research Institute, Toronto Rehabilitation Institute, University Health Network, Toronto, Canada<br>
‚Ä¢ Dataset: MAISON-LLF, Samples: 560, Modality: Accelerometer, GPS, step count, binary motion events, heart rate, sleep data, clinical questionnaires<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.13335"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan 430074, China; S-Lab for Advanced Intelligence, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: ZJU-MoCap-Blur, Samples: 6 sequences, Modality: Synthesized motion-blurred RGB videos + SMPL poses + foreground masks<br>
‚Ä¢ Dataset: Real-Human-Blur, Samples: None, Modality: Monocular motion-blurred RGB videos + SMPL poses + foreground masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Machine Learning Modeling for Multi-order Human Visual Motion Processing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.12810"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/anoymized/multi-order-motion-model"><img src="https://img.shields.io/github/stars/anoymized/multi-order-motion-model.svg?style=social&label=Star"></a><br><a href="https://anoymized.github.io/motion-model-website/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Graduate School of Informatics, Kyoto University, Kyoto, 606-8501, Japan.<br>
‚Ä¢ Dataset: Material-Controlled Motion Dataset, Samples: None, Modality: RGB videos + optical flow<br>
‚Ä¢ Dataset: Second-order Motion Benchmark, Samples: 40 scenes, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Int2Planner: An Intention-based Multi-modal Motion Planner for Integrated Prediction and Planning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.12799"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cxlz/Int2Planner"><img src="https://img.shields.io/github/stars/cxlz/Int2Planner.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence & Department of CSE & MoE Lab of AI, Shanghai Jiao Tong University; COWAROBOT Co. Ltd.<br>
‚Ä¢ Dataset: private dataset, Samples: 680964, Modality: trajectory data, localization data, route path information<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Interaction Dataset of Autonomous Vehicles with Traffic Lights and Signs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.12536"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://uwmadison.box.com/s/dbysk2jl15w0j56hd02rfaosuhvx3zu0"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Civil and Environmental Engineering, University of Wisconsin-Madison, Madison, WI 53706, United States<br>
‚Ä¢ Dataset: Interaction Dataset of Autonomous Vehicles with Traffic Lights and Signs, Samples: 82132, Modality: Vehicle trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>TOFFE -- Temporally-binned Object Flow from Events for High-speed and Energy-Efficient Object Detection and Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.12482"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Elmore Family School of Electrical and Computer Engineering, Purdue University<br>
‚Ä¢ Dataset: TOFFE dataset, Samples: None, Modality: Synthetic data from Gazebo simulator including RGB frames, depth, event camera data, 6-DoF object pose, and object velocity.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>TalkingEyes: Pluralistic Speech-Driven 3D Eye Gaze Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.09921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lkjkjoiuiu.github.io/TalkingEyes Home/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Informatics, Xiamen University, China<br>
‚Ä¢ Dataset: TalKingEyesDataset (TKED), Samples: 5982, Modality: Audio + 3D mesh motion sequences (FLAME parameters for eye gaze, head, and face)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>SMPLest-X: Ultimate Scaling for Expressive Human Pose and Shape Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.09782"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wqyin/SMPLest-X"><img src="https://img.shields.io/github/stars/wqyin/SMPLest-X.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Tokyo, SenseTime Research<br>
‚Ä¢ Dataset: SynHand, Samples: 462800, Modality: Synthetic RGB images + SMPL-X annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Robust Single Object Tracking in LiDAR Point Clouds under Adverse Weather Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.07133"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Mathematical Sciences, Dalian University of Technology, China<br>
‚Ä¢ Dataset: KITTI-A, Samples: 2730, Modality: LiDAR point clouds<br>
‚Ä¢ Dataset: nuScenes-A, Samples: 82770, Modality: LiDAR point clouds<br>
‚Ä¢ Dataset: CADC-SOT, Samples: 7375, Modality: LiDAR point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian Splatting Based Data Augmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.02845"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://iscas3dv.github.io/HOGSA/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Software, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: HOGSA, Samples: 2400000, Modality: RGB images + 3D hand/object poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal Conditions and Larger Scene</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.02807"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Computer Engineering, Peking University, China<br>
‚Ä¢ Dataset: AE-NeRF Synthetic Event Dataset, Samples: 8, Modality: Synthetic event streams, RGB images, ground truth camera poses, estimated camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>WorldPose: A World Cup Dataset for Global 3D Human Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.02771"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://eth-ait.github.io/WorldPoseDataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich<br>
‚Ä¢ Dataset: WorldPose, Samples: 88 sequences, Modality: RGB videos + 3D human poses (SMPL) + global trajectories + camera parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>JoyGen: Audio-Driven 3D Depth-Aware Talking-Face Video Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.01798"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JOY-MM/JoyGen"><img src="https://img.shields.io/github/stars/JOY-MM/JoyGen.svg?style=social&label=Star"></a><br><a href="https://joy-mm.github.io/JoyGen"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: JD.Com, Inc.<br>
‚Ä¢ Dataset: Chinese talking-face dataset, Samples: 1100, Modality: RGB videos + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Free-Form Motion Control: A Synthetic Video Generation Dataset with Controllable Camera and Object Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.01425"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://henghuiding.github.io/SynFMC"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: SynFMC, Samples: 26000, Modality: Synthetic videos + 6D object poses + 6D camera poses + segmentation masks + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>L3D-Pose: Lifting Pose for 3D Avatars from a Single Camera in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.01174"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://soumyaratnadebnath.github.io/L3D-Pose"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IIT Gandhinagar, India<br>
‚Ä¢ Dataset: Deep Macaque, Samples: 8000, Modality: Synthetic renderings + 2D/3D pose data<br>
‚Ä¢ Dataset: Deep Horse, Samples: 6000, Modality: Synthetic renderings + 2D/3D pose data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>DynamicLip: Shape-Independent Continuous Authentication via Lip Articulator Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.01032"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xxxxx"><img src="https://img.shields.io/github/stars/github.com/xxxxx.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100085, China<br>
‚Ä¢ Dataset: Dynamic Lip Authentication Dataset, Samples: 50 subjects, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>T-DOM: A Taxonomy for Robotic Manipulation of Deformable Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.20998"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/t-dom"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institut de Rob√≤tica i Inform√†tica Industrial, CSIC-UPC, Barcelona, Spain<br>
‚Ä¢ Dataset: Deformable Object Manipulation Dataset, Samples: 10 tasks, Modality: RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>DAVE: Diverse Atomic Visual Elements Dataset with High Representation of Vulnerable Road Users in Complex and Unpredictable Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.20042"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: DA VE, Samples: 1231, Modality: RGB videos + GPS + Bounding Box/Action Annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>UniAvatar: Taming Lifelike Audio-Driven Talking Head Generation with Comprehensive Motion and Lighting Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.19860"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Li Auto<br>
‚Ä¢ Dataset: DH-FaceDrasMvVid-100, Samples: None, Modality: RGB videos<br>
‚Ä¢ Dataset: DH-FaceReliVid-200, Samples: None, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>RobotDiffuse: Motion Planning for Redundant Manipulator based on Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.19500"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ACRoboT-buaa/RobotDiffuse"><img src="https://img.shields.io/github/stars/ACRoboT-buaa/RobotDiffuse.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software, Beihang University, Beijing, China.<br>
‚Ä¢ Dataset: Robot-obtalcles-panda (ROP), Samples: 140000, Modality: Robot pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Learning Monocular Depth from Events via Egomotion Compensation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.19067"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not specified in the document<br>
‚Ä¢ Dataset: EventCitySim, Samples: 5, Modality: RGB images, depth maps, event data, IMU measurements, gyroscope data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Mimicking-Bench: A Benchmark for Generalizable Humanoid-Scene Interaction Learning via Human Mimicking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.17730"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mimicking-bench.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: Mimicking-Bench, Samples: 23490, Modality: Human skeleton motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>V$^2$-SfMLearner: Learning Monocular Depth and Ego-motion for Multimodal Wireless Capsule Endoscopy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.17595"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China<br>
‚Ä¢ Dataset: Multimodal-WCE-1 (MM-WCE-1), Samples: 11, Modality: RGB videos + Vibration signals + Depth maps + Ego-motion trajectories<br>
‚Ä¢ Dataset: Multimodal-WCE-2 (MM-WCE-2), Samples: 11, Modality: RGB videos + Vibration signals + Depth maps + Ego-motion trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>WildPPG: A Real-World PPG Dataset of Long Continuous Recordings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.17540"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://siplab.org/projects/WildPPG"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science ETH Z√ºrich, Switzerland<br>
‚Ä¢ Dataset: WildPPG, Samples: 216 hours, Modality: PPG, Accelerometer, ECG, Temperature, Altitude<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>InterDance:Reactive 3D Dance Generation with Realistic Duet Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.16982"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://inter-dance.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: InterDance, Samples: None, Modality: MoCap (SMPL-X format)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Long-Term Upper-Limb Prosthesis Myocontrol via High-Density sEMG and Incremental Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.16271"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DarioDiDomenico/IncrHDsEMG"><img src="https://img.shields.io/github/stars/DarioDiDomenico/IncrHDsEMG.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/record/10801000"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Rehab Technologies Lab, Istituto Italiano di Tecnologia (IIT), Genoa, Italy, DET, Politecnico di Torino, Turin, Italy<br>
‚Ä¢ Dataset: DELTA, Samples: 2940, Modality: High-Density surface electromyography (HD-sEMG) signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>AutoLife: Automatic Life Journaling with Smartphones and LLMs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.15714"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: A self-collected human life dataset for life journaling, Samples: 58, Modality: smartphone sensor data (accelerometer, gyroscope, barometer, GPS speed, GPS location, WiFi signals)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.15664"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://virtualhumans.mpi-inf.mpg.de/scenic/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: T√ºbingen AI Center, University of T√ºbingen; Max Planck Institute for Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: The SCENIC Dataset, Samples: 15000, Modality: SMPL motion + text annotations + terrain meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Event-assisted 12-stop HDR Imaging of Dynamic Scene</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.14705"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://openimaginglab.github.io/Event-Assisted-12stops-HDR/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory<br>
‚Ä¢ Dataset: ESHDR, Samples: None, Modality: Synchronized LDR images and event camera streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Learning from Massive Human Videos for Universal Humanoid Pose Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.14172"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://usc-gvl.github.io/UH-1"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Southern California<br>
‚Ä¢ Dataset: Humanoid-X, Samples: 163800, Modality: RGB videos, text descriptions, SMPL human poses, humanoid keypoints, humanoid target DoF positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>TH√ñR-MAGNI Act: Actions for Human Motion Modeling in Robot-Shared Industrial Spaces</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.13729"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tmralmeida/thor-magni-actions"><img src="https://img.shields.io/github/stars/tmralmeida/thor-magni-actions.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AASS, ¬®Orebro University<br>
‚Ä¢ Dataset: TH¬®OR-MAGNI Act, Samples: 8.3 hours of labeled actions, Modality: Action labels, MoCap trajectories, Egocentric video, Gaze data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Move-in-2D: 2D-Conditioned Human Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.13185"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hhsinping.github.io/Move-in-2D"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Adobe Research, University of California, Merced<br>
‚Ä¢ Dataset: Humans-in-Context Motion (HiC-Motion), Samples: 300000, Modality: RGB videos + SMPL motion + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Continuous Patient Monitoring with AI: Real-Time Analysis of Video in Hospital Care Settings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.13152"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lookdeep/ai-norms-2024"><img src="https://img.shields.io/github/stars/lookdeep/ai-norms-2024.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: LookDeep Health<br>
‚Ä¢ Dataset: ai-norms-2024, Samples: 1466 patient-days, Modality: Computer vision predictions (object detection, role classification, motion estimation) from RGB/NIR video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>A New Adversarial Perspective for LiDAR-based 3D Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.13017"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Informatics, Xiamen University, China; Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, China<br>
‚Ä¢ Dataset: ROLiD, Samples: 1964 water mist sequences, 664 smoke sequences, Modality: LiDAR point cloud sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Task-Parameter Nexus: Task-Specific Parameter Learning for Model-Based Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.12448"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ILLINOIS.EDU<br>
‚Ä¢ Dataset: Trajectory Bank, Samples: 1200, Modality: 2D quadrotor polynomial trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Can video generation replace cinematographers? Research on the cinematic language of generated video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.12223"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tongji University<br>
‚Ä¢ Dataset: Cinematic2K, Samples: 2000, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Instruction-based Image Manipulation by Watching How Things Move</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.12087"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo<br>
‚Ä¢ Dataset: InstructMove dataset, Samples: 6000000, Modality: RGB image pairs + text instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.11974"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/declare-lab/EMMA-X"><img src="https://img.shields.io/github/stars/declare-lab/EMMA-X.svg?style=social&label=Star"></a><br><a href="https://declare-lab.github.io/Emma-X/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Singapore University of Technology and Design<br>
‚Ä¢ Dataset: hierarchical embodiment dataset, Samples: 60000, Modality: Robot manipulation trajectories (7D actions) + images + textual annotations (grounded reasoning, 3D movement plans) + 2D gripper positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.11198"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vita-epfl/GEM"><img src="https://img.shields.io/github/stars/vita-epfl/GEM.svg?style=social&label=Star"></a><br><a href="https://vita-epfl.github.io/GEM.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: √âcole Polytechnique F√©d√©rale de Lausanne (EPFL)<br>
‚Ä¢ Dataset: GEM Curated and Pseudo-Labeled Dataset, Samples: None, Modality: RGB videos + pseudo-labeled depth, ego-trajectories, and human poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>A Pioneering Neural Network Method for Efficient and Robust Fluid Simulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.10748"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software Engineering, Xi‚Äôan Jiaotong University, Xi‚Äôan, 710049, China<br>
‚Ä¢ Dataset: Fueltank dataset, Samples: 320000, Modality: SPH fluid particle simulation<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SUGAR: Subject-Driven Video Customization in a Zero-Shot Manner</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.10533"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Adobe Research<br>
‚Ä¢ Dataset: Unnamed synthetic dataset for subject-driven video customization, Samples: 2500000, Modality: RGB videos + text + images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.09623"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lwq20020127.github.io/OmniDrag"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Computer Engineering, Peking University<br>
‚Ä¢ Dataset: Move360, Samples: 1580, Modality: Omnidirectional videos (ODV)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.09283"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/NJU-PCALab/InstanceCap"><img src="https://img.shields.io/github/stars/NJU-PCALab/InstanceCap.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: InstanceVid, Samples: 22000, Modality: RGB videos + structured captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SyncViolinist: Music-Oriented Violin Motion Generation Based on Bowing and Fingering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.08343"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Kakanat/SyncViolinist"><img src="https://img.shields.io/github/stars/Kakanat/SyncViolinist.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Waseda University<br>
‚Ä¢ Dataset: SyncViolinist Dataset, Samples: 61, Modality: MoCap joints, audio, MIDI, bowing/fingering annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Generative Zoo</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.08101"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://genzoo.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T ¬®ubingen, Germany<br>
‚Ä¢ Dataset: GenZoo, Samples: 1000000, Modality: RGB images + 3D pose and shape parameters<br>
‚Ä¢ Dataset: GenZoo-Felidae, Samples: 100, Modality: RGB images + 3D pose and shape parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Repurposing Pre-trained Video Diffusion Models for Event-based Video Interpolation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07761"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vdm-evfi.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, Park<br>
‚Ä¢ Dataset: Clear-Motion, Samples: 9 sequences, Modality: RGB videos + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07759"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://fuxiao0719.github.io/projects/3dtrajmaster"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: 360‚ó¶-Motion Dataset, Samples: 54000, Modality: RGB videos + 6DoF pose trajectories + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07755"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Boston University<br>
‚Ä¢ Dataset: SAT (Spatial Aptitude Training), Samples: 175000, Modality: Simulated 2D images + Question-Answer pairs about static and dynamic (ego/object motion) spatial reasoning<br>
‚Ä¢ Dataset: SAT real-image dynamic test set, Samples: 150, Modality: Real-world images + Question-Answer pairs about dynamic spatial reasoning<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>On Motion Blur and Deblurring in Visual Place Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07751"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/bferrarini/MotionBlurGenerator"><img src="https://img.shields.io/github/stars/bferrarini/MotionBlurGenerator.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronics and Computer Science, University of Southampton, SO17 1BJ Southampton, U.K.<br>
‚Ä¢ Dataset: Blurry Places benchmark, Samples: 9 traverses, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Benchmarking Vision-Based Object Tracking for USVs in Complex Maritime Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07392"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Muhayyuddin/tracking"><img src="https://img.shields.io/github/stars/Muhayyuddin/tracking.svg?style=social&label=Star"></a><br><a href="https://muhayyuddin.github.io/tracking/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University, United Arab Emirates.<br>
‚Ä¢ Dataset: Real-world USV tracking dataset, Samples: 21, Modality: RGB videos<br>
‚Ä¢ Dataset: Simulated USV tracking dataset, Samples: 5, Modality: Simulated RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view RGB and Event Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.06770"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/4dqv/DynEventNeRF"><img src="https://img.shields.io/github/stars/4dqv/DynEventNeRF.svg?style=social&label=Star"></a><br><a href="https://4dqv.mpi-inf.mpg.de/DynEventNeRF/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Informatics, SIC; Saarland University<br>
‚Ä¢ Dataset: Dynamic EventNeRF Real Dataset, Samples: 16, Modality: multi-view (6) RGB and event streams<br>
‚Ä¢ Dataset: Dynamic EventNeRF Synthetic Dataset, Samples: 5, Modality: multi-view (5) synthetic RGB frames and event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Object Detection using Event Camera: A MoE Heat Conduction based Detector and A New Benchmark Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.06647"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/OpenEvDET1"><img src="https://img.shields.io/github/stars/Event-AHU/OpenEvDET1.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: EvDET200K, Samples: 10054, Modality: Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>doScenes: An Autonomous Driving Dataset with Natural Language Instruction for Human Interaction and Vision-Language Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05893"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.github.com/rossgreer/doScenes"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Merced<br>
‚Ä¢ Dataset: doScenes, Samples: 1000, Modality: vehicle trajectories, multimodal sensor data (cameras, LiDAR, radar), natural language instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>MotionShop: Zero-Shot Motion Transfer in Video Diffusion Models with Mixture of Score Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05355"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="motionshop-diffusion.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Virginia Tech<br>
‚Ä¢ Dataset: MotionBench, Samples: 1200, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>ACT-Bench: Towards Action Controllable World Models for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05337"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Turing Inc.<br>
‚Ä¢ Dataset: ACT-BENCH, Samples: 2286, Modality: RGB videos + trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Œª: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile Manipulation Robotics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05313"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="lambdabenchmark.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: LAMBDA (Œª), Samples: 571, Modality: RGB-D egocentric observations, segmentations, robot/object poses, discrete actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Text to Blind Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05277"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://blindways.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Boston University<br>
‚Ä¢ Dataset: BlindWays, Samples: 1029, Modality: IMU-based 3D joints, text descriptions, synchronized RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Monocular Dynamic Gaussian Splatting: Fast, Brittle, and Scene Complexity Rules</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.04457"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: instructive synthetic dataset, Samples: 30, Modality: Synthetic RGB videos + camera poses<br>
‚Ä¢ Dataset: D-NeRF, Nerfies, HyperNeRF, NeRF-DS, iPhone dataset (extensions), Samples: 50, Modality: Segmentation masks and improved camera poses for existing RGB video datasets<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.04244"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ivl.cs.brown.edu/research/gigahands.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: GigaHands, Samples: 14000, Modality: Multi-view RGB videos, 3D hand poses (keypoints, MANO meshes), 3D object poses, text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.04037"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://grisoon.github.io/INFP/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Bytedance<br>
‚Ä¢ Dataset: DyConv, Samples: over 200 hours of video clips, Modality: Video clips of dyadic conversations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Dense Scene Reconstruction from Light-Field Images Affected by Rolling Shutter</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.03518"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ICB-Vision-AI/DenseRSLF"><img src="https://img.shields.io/github/stars/ICB-Vision-AI/DenseRSLF.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universit ¬¥e de Bourgogne, CNRS UMR 6303 ICB; Universit ¬¥e de Franche-Comt ¬¥e, CNRS UMR 6174 FEMTO-ST<br>
‚Ä¢ Dataset: RSLF+, Samples: None, Modality: Light-field images + depth maps + visibility masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Video LLMs for Temporal Reasoning in Long Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02930"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.retrocausal.ai"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Retrocausal, Inc.<br>
‚Ä¢ Dataset: IndustryASM, Samples: 4803, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Adaptive LiDAR Odometry and Mapping for Autonomous Agricultural Mobile Robots in Unmanned Farms</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02899"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UCR-Robotics/AG-LOAM"><img src="https://img.shields.io/github/stars/UCR-Robotics/AG-LOAM.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, University of California-Riverside<br>
‚Ä¢ Dataset: AG-LOAM dataset, Samples: 18, Modality: LiDAR point clouds, robot odometry, GPS-RTK<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>emg2pose: A Large and Diverse Benchmark for Surface Electromyographic Hand Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02725"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/emg2pose"><img src="https://img.shields.io/github/stars/facebookresearch/emg2pose.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Reality Labs, Meta<br>
‚Ä¢ Dataset: emg2pose, Samples: 25253, Modality: sEMG + motion capture joint angles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02449"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://byencoder.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of Freiburg, 79110 Freiburg, Germany<br>
‚Ä¢ Dataset: BYE Dataset, Samples: 29, Modality: RGB-D images, instance masks, camera pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>It Takes Two: Real-time Co-Speech Two-person's Interaction Generation via Reactive Auto-regressive Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02419"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: InterAct++, Samples: 402, Modality: MoCap joints + Face + Audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Dual Exposure Stereo for Extended Dynamic Range 3D Imaging</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02351"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: POSTECH<br>
‚Ä¢ Dataset: Real-world Dataset, Samples: 7432, Modality: stereo RGB videos + LiDAR point clouds<br>
‚Ä¢ Dataset: Synthetic Dataset, Samples: 1200, Modality: synthetic stereo videos + depth maps + disparity maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Continuous-Time Human Motion Field from Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.01747"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Pennsylvania, USA<br>
‚Ä¢ Dataset: Beam-splitter Event Agile Human Motion Dataset (BEAHM), Samples: 40, Modality: Events + RGB videos + 3D meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Articulate3D: Holistic Understanding of 3D Scenes as Universal Scene Description</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.01398"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: INSAIT, Sofia University ‚ÄúSt. Kliment Ohridski‚Äù<br>
‚Ä¢ Dataset: Articulate3D, Samples: 280, Modality: 3D scene scans with part-level articulation kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Object Agnostic 3D Lifting in Space and Time</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.01166"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Adelaide<br>
‚Ä¢ Dataset: AnimalSyn3D, Samples: 678, Modality: 3D skeleton motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Human Action CLIPs: Detecting AI-generated Human Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.00526"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.huggingface.co/datasets/faridlab/deepaction"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google, Stanford University<br>
‚Ä¢ Dataset: DeepAction, Samples: 3200, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.00174"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://solami-ai.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SenseTime Research<br>
‚Ä¢ Dataset: SynMSI, Samples: 6300, Modality: SMPL-X joint rotations and synthesized speech<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>ETAP: Event-based Tracking of Any Point</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.00133"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tub-rip/ETAP"><img src="https://img.shields.io/github/stars/tub-rip/ETAP.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit√§t Berlin<br>
‚Ä¢ Dataset: EventKubric, Samples: 10173, Modality: Events + RGB videos + point tracks + optical flow + depth + segmentations<br>
‚Ä¢ Dataset: EVIMO2, Samples: None, Modality: ground truth point tracks (new annotation)<br>
‚Ä¢ Dataset: E2D2, Samples: None, Modality: ground truth point tracks (new annotation)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.00115"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fudan-generative-vision.github.io/OpenHumanVid"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: OpenHumanVid, Samples: 13200000, Modality: RGB videos + skeleton pose + speech audio + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>SIMS: Simulating Stylized Human-Scene Interactions with Retrieval-Augmented Script Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.19921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wenjiawang0312.github.io/projects/sims/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: ViconStyle, Samples: 415, Modality: MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>SkelMamba: A State Space Model for Efficient Skeleton Action Recognition of Neurological Disorders</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.19544"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Udine<br>
‚Ä¢ Dataset: Neurological Disorders (ND), Samples: 396, Modality: RGB videos + skeleton joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.19167"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/hand-tracking-toolkit"><img src="https://img.shields.io/github/stars/facebookresearch/hand-tracking-toolkit.svg?style=social&label=Star"></a><br><a href="https://facebookresearch.github.io/hot3d"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta Reality Labs<br>
‚Ä¢ Dataset: HOT3D, Samples: 3832, Modality: Multi-view egocentric RGB/monochrome videos + motion-capture poses (hands and objects) + eye gaze + SLAM point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Lifting Motion to the 3D World via 2D Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.18808"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: NicoleMove, Samples: None, Modality: RGB videos + 2D pose sequences<br>
‚Ä¢ Dataset: CatPlay, Samples: None, Modality: RGB videos + 2D keypoint sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>GaussianSpeech: Audio-Driven Gaussian Avatars</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.18675"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://shivangi-aneja.github.io/projects/gaussianspeech"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich<br>
‚Ä¢ Dataset: Multi-view Audio-Visual Dataset (unnamed), Samples: 2500, Modality: Multi-view RGB videos, audio, and 3D face trackings<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.18654"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Motion-AILab/AToM"><img src="https://img.shields.io/github/stars/Motion-AILab/AToM.svg?style=social&label=Star"></a><br><a href="https://atom-motion.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen International Graduate School, Tsinghua University<br>
‚Ä¢ Dataset: MotionPrefer, Samples: 47100, Modality: Generated 3D human motion sequences with text-based preference labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>MotionCharacter: Identity-Preserving and Motion Controllable Human Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.18281"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motioncharacter.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huazhong University of Science and Technology<br>
‚Ä¢ Dataset: Human-Motion, Samples: 106292, Modality: RGB videos + text captions + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Snake-Inspired Mobile Robot Positioning with Hybrid Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.17430"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ansfl/MoRPINet"><img src="https://img.shields.io/github/stars/ansfl/MoRPINet.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hatter Department of Marine Technologies, University of Haifa, Israel<br>
‚Ä¢ Dataset: MoRPINet, Samples: 13, Modality: IMU + GNSS-RTK<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Enhancing Lane Segment Perception and Topology Reasoning with Crowdsourcing Trajectory Priors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.17161"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wowlza/TrajTopo"><img src="https://img.shields.io/github/stars/wowlza/TrajTopo.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Vehicle and Mobility, Tsinghua University<br>
‚Ä¢ Dataset: Supplementary Trajectory Dataset for OpenLane-V2, Samples: None, Modality: crowdsourcing trajectory data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Leveraging Foundation Models To learn the shape of semi-fluid deformable objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16802"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Alstom transports, IMVIA EA 7535 laboratory, university of Burgundy<br>
‚Ä¢ Dataset: Weld Pool Dataset, Samples: 9, Modality: RGB videos + keypoint labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Bundle Adjusted Gaussian Avatars Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16758"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Artificial Intelligence Laboratory, The University of Tokyo<br>
‚Ä¢ Dataset: Synthetic ZJU-MoCap Deblurring Dataset, Samples: 6, Modality: RGB videos + SMPL parameters<br>
‚Ä¢ Dataset: 360-degree Hybrid-Exposure Human Motion Dataset, Samples: 8, Modality: Multi-view RGB videos (blurry and sharp) + SMPL parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>DiM-Gestor: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16729"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zf223669/DiMGestures"><img src="https://img.shields.io/github/stars/zf223669/DiMGestures.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Media Engineering, Communication University of Zhejiang, China<br>
‚Ä¢ Dataset: Chinese Co-Speech Gestures (CCG) dataset, Samples: 391, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Deep Learning for Motion Classification in Ankle Exoskeletons Using Surface EMG and IMU Signals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16273"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Sr933/Exoskeleton-Data-Acquisition-and-Processing-Code"><img src="https://img.shields.io/github/stars/Sr933/Exoskeleton-Data-Acquisition-and-Processing-Code.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.17863/CAM.113504"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Electrical Engineering Division, Department of Engineering, University of Cambridge, Cambridge CB3 0FA, UK; School of Clinical Medicine, University of Cambridge, Cambridge CB2 0SP, UK<br>
‚Ä¢ Dataset: None, Samples: 1504, Modality: sEMG and IMU signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>SMGDiff: Soccer Motion Generation using diffusion probabilistic models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16216"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Soccer-X, Samples: 2398, Modality: MoCap sequences (SMPL)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>KinMo: Kinematic-aware Human Motion Understanding and Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.15472"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://andypinxinliu.github.io/KinMo"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Irvine<br>
‚Ä¢ Dataset: KinMo, Samples: 14616, Modality: MoCap joints + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>A Benchmark Dataset for Collaborative SLAM in Service Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.14775"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vision3d-lab/CSE_Dataset"><img src="https://img.shields.io/github/stars/vision3d-lab/CSE_Dataset.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Artificial Intelligence Graduate School, UNIST, South Korea<br>
‚Ä¢ Dataset: C-SLAM dataset in Service Environments (CSE), Samples: 18, Modality: stereo RGB, stereo depth, IMU, GT pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>sEMG-based Gesture-Free Hand Intention Recognition: System, Dataset, Toolbox, and Benchmark Results</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.14131"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Intelligence Science and Technology, National University of Defense Technology, Changsha 410073, China<br>
‚Ä¢ Dataset: sEMG-based Gesture-Free Hand Intention Recognition Dataset, Samples: None, Modality: 8-channel sEMG signals, 3-channel IMU data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Sparse Input View Synthesis: 3D Representations and Reliable Priors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.13631"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nagabhushansn95.github.io/publications.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Electrical Communication Engineering, Indian Institute of Science<br>
‚Ä¢ Dataset: Indian Institute of Science Virtual Environment Exploration Dataset - Dynamic Scenes (IISc VEED-Dynamic), Samples: 800, Modality: RGB videos + depth + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>VioPose: Violin Performance 4D Pose Estimation by Hierarchical Audiovisual Inference</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.13607"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SeongJong-Yoo/VioPose"><img src="https://img.shields.io/github/stars/SeongJong-Yoo/VioPose.svg?style=social&label=Star"></a><br><a href="https://sj-yoo.info/viopose/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: VioDat, Samples: 639, Modality: 3D motion capture (kinematic joints), synchronized multi-view video (4 cameras), synchronized audio (4 microphones)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Can Reasons Help Improve Pedestrian Intent Estimation? A Cross-Modal Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.13302"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Visual Information Technology (CVIT) Lab, IIIT Hyderabad<br>
‚Ä¢ Dataset: PIE++, Samples: 1842, Modality: RGB videos + multi-label textual reason annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Enhancing Thermal MOT: A Novel Box Association Method Leveraging Thermal Identity and Motion Similarity</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.12943"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wassimea/thermalMOT"><img src="https://img.shields.io/github/stars/wassimea/thermalMOT.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Ottawa<br>
‚Ä¢ Dataset: RGB-Thermal MOT dataset, Samples: 30, Modality: RGB videos + Thermal videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>EROAM: Event-based Camera Rotational Odometry and Mapping in Real-time</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.11004"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wlxing1901/EROAM"><img src="https://img.shields.io/github/stars/wlxing1901/EROAM.svg?style=social&label=Star"></a><br><a href="https://wlxing1901.github.io/eroam/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, The University of Hong Kong, Hong Kong SAR, China<br>
‚Ä¢ Dataset: EROAM-campus, Samples: 10, Modality: Event camera + LiDAR + 3DoF rotational motion trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>The Oxford Spires Dataset: Benchmarking Large-Scale LiDAR-Visual Localisation, Reconstruction and Radiance Field Methods</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.10546"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Oxford Robotics Inst., Dept. of Eng. Science, Univ. of Oxford, UK<br>
‚Ä¢ Dataset: Oxford Spires Dataset, Samples: 24, Modality: RGB images, LiDAR, IMU, Ground Truth Trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.10504"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chenkang455/USP-Gaussian"><img src="https://img.shields.io/github/stars/chenkang455/USP-Gaussian.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Peking University<br>
‚Ä¢ Dataset: Synthetic Spike Dataset (based on Deblur-NeRF), Samples: None, Modality: Spike streams<br>
‚Ä¢ Dataset: Real-world Spike Dataset, Samples: None, Modality: Spike streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Gait Kinematics in Healthy Participants: A Motion Capture Dataset Under Weight Load and Knee Brace Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.10485"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://figshare.com/articles/dataset/IMU-Based_Motion_Capture_Data_for_Various_Walking_Tasks/26090200"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Electrical Engineering, K. N. Toosi University of Technology, Tehran, 1631714191, Iran<br>
‚Ä¢ Dataset: IMU-Based Motion Capture Data for Various Walking Tasks, Samples: None, Modality: IMU data (raw sensor, processed, Euler angles, joint kinematics)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.09921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://groundmore.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CRCV, University of Central Florida<br>
‚Ä¢ Dataset: GROUND-MORE, Samples: 1715, Modality: RGB videos + text questions + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>SINETRA: a Versatile Framework for Evaluating Single Neuron Tracking in Behaving Animals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.09462"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/raphaelreme/SINETRA"><img src="https://img.shields.io/github/stars/raphaelreme/SINETRA.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institut Pasteur, Universit ¬¥e de Paris-Cit ¬¥e, CNRS UMR 3691, BioImage Analysis Unit F-75015 Paris, France<br>
‚Ä¢ Dataset: SINETRA Synthetic Dataset, Samples: 15, Modality: Synthetic 2D/3D fluorescence videos with ground truth particle trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.08380"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://egovid.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Alibaba<br>
‚Ä¢ Dataset: EgoVid-5M, Samples: 5000000, Modality: RGB videos + kinematic control (VIO) + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.08279"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WU-CVGL/MBA-SLAM"><img src="https://img.shields.io/github/stars/WU-CVGL/MBA-SLAM.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology at Zhejiang University and the School of Engineering at Westlake University, Hangzhou, China<br>
‚Ä¢ Dataset: real-world motion-blurred SLAM dataset, Samples: 3, Modality: RGB-D video + MoCap ground truth trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.06757"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/quzefan/LuSh-NeRF"><img src="https://img.shields.io/github/stars/quzefan/LuSh-NeRF.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science City University of Hong Kong<br>
‚Ä¢ Dataset: LOL-BlurNeRF, Samples: 10, Modality: RGB videos + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>GraV: Grasp Volume Data for the Design of One-Handed XR Interfaces</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.05245"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HAL-UCSB/grav-sim"><img src="https://img.shields.io/github/stars/HAL-UCSB/grav-sim.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California Santa Barbara, CA, USA<br>
‚Ä¢ Dataset: GraV, Samples: 367, Modality: point clouds + motion cost<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Pose2Trajectory: Using Transformers on Body Pose to Predict Tennis Player's Trajectory</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.04501"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/alshami52/Pose2Trajectory.git"><img src="https://img.shields.io/github/stars/alshami52/Pose2Trajectory.git.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Science Department, University of Colorado, Colorado Springs<br>
‚Ä¢ Dataset: None, Samples: , Modality: 2D joint positions, bounding boxes, ball coordinates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>MA^2: A Self-Supervised and Motion Augmenting Autoencoder for Gait-Based Automatic Disease Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.03129"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/EchoItLiu/MA2-PyTorch"><img src="https://img.shields.io/github/stars/EchoItLiu/MA2-PyTorch.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information Technology and Media, Hexi University, Zhangye, 734000, P.R. China<br>
‚Ä¢ Dataset: tRGG, Samples: 101125, Modality: Ground Reaction Force (GRF)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Real-Time Detection for Small UAVs: Combining YOLO and Multi-frame Motion Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.02582"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Aerospace Engineering, Beijing Institute of Technology, Beijing 100081, China<br>
‚Ä¢ Dataset: Fixed-Wings Dataset, Samples: 13, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>GenXD: Generating Any 3D and 4D Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.02319"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gen-x-d.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National University of Singapore<br>
‚Ä¢ Dataset: CamVid-30K, Samples: 30000, Modality: videos with camera poses and object motion strength<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Object segmentation from common fate: Motion energy processing enables human-like zero-shot generalization to random dot stimuli</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.01505"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mtangemann/motion_energy_segmentation"><img src="https://img.shields.io/github/stars/mtangemann/motion_energy_segmentation.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of T√ºbingen, T√ºbingen AI Center<br>
‚Ä¢ Dataset: Synthetic video dataset for motion segmentation (unnamed), Samples: 1001, Modality: Synthetic RGB videos with ground truth masks and optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Nightbeat: Heart Rate Estimation From a Wrist-Worn Accelerometer During Sleep</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.00731"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/eth-siplab/Nightbeat"><img src="https://img.shields.io/github/stars/eth-siplab/Nightbeat.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, ETH Zurich, Z√ºrich, Switzerland<br>
‚Ä¢ Dataset: Nightbeat-DB, Samples: 42, Modality: 3-axis accelerometer signals + ECG signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Muscles in Time: Learning to Understand Human Motion by Simulating Muscle Activations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.00128"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/simplexsigil/MusclesInTime"><img src="https://img.shields.io/github/stars/simplexsigil/MusclesInTime.svg?style=social&label=Star"></a><br><a href="https://simplexsigil.github.io/mint"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Karlsruhe Institute of Technology<br>
‚Ä¢ Dataset: Muscles in Time (MinT), Samples: None, Modality: Simulated muscle activations + pose sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Learning Video Representations without Natural Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.24213"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://unicorn53547.github.io/video_syn_rep/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Accelerating and transforming textures, Samples: 9537, Modality: Synthetic RGB videos<br>
‚Ä¢ Dataset: Accelerating and transforming StyleGAN crops, Samples: 9537, Modality: Synthetic RGB videos<br>
‚Ä¢ Dataset: Accelerating and transforming image crops, Samples: 9537, Modality: Synthetic RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Exploiting Information Theory for Intuitive Robot Programming of Manual Activities</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.23963"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.13846970"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Human-Robot Interfaces and Interaction, Istituto Italiano di Tecnologia, Genoa, Italy<br>
‚Ä¢ Dataset: HANDSOME (HAND Skills demOnstrated by Multi-subjEcts), Samples: 250, Modality: RGB videos + 6D poses (from ArUco markers)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided Mixture-of-Experts</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.23836"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: HDAV (High-definition Audio-Visual dataset), Samples: 2203, Modality: RGB videos + audio + 3D human template parameter annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>GS-Blur: A 3D Scene-Based Dataset for Realistic Image Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.23658"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dongwoohhh/GS-Blur"><img src="https://img.shields.io/github/stars/dongwoohhh/GS-Blur.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of ECE&ASRI, Seoul National University, Korea<br>
‚Ä¢ Dataset: GS-Blur, Samples: 752335, Modality: Paired blurry and sharp RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>bit2bit: 1-bit quanta video reconstruction via self-supervised photon prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.23247"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lyehe/ssunet"><img src="https://img.shields.io/github/stars/lyehe/ssunet.svg?style=social&label=Star"></a><br><a href="https://drive.google.com/drive/folders/1M5bsmsaLBkYmO7nMUjK5_m71RonOp-P9"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Case Western Reserve University<br>
‚Ä¢ Dataset: bit2bit SPAD video dataset, Samples: 7 real SPAD videos (100k-130k frames each) and 1 synthetic video (3990 frames), Modality: 1-bit SPAD high-speed videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>NYC-Event-VPR: A Large-Scale High-Resolution Event-Based Visual Place Recognition Dataset in Dense Urban Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.21615"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ai4ce/NYC-Event-VPR"><img src="https://img.shields.io/github/stars/ai4ce/NYC-Event-VPR.svg?style=social&label=Star"></a><br><a href="https://ai4ce.github.io/NYC-Event-VPR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: New York University, Brooklyn, NY 11201, USA<br>
‚Ä¢ Dataset: NYC-Event-VPR, Samples: None, Modality: event streams, RGB videos, GPS trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Skinned Motion Retargeting with Dense Geometric Interaction Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20986"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/abcyzj/MeshRet"><img src="https://img.shields.io/github/stars/abcyzj/MeshRet.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Technology, BNRist, Tsinghua University; Key Laboratory of Pervasive Computing, Ministry of Education<br>
‚Ä¢ Dataset: ScanRet, Samples: 8298, Modality: Motion capture sequences on 3D scanned meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>CardiacNet: Learning to Reconstruct Abnormalities for Cardiac Disease Assessment from Echocardiogram Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20769"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xmed-lab/CardiacNet"><img src="https://img.shields.io/github/stars/xmed-lab/CardiacNet.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: CardiacNet-PAH, Samples: 496, Modality: Echocardiogram videos<br>
‚Ä¢ Dataset: CardiacNet-ASD, Samples: 231, Modality: Echocardiogram videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>YourSkatingCoach: A Figure Skating Video Benchmark for Fine-Grained Element Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20427"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Science, Academia Sinica<br>
‚Ä¢ Dataset: YourSkatingCoach, Samples: 454, Modality: RGB videos + 2D skeletons<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>NT-VOT211: A Large-Scale Benchmark for Night-time Visual Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20421"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LiuYuML/NV-VOT211"><img src="https://img.shields.io/github/stars/LiuYuML/NV-VOT211.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xinjiang University<br>
‚Ä¢ Dataset: NT-VOT211, Samples: 211, Modality: RGB videos + bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>SFTrack: A Robust Scale and Motion Adaptive Algorithm for Tracking Small and Fast Moving Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20079"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Immersive Media Engineering & Department of Computer Science Education, Sungkyunkwan University<br>
‚Ä¢ Dataset: Refined UAVDT, Samples: 55, Modality: RGB videos + bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>On Occlusions in Video Action Detection: Benchmark Datasets And Training Recipes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.19553"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rajatmodi62/OccludedActionBenchmark"><img src="https://img.shields.io/github/stars/rajatmodi62/OccludedActionBenchmark.svg?style=social&label=Star"></a><br><a href="nan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CRCV, University of Central Florida<br>
‚Ä¢ Dataset: O-UCF, Samples: 20306, Modality: RGB videos<br>
‚Ä¢ Dataset: OVIS-UCF, Samples: 20306, Modality: RGB videos<br>
‚Ä¢ Dataset: O-JHMDB, Samples: 5896, Modality: RGB videos<br>
‚Ä¢ Dataset: OVIS-JHMDB, Samples: 5896, Modality: RGB videos<br>
‚Ä¢ Dataset: Real-OUCF, Samples: 1743, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>x-RAGE: eXtended Reality -- Action & Gesture Events Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.19486"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.com/NVM_IITD_Research/xrage"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Indian Institute of Technology Delhi, New Delhi, India - 110016<br>
‚Ä¢ Dataset: X-RAGE, Samples: 8064, Modality: event camera<br>
</td></tr>
</table>

## üí™ How to Contribute

If you have a paper or are aware of relevant research that should be incorporated, please contribute via pull requests, issues, email, or other suitable methods.
