# Awesome-Motion-Datasets

## üîç Related Papers

‚ö†Ô∏è Automated analysis may be inaccurate.

<table style="width: 100%;">
<tr><td><strong>Date</strong></td><td><strong>Paper</strong></td><td><strong>Contribution</strong></td><td><strong>Links</strong></td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2026</td>
  <td style="width:70%;"><strong>MiDAS: A Multimodal Data Acquisition System and Dataset for Robot-Assisted Minimally Invasive Surgery</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2602.12407"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/uva-dsa/MiDAS"><img src="https://img.shields.io/github/stars/uva-dsa/MiDAS.svg?style=social&label=Star"></a><br><a href="https://uva-dsa.github.io/MiDAS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Virginia<br>
‚Ä¢ Dataset: MiDAS Raven-II Peg Transfer Dataset, Samples: 15 trials (345 gesture samples), Modality: robot kinematics (MTM/PSM), electromagnetic hand tracking (EmHT), RGB-D hand tracking, foot pedal sensing, video<br>
‚Ä¢ Dataset: MiDAS da Vinci Xi Suturing Dataset (Hernia repair), Samples: 17 trials (1,724 gesture samples), Modality: electromagnetic hand tracking (EmHT), RGB-D hand tracking, foot pedal sensing, video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2026</td>
  <td style="width:70%;"><strong>Depth Completion in Unseen Field Robotics Environments Using Extremely Sparse Depth Measurements</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2602.03209"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/is-mpg/amass"><img src="https://img.shields.io/github/stars/is-mpg/amass.svg?style=social&label=Star"></a><br><a href="https://amass.is.tue.mpg.de/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems<br>
‚Ä¢ Dataset: AMASS (Archive of Motion Capture as Surface Shapes), Samples: 40,000+, Modality: MoCap joints / 3D mesh (SMPL)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.19582"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yjwangtj/ScenePilot-Bench"><img src="https://img.shields.io/github/stars/yjwangtj/ScenePilot-Bench.svg?style=social&label=Star"></a><br><a href="https://huggingface.co/datasets/larswangtj/ScenePilot-4k"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Automotive and Energy Engineering, Tongji University<br>
‚Ä¢ Dataset: ScenePilot-4K, Samples: 2,770,000 sequences (based on 27.7M frames across 5-second clips), Modality: First-person driving videos, ego-trajectories, and camera parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>Video Individual Counting and Tracking from Moving Drones: A Benchmark and Methods</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.12500"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sun Yat-sen University<br>
‚Ä¢ Dataset: MovingDroneCrowd++, Samples: 27,866 pedestrian trajectories, Modality: RGB videos + pedestrian trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.12224"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/EricGuo5513/HumanML3D"><img src="https://img.shields.io/github/stars/EricGuo5513/HumanML3D.svg?style=social&label=Star"></a><br><a href="None stated"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: HumanML3D, Samples: 14,616 motion sequences, Modality: 3D MoCap joints + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>The Mini Wheelbot Dataset: High-Fidelity Data for Robot Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.11394"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wheelbot/dataset"><img src="https://img.shields.io/github/stars/wheelbot/dataset.svg?style=social&label=Star"></a><br><a href="https://github.com/wheelbot/dataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Data Science in Mechanical Engineering (DSME), RWTH Aachen University, Germany<br>
‚Ä¢ Dataset: Mini Wheelbot Dataset, Samples: 383 trajectories, Modality: MoCap (Vicon position and orientation), IMU (gyroscope and accelerometer), wheel encoders (angle and velocity), third-person RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>FrankenMotion: Part-level Human Motion Generation and Composition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.10909"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/coral79/frankenmotion"><img src="https://img.shields.io/github/stars/coral79/frankenmotion.svg?style=social&label=Star"></a><br><a href="https://coral79.github.io/frankenmotion/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: T√ºbingen AI Center, University of T√ºbingen, Germany<br>
‚Ä¢ Dataset: FrankenStein, Samples: 16,000, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.10716"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wild-rayzer.cs.virginia.edu/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Virginia<br>
‚Ä¢ Dataset: Dynamic RealEstate10K (D-RE10K), Samples: 15,000 sequences, Modality: RGB dynamic videos with motion masks<br>
‚Ä¢ Dataset: D-RE10K-iPhone, Samples: 50 sequences, Modality: paired transient and clean RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.10632"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: HKUST<br>
‚Ä¢ Dataset: CoMoVi Dataset, Samples: 54,053 clips, Modality: 3D SMPL motions, high-resolution RGB videos, and text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.10606"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="not stated"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Software, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: RSATalker dataset, Samples: not stated, Modality: speech‚Äìmesh‚Äìimage triplets<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.10054"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/robotic-vision-lab/ueof"><img src="https://img.shields.io/github/stars/robotic-vision-lab/ueof.svg?style=social&label=Star"></a><br><a href="https://robotic-vision-lab.github.io/ueof"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Texas at Arlington<br>
‚Ä¢ Dataset: UEOF (Underwater Event-Based Optical Flow), Samples: 13,714 RGB frames (evaluation intervals), 4.94 billion events, 12 minutes 51 seconds duration, Modality: RGB frames, event streams, dense ground-truth optical flow, camera ego-velocities (6-DoF linear and angular)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>DeTracker: Motion-decoupled Vehicle Detection and Tracking in Unstabilized Satellite Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.09240"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China<br>
‚Ä¢ Dataset: SDM-Car-SU, Samples: 807 video clips (U1: 292, U2: 283, U3: 232), Modality: Satellite video (RGB frames) with vehicle trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>Instance-Aligned Captions for Explainable Video Anomaly Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.08155"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SungKyunKwan University<br>
‚Ä¢ Dataset: VIEW360+, Samples: 1443 videos (868 new videos added), Modality: 360-degree egocentric RGB video with instance segmentation masks and motion attributes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>Motion Focus Recognition in Fast-Moving Egocentric Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.07154"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arazi2/aisends"><img src="https://img.shields.io/github/stars/arazi2/aisends.svg?style=social&label=Star"></a><br><a href="https://arazi2.github.io/aisends"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Clemson University<br>
‚Ä¢ Dataset: egocentric action dataset, Samples: 206 clips, Modality: egocentric RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>eSkiTB: A Synthetic Event-based Dataset for Tracking Skiers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.06647"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/eventbasedvision/eSkiTB"><img src="https://img.shields.io/github/stars/eventbasedvision/eSkiTB.svg?style=social&label=Star"></a><br><a href="https://github.com/eventbasedvision/eSkiTB"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Arizona State University<br>
‚Ä¢ Dataset: eSkiTB (event SkiTB), Samples: 300 sequences, Modality: synthetic event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>UMLoc: Uncertainty-Aware Map-Constrained Inertial Localization with Quantified Bounds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.06602"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/m9alharbi/umloc.git"><img src="https://img.shields.io/github/stars/m9alharbi/umloc.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: King Abdullah University of Science and Technology (KAUST)<br>
‚Ä¢ Dataset: UMLoc dataset, Samples: 60 sequences (7,488 seconds of travel time), Modality: 6-DoF IMU, ground-truth poses, floor-plan maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>SRFlow: A Dataset and Regularization Model for High-Resolution Facial Optical Flow via Splatting Rasterization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.06479"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Guangdong University of Technology<br>
‚Ä¢ Dataset: Splatting Rasterization Flow (SRFlow), Samples: 11161 image pairs, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>Low-Back Pain Physical Rehabilitation by Movement Analysis in Clinical Trial</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.06138"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: U2IS, ENSTA / IP Paris, France<br>
‚Ä¢ Dataset: Keraal dataset, Samples: 2622, Modality: RGB videos, depth maps, 3D skeletal joint positions (Kinect V2), 2D keypoints (OpenPose, BlazePose), Vicon motion capture<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.05844"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pku-mocca.github.io/Dextercap-Page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: DexterHand, Samples: 8 sequences (totaling 4936.65 seconds), Modality: 3D hand and object motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.05237"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://objectforesight.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Washington<br>
‚Ä¢ Dataset: ObjectForesight Dataset, Samples: 2,073,109, Modality: 6-DoF object trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.05138"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sixiaozheng.github.io/VerseCrafter_page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: VerseControl4D, Samples: 36,000 clips, Modality: videos, camera trajectories, 3D Gaussian trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>Detector-Augmented SAMURAI for Long-Duration Drone Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.04798"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.17182190"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for the Protection of Terrestrial Infrastructures, German Aerospace Center (DLR), Sankt Augustin, Germany<br>
‚Ä¢ Dataset: Long-Duration Drone Tracking Dataset, Samples: 4 long-duration sequences (18,932 frames), Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.03713"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China<br>
‚Ä¢ Dataset: BREATH, Samples: 148,738 frames (across 62/66 videos/sequences), Modality: 6-DoF endoscope pose trajectories, RGB video frames, depth, anatomical labels, and 3D airway models<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>Listen to Rhythm, Choose Movements: Autoregressive Multimodal Dance Generation via Diffusion and Mamba with Decoupled Dance Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.03323"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Communication University of China<br>
‚Ä¢ Dataset: Decoupled Dance Dataset, Samples: 87 sequences, Modality: MoCap joints, audio, text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>M-SEVIQ: A Multi-band Stereo Event Visual-Inertial Quadruped-based Dataset for Perception under Rapid Motion and Challenging Illumination</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.02777"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/202510upinlbs-M-SEVIQ-8FA7"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Key Laboratory of Navigation & Location Based Services, Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: M-SEVIQ, Samples: 30+ sequences, Modality: stereo event streams, RGB-D images, IMU data, RTK-GPS, and robot joint encoders (kinematics)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>360DVO: Deep Visual Odometry for Monocular 360-Degree Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.02309"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://chris1004336379.github.io/360DVO-homepage"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Division of Integrative Systems and Design, Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: 360DVO dataset, Samples: 20 sequences, Modality: 360-degree RGB videos and camera trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.01749"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mango-project/mango-project.github.io"><img src="https://img.shields.io/github/stars/mango-project/mango-project.github.io.svg?style=social&label=Star"></a><br><a href="https://mango-project.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: International Digital Economy Academy<br>
‚Ä¢ Dataset: MANGO-Dialog, Samples: 5,000+ dialogue clips (over 50 hours), Modality: 2D-3D aligned conversational data (RGB videos and 3D FLAME motion parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.01528"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://drivinggen-bench.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto<br>
‚Ä¢ Dataset: DrivingGen, Samples: 400, Modality: RGB videos, scene descriptions, and ego trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>Garment Inertial Denoiser (GID): Endowing Accurate Motion Capture via Loose IMU Denoiser</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.01360"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xiamen University<br>
‚Ä¢ Dataset: GarMoCap, Samples: 180 minutes (comprising 20 users for Dupper_GID and 8 users for Dfull_GID), Modality: Loose-wear IMU signals (acceleration and rotation) and MoCap joints (SMPL)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2026</td>
  <td style="width:70%;"><strong>SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2601.00590"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AIGeeksGroup/SafeMo"><img src="https://img.shields.io/github/stars/AIGeeksGroup/SafeMo.svg?style=social&label=Star"></a><br><a href="https://aigeeksgroup.github.io/SafeMo"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Australian National University<br>
‚Ä¢ Dataset: SafeMoVQ-29K, Samples: 17,200 motion clips, Modality: 3D human motion (joints)<br>
‚Ä¢ Dataset: SafeMoVAE-29K, Samples: 17,200 motion clips, Modality: 3D human motion (joints)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.25075"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zheninghuang.github.io/Space-Time-Pilot/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Cambridge<br>
‚Ä¢ Dataset: Cam√óTime, Samples: 180k videos from 500 animations, Modality: RGB videos with full space-time grid rendering<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>World In Your Hands: A Large-Scale and Open-source Ecosystem for Learning Human-centric Manipulation in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.24310"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tars-robotics/World-In-Your-Hands"><img src="https://img.shields.io/github/stars/tars-robotics/World-In-Your-Hands.svg?style=social&label=Star"></a><br><a href="https://wiyh.tars-ai.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TARS Robotics<br>
‚Ä¢ Dataset: World In Your Hands (WiYH), Samples: 125.4k clips / 1045 hours, Modality: RGB videos, depth maps, 3D hand poses, 6D wrist trajectories, IMU localization, tactile signals, camera calibration<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.22979"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Mathematics, Renmin University of China, Beijing, China<br>
‚Ä¢ Dataset: MoCapCube6D, Samples: None, Modality: RGB + Event streams + MoCap ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.22808"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: THU<br>
‚Ä¢ Dataset: Human Reaction Dataset (HRD), Samples: 3500, Modality: egocentric RGB video + 3D human reaction motion (HumanML3D representation)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.22324"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jiro-zhang.github.io/DeMoGen/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ReLER, AAII, University of Technology Sydney<br>
‚Ä¢ Dataset: DecompML, Samples: 87,384 decomposed text groups (174,768 sentences total), Modality: Text annotations for motion decomposition (extension of HumanML3D)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Human Motion Estimation with Everyday Wearables</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.21209"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pie-lab.cn/EveryWear/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing Institute of Technology<br>
‚Ä¢ Dataset: Ego-Elec, Samples: 2.88M frames (9 hours), Modality: RGB images from 3 egocentric cameras + IMU signals from consumer devices (smartphone, smartwatch, earbuds) + MoCap ground-truth 3D annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.20847"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/paragkhanna1/YCB-Handovers"><img src="https://img.shields.io/github/stars/paragkhanna1/YCB-Handovers.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: YCB-Handovers, Samples: 2771, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.19551"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Soochow University, Suzhou, China<br>
‚Ä¢ Dataset: L2-EMG Dataset, Samples: 19916, Modality: 3D human motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.19159"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://OmniMoGen.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: X2Mo, Samples: 137,000, Modality: MoCap joints<br>
‚Ä¢ Dataset: AnyContext, Samples: null, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.18814"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yuxiaoyang23.github.io/EchoMotion-webpage/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: HuMoVe, Samples: 80,000, Modality: SMPL motion parameters, RGB videos, text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.18619"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/zzhou292/DreamerBench"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Wisconsin‚ÄìMadison<br>
‚Ä¢ Dataset: DreamerBench, Samples: , Modality: RGB frames, contact splat images, joint angles, physics annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.16907"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://egoman-project.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta<br>
‚Ä¢ Dataset: EgoMAN dataset, Samples: 219,000 6-DoF trajectories, Modality: 6-DoF wrist trajectories (3D position + 6D rotation), egocentric RGB videos, interaction stage annotations, 3M QA pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.16727"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://omg-bench.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sun Yat-sen University<br>
‚Ä¢ Dataset: OMG-Bench, Samples: 13,948 instances across 1,272 sequences, Modality: 21-joint hand skeleton<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.16456"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/masashi-hatano/prime-and-reach"><img src="https://img.shields.io/github/stars/masashi-hatano/prime-and-reach.svg?style=social&label=Star"></a><br><a href="https://masashi-hatano.github.io/prime-and-reach/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Keio University, Japan<br>
‚Ä¢ Dataset: Prime and Reach (P&R) sequences, Samples: 23,728, Modality: 3D human pose (SMPL-H, SMPL-X, and 3D skeletons)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.16199"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Waterloo<br>
‚Ä¢ Dataset: Syn2Sport, Samples: 21203, Modality: 4D human motion sequences (SMPL-X, 2D/3D poses)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.16019"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="omitted for blind review"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Yale University<br>
‚Ä¢ Dataset: SEAN TOGETHER v2, Samples: 404, Modality: robot and human motion trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Emotion Recognition in Signers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.15376"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: FIRST, Institute of Integrated Research, Institute of Science Tokyo<br>
‚Ä¢ Dataset: eJSL, Samples: 1092, Modality: RGB video<br>
‚Ä¢ Dataset: BOBSL-A_TEA, Samples: 113826, Modality: RGB video<br>
‚Ä¢ Dataset: BOBSL-M_C, Samples: 930, Modality: RGB video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.14938"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zhenzhiwang.github.io/talkverse/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: TalkVerse, Samples: 2,300,000 clips, Modality: RGB videos, 2D pose skeletons (DWPose), speech audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.14595"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Chair of Robotics, Artificial Intelligence and Real-time Systems, Technical University of Munich, Munich, Germany<br>
‚Ä¢ Dataset: TUMTraf EMOT, Samples: 6 event sequences (comprising 54M bounding boxes and approximately 450M events), Modality: Event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.14056"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://facedit.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: POSTECH<br>
‚Ä¢ Dataset: FacEDiTBench, Samples: 250, Modality: RGB video and audio pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Robust Motion Generation using Part-level Reliable Data from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.12703"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Renmin University of China<br>
‚Ä¢ Dataset: K700-M, Samples: 198,627, Modality: 3D human motion sequences reconstructed from web videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.12378"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanyang Technological University<br>
‚Ä¢ Dataset: M4Human, Samples: 661K frames (999 sequences), Modality: mmWave radar (raw tensors and point clouds), RGB, depth, and MoCap (3D meshes, skeletons, and global trajectories)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>KeyframeFace: From Text to Expressive Facial Keyframes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.11321"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wjc12345123/KeyframeFace"><img src="https://img.shields.io/github/stars/wjc12345123/KeyframeFace.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Westlake University<br>
‚Ä¢ Dataset: KeyframeFace, Samples: 2,100 motion sequences (expressive scripts paired with monocular videos), Modality: ARKit-based blendshape coefficients (61-dimensional), monocular videos, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.10956"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Wentao-Zhou/StereoWalker"><img src="https://img.shields.io/github/stars/Wentao-Zhou/StereoWalker.svg?style=social&label=Star"></a><br><a href="https://www.cs.virginia.edu/~tsx4zn/stereowalk/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Virginia<br>
‚Ä¢ Dataset: DIVERCITY, Samples: 500 clips, Modality: Stereo RGB videos and trajectory labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.10945"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://henghuiding.com/MeViS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University, China<br>
‚Ä¢ Dataset: MeViS (MeViSv2), Samples: 2,006 videos, 8,171 objects, 33,072 human-annotated motion expressions, Modality: RGB videos, segmentation masks, bounding box trajectories, text expressions, audio expressions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.10927"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wolfv0/FoundationMotionDataset"><img src="https://img.shields.io/github/stars/wolfv0/FoundationMotionDataset.svg?style=social&label=Star"></a><br><a href="https://wolfv0.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MIT<br>
‚Ä¢ Dataset: FoundationMotion Dataset, Samples: 467000, Modality: RGB videos and bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Topology-Agnostic Animal Motion Generation from Text Prompt</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.10352"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua Shenzhen International Graduate School, China<br>
‚Ä¢ Dataset: OmniZoo, Samples: 32,979 sequences, Modality: text, video, skeletons, and meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.10321"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Seoul National University<br>
‚Ä¢ Dataset: MVPose3D, Samples: 215,039 frames, Modality: dense multi-view point clouds, synchronized IMUs, multi-view RGB images, 3D joint coordinates, and joint rotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>MotionEdit: Benchmarking and Learning Motion-Centric Image Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.10284"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motion-edit.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tencent AI, Seattle<br>
‚Ä¢ Dataset: MotionEdit, Samples: 10,157, Modality: RGB image pairs (input, edit instruction, ground truth target) extracted from video frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Visual Heading Prediction for Autonomous Aerial Vehicles</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.09898"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Kooroshraf/UAV-UGV-Integration"><img src="https://img.shields.io/github/stars/Kooroshraf/UAV-UGV-Integration.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: North Carolina A&T State University<br>
‚Ä¢ Dataset: UAV-UGV coordination dataset, Samples: over 13,000 annotated images, Modality: monocular camera images synchronized with VICON 6DOF motion capture data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Beyond Sequences: A Benchmark for Atomic Hand-Object Interaction Using a Static RNN Encoder</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.09626"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/YousefAMovahed/beyond-sequences-hoi-benchmark"><img src="https://img.shields.io/github/stars/YousefAMovahed/beyond-sequences-hoi-benchmark.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Tehran<br>
‚Ä¢ Dataset: Atomic HOI Benchmark, Samples: 27,476, Modality: statistical-kinematic feature vectors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.09417"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MBZUAI<br>
‚Ä¢ Dataset: HeadSwapBench, Samples: 8,566 videos (8,066 training and 500 test videos), Modality: RGB videos with synchronized facial poses and expressions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.09335"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Yonsei University<br>
‚Ä¢ Dataset: MvMi dataset (Multi-view Multi-illuminated dataset), Samples: 11.5 million frames, Modality: Multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.08765"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ali-vilab/Wan-Move"><img src="https://img.shields.io/github/stars/ali-vilab/Wan-Move.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tongyi Lab, Alibaba Group<br>
‚Ä¢ Dataset: MoveBench, Samples: 1018 videos, Modality: RGB videos + point trajectories + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.07500"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Information Science and Technology, Beijing University of Technology, Beijing, China<br>
‚Ä¢ Dataset: MultiMotionEval, Samples: 103, Modality: RGB videos + instance-level masks + trajectory annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Benchmarking Humanoid Imitation Learning with Motion Difficulty</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.07248"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xiamen University<br>
‚Ä¢ Dataset: MD-AMASS, Samples: over 30,000, Modality: MoCap joints (SMPL)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Unified Camera Positional Encoding for Controlled Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.07237"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chengzhag/UCPE"><img src="https://img.shields.io/github/stars/chengzhag/UCPE.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Monash University<br>
‚Ä¢ Dataset: camera-diverse video dataset, Samples: 48,000, Modality: RGB videos + 6-DoF camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.05277"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huawei Technologies Canada Co., Ltd.<br>
‚Ä¢ Dataset: Temporal Understanding in Autonomous Driving (TAD), Samples: 4,481 segment-level vehicle action annotations, Modality: RGB videos + vehicle trajectories + vehicle action labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>From Generated Human Videos to Physically Plausible Robot Trajectories</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.05094"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/genmimic/genmimic"><img src="https://img.shields.io/github/stars/genmimic/genmimic.svg?style=social&label=Star"></a><br><a href="https://genmimic.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Berkeley<br>
‚Ä¢ Dataset: GenMimicBench, Samples: 428, Modality: Generated human-action RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>BulletTime: Decoupled Control of Time and Camera Pose for Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.05076"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/19reborn/Bullet4D"><img src="https://img.shields.io/github/stars/19reborn/Bullet4D.svg?style=social&label=Star"></a><br><a href="https://19reborn.github.io/Bullet4D/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Zurich<br>
‚Ä¢ Dataset: 4D-controlled synthetic dataset, Samples: 20,000 videos, Modality: RGB videos with camera trajectories, world-time labels, and MoCap-driven character motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.05044"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Zhangyr2022/MoRe4D"><img src="https://img.shields.io/github/stars/Zhangyr2022/MoRe4D.svg?style=social&label=Star"></a><br><a href="https://ivg-yanranzhang.github.io/MoRe4D/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: TrajScene-60K, Samples: 60,000, Modality: 4D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.04862"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://biotuch.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems<br>
‚Ä¢ Dataset: BioTUCH dataset, Samples: 91 sequences (82 contact gestures, 9 adversarial non-contact gestures), 19,183 contact frames, Modality: Synchronized RGB video, bioimpedance measurements, and 3D motion-capture (SMPL-X meshes)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.04813"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lucywang720/MOVE"><img src="https://img.shields.io/github/stars/lucywang720/MOVE.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: MOVE, Samples: 75,000 timesteps, Modality: RGB videos + robot actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.04537"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://showlab.github.io/X-Humanoid/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National University of Singapore<br>
‚Ä¢ Dataset: Synthetic Paired Human-Humanoid Video Dataset, Samples: 11,172 pairs (17+ hours), Modality: RGB videos<br>
‚Ä¢ Dataset: Robotized Ego-Exo4D, Samples: 3.6 million frames (60+ hours), Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Look Around and Pay Attention: Multi-camera Point Tracking Reimagined with Transformers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.04213"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ostadabbas/Look-Around-and-Pay-Attention-LAPA-"><img src="https://img.shields.io/github/stars/ostadabbas/Look-Around-and-Pay-Attention-LAPA-.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northeastern University<br>
‚Ä¢ Dataset: TAPVid-3D-MC, Samples: None, Modality: Multi-view RGB videos + 3D point trajectories + Camera calibration<br>
‚Ä¢ Dataset: PointOdyssey-MC, Samples: None, Modality: Multi-view RGB videos + 3D point trajectories + Camera calibration<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>PerFACT: Motion Policy with LLM-Powered Dataset Synthesis and Fusion Action-Chunking Transformers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.03444"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Texas A&M University<br>
‚Ä¢ Dataset: PerFACT planning dataset, Samples: 3,500,000 trajectories, Modality: robot joint trajectories, point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.03000"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dynamic-verse.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xiamen University<br>
‚Ä¢ Dataset: DynamicVerse, Samples: 100,000+ videos, Modality: RGB videos, metric-scale point maps, camera parameters, object masks, and hierarchical captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Taming Camera-Controlled Video Generation with Verifiable Geometry Reward</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.02870"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AIsphere<br>
‚Ä¢ Dataset: CAMVERSE dataset, Samples: 315,000, Modality: RGB videos + camera trajectories (3D camera poses)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.02648"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JHXion9/PoreTrack3D"><img src="https://img.shields.io/github/stars/JHXion9/PoreTrack3D.svg?style=social&label=Star"></a><br><a href="https://github.com/JHXion9/PoreTrack3D"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Automation, Guangdong University of Technology, Guangzhou, China<br>
‚Ä¢ Dataset: PoreTrack3D, Samples: 440,000 facial trajectories (from 128 multi-view sequences), Modality: 3D point trajectories (facial landmarks and pore-scale keypoints)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.01582"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology Beijing<br>
‚Ä¢ Dataset: RoleMotion, Samples: 10296, Modality: MoCap joints (body and hands)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2025</td>
  <td style="width:70%;"><strong>Modality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2512.01358"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Seoul National University<br>
‚Ä¢ Dataset: Unitree G1 Multi-Modal Dataset (Pick Apple to Bowl), Samples: 5000 demonstrations, Modality: RGB-D, proprioception (joint positions, velocities, end-effector poses), joint-space reference trajectories, fingertip and palm contact-force vectors<br>
‚Ä¢ Dataset: Augmented Fourier GR1 Dataset, Samples: not stated, Modality: RGB-D, proprioception, binary contact signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2025</td>
  <td style="width:70%;"><strong>OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2511.03571"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MasterHow/OneOcc"><img src="https://img.shields.io/github/stars/MasterHow/OneOcc.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ZJU<br>
‚Ä¢ Dataset: QuadOcc, Samples: 24000, Modality: panoramic RGB, semantic occupancy (from LiDAR aggregation), pose<br>
‚Ä¢ Dataset: Human360Occ (H3O), Samples: 8000, Modality: simulated panoramic RGB, metric depth, semantic occupancy, pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2025</td>
  <td style="width:70%;"><strong>SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2511.03325"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/madratak/SurgViVQA"><img src="https://img.shields.io/github/stars/madratak/SurgViVQA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano, Italy.<br>
‚Ä¢ Dataset: REAL-Colon-VQA, Samples: 5200, Modality: colonoscopic RGB videos + temporal motion cues (e.g., scope movement, lesion motion, tool-tissue interactions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2025</td>
  <td style="width:70%;"><strong>EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2511.02953"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Surrey<br>
‚Ä¢ Dataset: EvtSlowTV, Samples: 40, Modality: event camera streams (synthetic from RGB videos)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2025</td>
  <td style="width:70%;"><strong>TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2511.02832"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/YanjieZe/TWIST2"><img src="https://img.shields.io/github/stars/YanjieZe/TWIST2.svg?style=social&label=Star"></a><br><a href="https://twist-data.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Amazon FAR<br>
‚Ä¢ Dataset: TWIST2 Dataset, Samples: None, Modality: whole-body humanoid teleoperation trajectories, egocentric stereo vision, proprioception<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2025</td>
  <td style="width:70%;"><strong>Estimation of Segmental Longitudinal Strain in Transesophageal Echocardiography by Deep Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2511.02210"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://kiss.folk.ntnu.no/jbhi/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Faculty of Information Technology and Electrical Engineering, Norwegian University of Science and Technology, 7034 Trondheim, Norway<br>
‚Ä¢ Dataset: synTEE, Samples: 240, Modality: synthetic TEE B-mode videos + ground truth myocardial motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2025</td>
  <td style="width:70%;"><strong>Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2511.01502"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="mias.group/DiMoDE"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University, Shanghai 201210, China<br>
‚Ä¢ Dataset: MIAS-Odom, Samples: 11608, Modality: RGB videos + LiDAR + IMU + camera trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2025</td>
  <td style="width:70%;"><strong>Web-Scale Collection of Video Data for 4D Animal Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2511.01169"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/briannlongzhao/Animal-in-Motion"><img src="https://img.shields.io/github/stars/briannlongzhao/Animal-in-Motion.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: Animal-in-Motion (AiM), Samples: 230, Modality: RGB videos + instance masks + keypoints + optical flow + depth maps + occlusion boundaries + DINO features<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2025</td>
  <td style="width:70%;"><strong>OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2511.00510"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xifen523/OmniTrack"><img src="https://img.shields.io/github/stars/xifen523/OmniTrack.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence and Robotics, Hunan University, Changsha 410012, China<br>
‚Ä¢ Dataset: EmboTrack, Samples: 44, Modality: panoramic RGB videos with object trajectories<br>
‚Ä¢ Dataset: QuadTrack, Samples: 32, Modality: panoramic RGB videos with object trajectories from quadruped robot<br>
‚Ä¢ Dataset: BipTrack, Samples: 12, Modality: panoramic RGB videos with object trajectories from bipedal wheel-legged robot<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2025</td>
  <td style="width:70%;"><strong>Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2511.00503"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://paulpanwang.github.io/Diff4Splat"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: Stereo4D, Samples: 74000, Modality: RGB videos + metric depth + point motion trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>DANCER: Dance ANimation via Condition Enhancement and Rendering with diffusion model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.27169"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stony Brook University, New York, USA<br>
‚Ä¢ Dataset: TikTok-3K, Samples: 3000, Modality: RGB videos + pose sequences (segmentation maps, depth maps, normal maps, skeleton maps)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>The Quest for Generalizable Motion Generation: Data, Model, and Evaluation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.26794"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanyang Technological University<br>
‚Ä¢ Dataset: ViMoGen-228K, Samples: 228236, Modality: 3D human motion sequences (SMPL-X) with text annotations, derived from optical MoCap, in-the-wild RGB videos, and synthetic videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>PHUMA: Physically-Grounded Humanoid Locomotion Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.26236"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://davian-robotics.github.io/PHUMA"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST<br>
‚Ä¢ Dataset: PHUMA, Samples: 76010, Modality: MoCap joints and video-derived motion (SMPL-X)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Robotic Assistant: Completing Collaborative Tasks with Dexterous Vision-Language-Action Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.25713"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: nan<br>
‚Ä¢ Dataset: Collaborative Robot Manipulation Dataset, Samples: 340, Modality: RGB videos, robot kinematics, human hand pose (MoCap), text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Towards Fine-Grained Human Motion Video Captioning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.24767"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University, Shenzhen, China<br>
‚Ä¢ Dataset: Human Motion Insight (HMI) Dataset, Samples: 115000, Modality: RGB videos + human mesh recovery (SMPL-based) + ViTPose keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.24231"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://waseemshariff126.github.io/microsaccades/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: C3I Imaging Lab, School of Engineering, University of Galway, Ireland<br>
‚Ä¢ Dataset: Synthetic Microsaccade Dataset, Samples: 175000, Modality: Event-based eye movement sequences (simulated from Blender RGB + v2e event conversion)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.24117"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pie-lab.cn/DogMo/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science & Technology, Beijing Institute of Technology<br>
‚Ä¢ Dataset: DogMo, Samples: 1200, Modality: multi-view RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.23842"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northeastern University<br>
‚Ä¢ Dataset: ASL STEM Dialogue Motion Capture Dataset, Samples: 2, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Awakening Facial Emotional Expressions in Human-Robot</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.23059"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Machine Intelligence, University of Shanghai for Science and Technology, Shanghai 200093, China<br>
‚Ä¢ Dataset: Rena Facial Database, Samples: 9000, Modality: RGB videos / images + servomotor commands<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>DynamicTree: Interactive Real Tree Animation via Sparse Voxel Spectrum</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.22213"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dynamictree-dev/DynamicTree.github.io"><img src="https://img.shields.io/github/stars/dynamictree-dev/DynamicTree.github.io.svg?style=social&label=Star"></a><br><a href="https://dynamictree-dev.github.io/DynamicTree.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sun Yat-sen University<br>
‚Ä¢ Dataset: 4DTree, Samples: 8786, Modality: animated mesh sequences with semantic labels and 100-frame motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>MOGRAS: Human Motion with Grasping in 3D Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.22199"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://kunal-kamalkishor-bhosikar.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Machine Learning Lab, International Institute of Information Technology, Hyderabad, India<br>
‚Ä¢ Dataset: MOGRAS, Samples: 14238, Modality: full-body motion sequences (SMPL-X poses) in 3D scenes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Addressing Corner Cases in Autonomous Driving: A World Model-based Approach with Mixture of Experts and LLMs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.21867"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau SAR, China<br>
‚Ä¢ Dataset: nuScenes-corner, Samples: 3392, Modality: vehicle trajectories, HD maps, BEV features, contextual traffic scenes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Gestura: A LVLM-Powered System Bridging Motion and Semantics for Real-Time Free-Form Gesture Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.21814"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://evans-lx.github.io/Gestura/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Artificial Intelligence (TeleAI) of China Telecom, China<br>
‚Ä¢ Dataset: GestureInt, Samples: 159561, Modality: RGB videos (egocentric and exocentric) with hand landmarks (MediaPipe)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Group Inertial Poser: Multi-Person Pose and Global Translation from Sparse Inertial Sensors and Ultra-Wideband Ranging</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.21654"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="github.com/eth-siplab/GroupInertialPoser"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, ETH Z√ºrich, Switzerland<br>
‚Ä¢ Dataset: GIP-DB, Samples: 200, Modality: IMU signals, UWB-based distance measurements, SMPL body motion parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.21571"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://microsoft.github.io/VITRA/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: hand-VLA, Samples: 1000000, Modality: egocentric RGB videos + 3D hand motion + camera motion + language instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.20126"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UL Lafayette<br>
‚Ä¢ Dataset: Racquetball RGB-D dataset, Samples: 12, Modality: RGB videos + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Is This Tracker On? A Benchmark Protocol for Dynamic Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.19819"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ilonadem/itto"><img src="https://img.shields.io/github/stars/ilonadem/itto.svg?style=social&label=Star"></a><br><a href="https://glab-caltech.github.io/ITTO/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: California Institute of Technology<br>
‚Ä¢ Dataset: ITTO, Samples: 72, Modality: RGB videos + point tracks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.19789"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/GuoweiXu368/OmniMocap-X"><img src="https://img.shields.io/github/stars/GuoweiXu368/OmniMocap-X.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: OmniMoCap-X, Samples: 64300000, Modality: MoCap joints (SMPL-X format)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.17101"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yinlu5942/SAIP"><img src="https://img.shields.io/github/stars/yinlu5942/SAIP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xiamen University, China<br>
‚Ä¢ Dataset: Multi-shape Inertial MoCap Dataset (MID), Samples: 1500000.0, Modality: IMU data (acceleration, angular velocity, orientation) and motion data (SMPL joint rotations and global root position)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.16833"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/MML-Group/M2HVideo-data"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Harbin Institute of Technology, Shenzhen, Xili University Town, Shenzhen 518055, China<br>
‚Ä¢ Dataset: MannequinVideos, Samples: 24, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.16258"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.meta.com/emerging-tech/codec-avatars/embody-3d"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Codec Avatars Lab, Meta<br>
‚Ä¢ Dataset: Embody 3D, Samples: 54000000, Modality: 3D motion (full body and hand tracking, body shape in SMPL-X), audio, text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>DexCanvas: Bridging Human Demonstrations and Robot Learning for Dexterous Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.15786"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dexrobot/dexcanvas"><img src="https://img.shields.io/github/stars/dexrobot/dexcanvas.svg?style=social&label=Star"></a><br><a href="https://huggingface.co/datasets/DEXROBOT/DexCanvas"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DexRobot Co. Ltd.<br>
‚Ä¢ Dataset: DexCanvas, Samples: 3000000000, Modality: MoCap joints, RGB-D videos, force vectors, contact points, object poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Valeo Near-Field: a novel dataset for pedestrian intent detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.15673"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Valeo, BRAIN Division, 6 rue Daniel Costantini 94000 Cr√©teil - France<br>
‚Ä¢ Dataset: Valeo Near-Field (VNF), Samples: 300, Modality: fisheye camera feeds, LiDAR laser scans, ultrasonic sensor readings, motion capture-based 3D body poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Iterative Motion Compensation for Canonical 3D Reconstruction from UAV Plant Images Captured in Windy Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.15491"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Autonomous Intelligent Systems - Computer Science Institute VI and Center for Robotics, University of Bonn, Germany<br>
‚Ä¢ Dataset: UAV Plant Imaging Dataset, Samples: 8544, Modality: RGB videos + optical flow + camera poses + ArUco markers<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>MAVR-Net: Robust Multi-View Learning for MAV Action Recognition with Cross-View Attention</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.15448"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/iAerialRobo/MAVR-Net.git"><img src="https://img.shields.io/github/stars/iAerialRobo/MAVR-Net.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Aerospace Engineering, Universiti Sains Malaysia, 14300 Nibong Tebal, Pulau Pinang, Malaysia.<br>
‚Ä¢ Dataset: MAVR-Net Dataset, Samples: 4500, Modality: RGB videos + optical flow + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>RealDPO: Real or Not Real, that is the Preference</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.14955"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://Vchitect.github.io/RealDPO-Project"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: RealAction-5K, Samples: 5000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.13729"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lifmcr.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich, Munich, Germany<br>
‚Ä¢ Dataset: LiFMCR, Samples: 7, Modality: synchronized light field image sequences + 6-DoF pose trajectories (Vicon motion capture)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.12703"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Engineering ‚ÄúEnzo Ferrari‚Äù University of Modena and Reggio Emilia<br>
‚Ä¢ Dataset: CAM-based Dataset, Samples: 16051, Modality: Cooperative Awareness Messages (CAMs) including position, speed, and heading<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>MMOT: The First Challenging Benchmark for Drone-based Multispectral Multi-Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.12565"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Annzstbl/MMOT"><img src="https://img.shields.io/github/stars/Annzstbl/MMOT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing Institute of Technology<br>
‚Ä¢ Dataset: MMOT, Samples: 125, Modality: multispectral video sequences (8-band, visible to near-infrared) with oriented bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.10976"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ByteDance<br>
‚Ä¢ Dataset: STV-205k, Samples: 205000, Modality: RGB videos + object bounding boxes + spatio-temporal annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>BurstDeflicker: A Benchmark Dataset for Flicker Removal in Dynamic Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.09996"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/qulishen/BurstDeflicker"><img src="https://img.shields.io/github/stars/qulishen/BurstDeflicker.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nankai International Advanced Research Institute (SHENZHEN¬∑FUTIAN)<br>
‚Ä¢ Dataset: BurstDeflicker, Samples: 3690, Modality: RGB videos with flicker artifacts and motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Advancing Intoxication Detection: A Smartwatch-Based Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.09916"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California Irvine<br>
‚Ä¢ Dataset: Smartwatch-Based Intoxication Detection Dataset, Samples: 30, Modality: accelerometer, gyroscope, heart rate, TAC<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Online IMU-odometer Calibration using GNSS Measurements for Autonomous Ground Vehicle Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.08880"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="not provided"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Department of Aeronautical and Aviation Engineering, Hong Kong Polytechnic University<br>
‚Ä¢ Dataset: not specified, Samples: not specified, Modality: IMU + 2D odometer + raw GNSS (rover and base stations)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.08807"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://humanoideveryday.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Southern California<br>
‚Ä¢ Dataset: Humanoid Everyday, Samples: 10300, Modality: RGB + depth + LiDAR + tactile + joint poses + joint actions + human poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.07249"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://talkcuts.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UMass Amherst<br>
‚Ä¢ Dataset: TalkCuts, Samples: 164000, Modality: RGB videos + 2D pose keypoints + 3D SMPL-X motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Stable Robot Motions on Manifolds: Learning Lyapunov-Constrained Neural Manifold ODEs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.05707"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://drive.google.com/file/d/1WZlMBKk4kJngwMAxgGIs09xvDWu2HSgU/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer and Information Sciences University of Konstanz<br>
‚Ä¢ Dataset: Improved Riemannian LASA Datasets, Samples: 120 sequences (4 demonstrations for each of 30 LASA shapes), Modality: Motion trajectories on Riemannian manifolds (S3 and S2++)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Pulp Motion: Framing-aware multimodal camera and human motion generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.05097"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: LIX, √âcole Polytechnique, CNRS, IPP<br>
‚Ä¢ Dataset: PulpMotion, Samples: 193000, Modality: 3D human motion and camera trajectories with textual captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.04312"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/TaatiTeam/CARE-PD/"><img src="https://img.shields.io/github/stars/CARE-PD/.svg?style=social&label=Star"></a><br><a href="https://neurips2025.care-pd.ca"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto, Vector Institute, KITE Research Institute-UHN<br>
‚Ä¢ Dataset: CARE-PD, Samples: 8477, Modality: 3D SMPL mesh<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Learning Efficient Meshflow and Optical Flow from Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.04111"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/boomluo02/EEMFlowPlus"><img src="https://img.shields.io/github/stars/boomluo02/EEMFlowPlus.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: HREM (High-Resolution Event Meshflow), Samples: 20,000 train samples, 8,000 test samples, Modality: Event streams + meshflow + optical flow<br>
‚Ä¢ Dataset: HREM+, Samples: 20,000 train samples, 8,000 test samples, Modality: Multi-density event streams + meshflow + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.03874"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China<br>
‚Ä¢ Dataset: DHQA-4D, Samples: 1920, Modality: dynamic textured/non-textured 4D meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>Creative synthesis of kinematic mechanisms</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.03308"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/CreativeMachinesLab/GenMech"><img src="https://img.shields.io/github/stars/CreativeMachinesLab/GenMech.svg?style=social&label=Star"></a><br><a href="https://jl6017.github.io/GenMech/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Columbia University<br>
‚Ä¢ Dataset: Planar Linkages Dataset, Samples: 850000, Modality: RGB images of planar mechanisms and corresponding motion curves<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.02722"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JunyuShi02/MoGIC"><img src="https://img.shields.io/github/stars/JunyuShi02/MoGIC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: Mo440H, Samples: 210000, Modality: Motion sequences (22-joint format), Textual descriptions, Image sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.00806"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: Trajectory Dataset (proposed TrajVLM-Gen dataset), Samples: 1300000, Modality: image-video-trajectory pairs (RGB videos + bounding box trajectories)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2025</td>
  <td style="width:70%;"><strong>AI-Based Stroke Rehabilitation Domiciliary Assessment System with ST_GCN Attention</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2510.00049"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LimSuH/NRC-rehab"><img src="https://img.shields.io/github/stars/LimSuH/NRC-rehab.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Gachon University<br>
‚Ä¢ Dataset: NRC, Samples: 1142, Modality: RGB-D videos with skeleton keypoints (25 joints)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Benchmarking Egocentric Visual-Inertial SLAM at City Scale</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.26639"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="lamaria.ethz.ch"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Zurich<br>
‚Ä¢ Dataset: LaMAria, Samples: 63, Modality: Egocentric visual-inertial data (multiple grayscale cameras, RGB camera, IMUs, magnetometer, barometer, GNSS, WiFi/Bluetooth) with centimeter-accurate pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.26636"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SafeRL-Lab/AccidentBench"><img src="https://img.shields.io/github/stars/SafeRL-Lab/AccidentBench.svg?style=social&label=Star"></a><br><a href="https://github.com/SafeRL-Lab/AccidentBench"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: AccidentBench, Samples: 2000, Modality: RGB videos + question-answer pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.26633"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://omniretarget.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MIT / Amazon FAR<br>
‚Ä¢ Dataset: OMNIRETARGET retargeted dataset, Samples: Over 8 hours of trajectories (from OMOMO, LAFAN1, and in-house MoCap), Modality: Retargeted robot kinematic trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>A Multi-purpose Tracking Framework for Salmon Welfare Monitoring in Challenging Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.25969"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/espenbh/BoostCompTrack"><img src="https://img.shields.io/github/stars/espenbh/BoostCompTrack.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Norwegian University of Science and Technology, Norway<br>
‚Ä¢ Dataset: CrowdedSalmon (CS), Samples: 1010 salmon boxes in 31 training images, 871 salmon boxes in 6 validation images, Modality: RGB videos with bounding box annotations for salmon and body parts<br>
‚Ä¢ Dataset: TurningSalmon (TS), Samples: 679 salmon boxes in 8 training images, 146 salmon in 100 validation frames, Modality: RGB videos with bounding box annotations for salmon and body parts<br>
‚Ä¢ Dataset: TailbeatWavelength (TBW), Samples: 1529 salmon boxes in 46 training images, 18 salmon in 1000 validation frames, Modality: RGB videos with bounding box annotations and tail beat extrema labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.24997"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yuyangyin.github.io/PanoWorld-X/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing Jiaotong University<br>
‚Ä¢ Dataset: PanoExplorer, Samples: 116759, Modality: 360¬∞ panoramic videos with 3D exploration routes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Event-based Facial Keypoint Alignment via Cross-Modal Fusion Attention and Self-Supervised Multi-Event Representation Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.24968"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Electrical Engineering, Hongik University, Seoul 04066, South Korea<br>
‚Ä¢ Dataset: E-SIE, Samples: 720, Modality: RGB videos + Event camera streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Biomechanical-phase based Temporal Segmentation in Sports Videos: a Demonstration on Javelin-Throw</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.24606"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Bikudebug/Javelin_Throw_Dataset"><img src="https://img.shields.io/github/stars/Bikudebug/Javelin_Throw_Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Physics, Indian Institute of Technology Gandhinagar<br>
‚Ä¢ Dataset: Javelin Throw Dataset, Samples: 211, Modality: RGB videos with frame-level annotations (4 phases) and 2D pose sequences (16-joint skeletons in MPII format)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Understanding Cognitive States from Head & Hand Motion Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.24255"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Illinois Institute of Technology<br>
‚Ä¢ Dataset: VR Head and Hand Motion Dataset with Cognitive State Annotations, Samples: None, Modality: VR head and hand motion trajectories (18DOF), First-person video, Frame-level cognitive state annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>AssemblyHands-X: Modeling 3D Hand-Body Coordination for Understanding Bimanual Human Activities</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.23888"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo, Tokyo, Japan<br>
‚Ä¢ Dataset: AssemblyHands-X, Samples: None, Modality: Multi-view RGB videos + 3D keypoint poses + SMPL-X parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>InteractMove: Text-Controlled Human-Object Interaction Generation in 3D Scenes with Movable Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.23612"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Cxhcmhhh/InteractMove"><img src="https://img.shields.io/github/stars/Cxhcmhhh/InteractMove.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Wangxuan Institute of Computer Technology, Peking University<br>
‚Ä¢ Dataset: InteractMove, Samples: 30500, Modality: 3D motion sequences + object trajectories + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Developing Vision-Language-Action Model from Egocentric Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.21986"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kyoto University<br>
‚Ä¢ Dataset: Not explicitly named, referred to as 'our dataset', Samples: 45157, Modality: RGB videos + 6DoF object pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.21919"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://reinliu.github.io/text2move/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Sydney<br>
‚Ä¢ Dataset: Synthetic Moving Sound Dataset, Samples: 76850, Modality: Binaural audio + 3D spatial trajectories + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.21086"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yu-shaonian.github.io/UniTransfer-Web/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Lab of CAD&CG, Zhejiang University<br>
‚Ä¢ Dataset: OpenAnimal, Samples: 10000, Modality: RGB videos (single-animal sequences with diverse motion patterns)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>AI-Enabled Crater-Based Navigation for Lunar Mapping</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.20748"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Adelaide, Australian Institute for Machine Learning<br>
‚Ä¢ Dataset: CRESENT-365, Samples: 15283, Modality: Synthetic lunar surface images with ground truth poses for crater-based navigation<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.20358"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cwchenwang.github.io/physctrl"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Pennsylvania<br>
‚Ä¢ Dataset: Physics Simulation Dataset, Samples: 550000, Modality: 3D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.18566"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Optical Science and Engineering, Zhejiang University, China<br>
‚Ä¢ Dataset: ZJU-MoCap-Blur, Samples: 6, Modality: RGB videos with simulated motion blur<br>
‚Ä¢ Dataset: MMHPSD-Blur, Samples: 6, Modality: RGB videos with simulated motion blur<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.18455"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="geodex2p.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Thomas Lord Department of Computer Science at the University of Southern California, USA<br>
‚Ä¢ Dataset: GD2P Dataset, Samples: 1300000, Modality: Hand poses, object point clouds, basis point set representations, physics simulation validation data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.18387"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cogsys-tuebingen.github.io/blurball/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Tuebingen<br>
‚Ä¢ Dataset: Table tennis ball detection dataset, Samples: 64119, Modality: RGB videos with ball position, orientation, and blur length annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.17513"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mediax-sjtu.github.io/4DGCPro"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Cooperative Medianet Innovation Center, Shanghai Jiaotong University<br>
‚Ä¢ Dataset: 4DGCPro dataset, Samples: None, Modality: RGB videos (81 synchronized Z-CAM cinema cameras, 3840√ó2160)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.17168"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beihang University<br>
‚Ä¢ Dataset: HAGE, Samples: ~2.5 hours of curated recordings from 8 subjects, Modality: synchronized 16kHz audio, binocular gaze, 3D head rotations, 1080 √ó1080 video, 3D facial parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.14636"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WeiYuFei0217/ZJH-VO-Dataset/"><img src="https://img.shields.io/github/stars/ZJH-VO-Dataset/.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: ZJH-VO Multi-Scale Dataset, Samples: 12 trajectories with 12,666 frames, Modality: Monocular RGB videos, 2D LiDAR ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.14063"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Georgia Institute of Technology, School of Aerospace Engineering<br>
‚Ä¢ Dataset: 7daysJune subset (of TartanAviation dataset), Samples: None, Modality: Aircraft trajectory (ADS-B data) + audio radio calls (CTAF)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Towards Context-Aware Human-like Pointing Gestures with RL Motion Imitation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.12880"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Division of Speech, Music and Hearing, KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: Human Pointing Gestures Dataset, Samples: 83, Modality: Optical MoCap joints + hand motion capture gloves + head-mounted iPhone face/voice capture<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.12430"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Mechanical Engineering, Purdue University, USA<br>
‚Ä¢ Dataset: MechBench, Samples: 693, Modality: CAD point clouds, SE(3) motion trajectories, twist vectors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.11323"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SongJgit/filternet, https://github.com/SongJgit/TBDTracker"><img src="https://img.shields.io/github/stars/SongJgit/TBDTracker.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Army Engineering University, Shijiazhuang, 050003, Heibei, China<br>
‚Ä¢ Dataset: Semi-simulated motion estimation dataset, Samples: Large-scale, constructed from multiple sequences of MOT17, MOT20, SoccerNet, and DanceTrack, Modality: 2D Bounding-box trajectories (position, aspect ratio, height) with simulated noise<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.10961"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/fsa125/HR-pQCT-Motion-Correction-ESWGAN-GP"><img src="https://img.shields.io/github/stars/fsa125/HR-pQCT-Motion-Correction-ESWGAN-GP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Weldon School of Biomedical Engineering, Purdue University, West Lafayette, IN, USA<br>
‚Ä¢ Dataset: HR-pQCT Motion-Corrupted and Ground-Truth Paired Dataset, Samples: 483, Modality: HR-pQCT 3D bone images, 2D slices<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.10687"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stability AI<br>
‚Ä¢ Dataset: KinematicParts20K, Samples: over 20,000, Modality: Multi-view RGB videos + Kinematic part segmentation maps (derived from rigging annotations and skinning weights)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.10096"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/hhi-assist/home"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: VITA laboratory, EPFL, Lausanne, Switzerland<br>
‚Ä¢ Dataset: HHI-Assist, Samples: 908, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.09971"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dolby Laboratories, Inc.<br>
‚Ä¢ Dataset: EventAID, Samples: 1600+ seconds of capture, Modality: Events + RGB images/videos<br>
‚Ä¢ Dataset: LED, Samples: 3000 sequences, Modality: Events + RGB images/videos<br>
‚Ä¢ Dataset: RLED, Samples: 64200 aligned image and event pairs, Modality: Events + RGB images<br>
‚Ä¢ Dataset: SEE-600K, Samples: 610126 image-event pairs, Modality: Events + RGB images<br>
‚Ä¢ Dataset: EDS, Samples: None, Modality: Events + RGB images + IMU measurements<br>
‚Ä¢ Dataset: TUM-VIE, Samples: None, Modality: Stereo event camera data<br>
‚Ä¢ Dataset: BlinkFlow, Samples: None, Modality: Event-based optical flow<br>
‚Ä¢ Dataset: Real-World-Blur, Samples: 5 scenes, Modality: Color events + RGB images<br>
‚Ä¢ Dataset: Real-World-Challenge, Samples: 5 scenes, Modality: Color events + RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.09720"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lachlanchumbley.github.io/ColesObjectSet/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Deakin University, Australia<br>
‚Ä¢ Dataset: Australian Supermarket Object Set (ASOS), Samples: 50, Modality: 3D textured meshes, RGB images, physical objects<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>SpatialVID: A Large-Scale Video Dataset with Spatial Annotations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.09676"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nju-3dv.github.io/projects/SpatialVID"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: SpatialVID, Samples: 2.7 million clips, Modality: RGB videos, camera poses, depth maps, dynamic masks, structured captions, motion instructions<br>
‚Ä¢ Dataset: SpatialVID-HQ, Samples: 0.37 million clips, Modality: RGB videos, camera poses, depth maps, dynamic masks, structured captions, motion instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.09555"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wzyabcas/InterAct"><img src="https://img.shields.io/github/stars/wzyabcas/InterAct.svg?style=social&label=Star"></a><br><a href="https://sirui-xu.github.io/InterAct/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Illinois Urbana-Champaign<br>
‚Ä¢ Dataset: InterAct, Samples: 11350, Modality: 3D MoCap (SMPL-H/SMPL-X human models, rigid/dynamic objects, marker coordinates, velocities, signed distance vectors, foot-ground contact labels, object rotations/translations, BPS geometry)<br>
‚Ä¢ Dataset: InterAct-X, Samples: 16201, Modality: 3D MoCap (SMPL-H/SMPL-X human models, rigid/dynamic objects, marker coordinates, velocities, signed distance vectors, foot-ground contact labels, object rotations/translations, BPS geometry)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Motion-Based User Identification across XR and Metaverse Applications by Deep Classification and Similarity Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.08539"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of W√ºrzburg<br>
‚Ä¢ Dataset: Cross-application XR motion dataset, Samples: 49 users with over 60 hours of motion data, Modality: Head and hand controller tracking data (positions, rotations) in 5 VR applications<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>EHWGesture -- A dataset for multimodal understanding of clinical gestures</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.07525"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/smilies-polito/EHWGesture"><img src="https://img.shields.io/github/stars/smilies-polito/EHWGesture.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy<br>
‚Ä¢ Dataset: EHWGesture, Samples: 1100, Modality: RGB videos, Depth maps, Event camera data, Motion capture tracking<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>MIORe & VAR-MIORe: Benchmarks to Push the Boundaries of Restoration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.06803"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/george200150/MIORe"><img src="https://img.shields.io/github/stars/george200150/MIORe.svg?style=social&label=Star"></a><br><a href="https://github.com/george200150/MIORe"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Vision Lab, CAIDAS & IFI, University of W√ºrzburg<br>
‚Ä¢ Dataset: MIORe, Samples: 333, Modality: RGB videos + optical flow<br>
‚Ä¢ Dataset: VAR-MIORe, Samples: 333, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.06607"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://skel.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems<br>
‚Ä¢ Dataset: BioAMASS, Samples: 2198, Modality: Synthetic MoCap markers + Optimized biomechanical skeleton parameters (scale and pose) + Paired SMPL meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.05747"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hku-cg.github.io/interact/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: InterAct, Samples: 241, Modality: Motion-capture (body markers, finger markers), facial mesh animations, speech audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.05330"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SeyedMuhammadHosseinMousavi/Multi-Modal-Fusion"><img src="https://img.shields.io/github/stars/SeyedMuhammadHosseinMousavi/Multi-Modal-Fusion.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Cyrus Intelligence Research Ltd<br>
‚Ä¢ Dataset: MVRS (Multimodal Virtual Reality Stimuli-based emotion recognition dataset), Samples: 13, Modality: ['MoCap joints (Microsoft Kinect v2)', 'EMG signals', 'GSR signals', 'Eye-tracking (FHD webcam)', 'VR stimuli videos']<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Evaluating Idle Animation Believability: a User Perspective</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.05023"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computational Science and Artificial Intelligence, University of the Basque Country, Gipuzkoa, Spain<br>
‚Ä¢ Dataset: ReActIdle, Samples: 27,273 frames (15.15 mins) genuine idle motion + 55,039 frames (30.57 mins) acted idle motion, Modality: MoCap joints (3D BVH format)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.04117"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MustafaSakhai/videos-specifications-rgb_to_dvs-v2e"><img src="https://img.shields.io/github/stars/MustafaSakhai/videos-specifications-rgb_to_dvs-v2e.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.17030898"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Computer Science, Electronics and Telecommunications, AGH University of Science and Technology, 30-059 Krakow, Poland<br>
‚Ä¢ Dataset: DVS-PedX, Samples: 198 synthetic sequences + 346 real-converted sequences, Modality: DVS event streams (AEDAT), RGB frames, accumulated DVS 'event frames'<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.02807"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MSiam/PixFoundation-2.0.git"><img src="https://img.shields.io/github/stars/MSiam/PixFoundation-2.0.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: nan<br>
‚Ä¢ Dataset: MoCentric-Bench, Samples: 793, Modality: RGB videos + referring expressions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>Articulated Object Estimation in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.01708"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://artipoint.cs.uni-freiburg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Freiburg<br>
‚Ä¢ Dataset: Arti4D, Samples: 45 RGB-D sequences containing 414 human-object interactions, Modality: RGB-D videos, articulated object interactions, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2025</td>
  <td style="width:70%;"><strong>InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2509.00767"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mael-zys.github.io/InterPose/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)<br>
‚Ä¢ Dataset: InterPose, Samples: 73814, Modality: SMPL-X body/hand pose parameters + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.21732"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Systems and Robotics, University of Lisbon<br>
‚Ä¢ Dataset: DMDBench, Samples: 1000, Modality: RGB images of Digital Measurement Devices (DMDs) with motion blur, clutter, and occlusion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.21635"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cifasis/rosariov2/"><img src="https://img.shields.io/github/stars/rosariov2/.svg?style=social&label=Star"></a><br><a href="https://cifasis.github.io/rosariov2/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CIFASIS (CONICET-UNR), Rosario, Santa Fe, Argentina<br>
‚Ä¢ Dataset: The Rosario Dataset v2, Samples: 6, Modality: Stereo IR camera, RGB camera, IMU (6-DoF and 9-DoF), GNSS (SPP, RTK, PPK), wheel odometry<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>EmoCAST: Emotional Talking Portrait via Emotive Text Description</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.20615"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/GVCLab/EmoCAST"><img src="https://img.shields.io/github/stars/GVCLab/EmoCAST.svg?style=social&label=Star"></a><br><a href="https://github.com/GVCLab/EmoCAST"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Macau<br>
‚Ä¢ Dataset: Emotive Text-to-Talking Head (ETTH), Samples: 158 hours (across 15k+ identities), Modality: RGB videos + emotive text descriptions + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.19926"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Colin-Jing/FARM"><img src="https://img.shields.io/github/stars/Colin-Jing/FARM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology (Guangzhou), China<br>
‚Ä¢ Dataset: High-Dynamic Humanoid Motion (HDHM), Samples: 3593, Modality: MoCap joints (SMPL format)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.19895"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Intelligent Robotics and Advanced Manufacturing, Fudan University<br>
‚Ä¢ Dataset: PersonaVid, Samples: 18867, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.19002"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering, The Hong Kong Polytechnic University (PolyU), Kowloon, Hong Kong<br>
‚Ä¢ Dataset: HPose, Samples: 31925, Modality: 6D joint poses, contextual semantics annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.18634"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software Engineering, Huazhong University of Science and Technology, Wuhan, China<br>
‚Ä¢ Dataset: Harmonizing Motion-Detail 270K (HMD-270K), Samples: 270000, Modality: RGB videos + textual captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.17643"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/eventbasedvision/SEBVS"><img src="https://img.shields.io/github/stars/eventbasedvision/SEBVS.svg?style=social&label=Star"></a><br><a href="https://eventbasedvision.github.io/SEBVS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Arizona State University<br>
‚Ä¢ Dataset: ERPNav, Samples: 32033, Modality: RGB frames, event frames, robot velocity commands<br>
‚Ä¢ Dataset: ERPArm, Samples: 18417, Modality: RGB frames, event frames, 6-DoF end-effector poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>MoCo: Motion-Consistent Human Video Generation via Structure-Appearance Decoupling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.17404"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hywang2002.github.io/MoCo"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University, Li Auto<br>
‚Ä¢ Dataset: MoVid, Samples: 30000, Modality: RGB videos with text annotations and estimated human poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>DanceEditor: Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.17342"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lzvsdy.github.io/DanceEditor/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: DanceRemix, Samples: 84523 pairs, Modality: SMPL joints, music signals, text prompts<br>
‚Ä¢ Dataset: DanceRemix-X, Samples: Not specified, an extension of DanceRemix with three-level edit descriptions., Modality: SMPL joints, music signals, text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.17255"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Eberhard-Karls-Universit ¬®at T¬®ubingen<br>
‚Ä¢ Dataset: EgoSLAM-Drive, Samples: 9, Modality: Egocentric RGB videos, IMU, 6DoF poses, AR annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.16911"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gprerit96.github.io/mdd-page"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Purdue University, West Lafayette, IN, USA<br>
‚Ä¢ Dataset: MDD (Multimodal DuetDance), Samples: 10187, Modality: MoCap (SMPL-X parameters) + music + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Towards Open-Vocabulary Multimodal 3D Object Detection with Attributes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.16812"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.16904069"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Davis<br>
‚Ä¢ Dataset: OVAD, Samples: 84384, Modality: LiDAR and RGB images annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Prompting with Sign Parameters for Low-resource Sign Language Instruction Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.16076"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tariquzzamanf/SPIP"><img src="https://img.shields.io/github/stars/tariquzzamanf/SPIP.svg?style=social&label=Star"></a><br><a href="https://huggingface.co/datasets/aplycaebous/BdSLIG"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Engineering, Islamic University of Technology, Bangladesh<br>
‚Ä¢ Dataset: BdSLIG, Samples: 60, Modality: RGB videos + textual instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Text-Driven 3D Hand Motion Generation from Sign Language Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.15902"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://imagine.enpc.fr/ Àúleore.bensabath/HandMDM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: LIGM, ¬¥Ecole des Ponts, IP Paris, Univ Gustave Eiffel, CNRS<br>
‚Ä¢ Dataset: BOBSL3DT, Samples: 1312339, Modality: 3D motion (SMPL-X parameters) + Text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.15232"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beihang University<br>
‚Ä¢ Dataset: HaL-13k, Samples: 13838, Modality: UAV trajectories + RGB video + point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.14797"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.17632/4rs5wpvckz.2"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Electrical Engineering (CSEE), Handong Global University, 558 Handong-ro, Buk-gu, Pohang, 37554, Gyeongsangbuk, Republic of Korea<br>
‚Ä¢ Dataset: Realistic License Plate Restoration and Recognition (RLPR), Samples: 200, Modality: RGB video sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>GeMS: Efficient Gaussian Splatting for Extreme Motion Blur</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.14682"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computational Imaging Lab, Department of Electrical Engineering, IIT Madras, Chennai, India.<br>
‚Ä¢ Dataset: EveGeMS, Samples: 8 scenes, Modality: blurry RGB images, event streams, ground truth sharp images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.14597"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.kaggle.com/datasets/nitishkumarmahala/motion-features-and-apperance-cues-datasets"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mathematics, Bioinformatics and Computer Applications, Maulana Azad National Institute of Technology Bhopal, India<br>
‚Ä¢ Dataset: Motion Features and Apperance Cues Datasets, Samples: None, Modality: RGB images and corresponding motion-encoded smoke masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.13911"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hihixiaolv.github.io/PhysGM.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing Institute of Technology<br>
‚Ä¢ Dataset: PhysAssets, Samples: 24000, Modality: Rendered simulation videos + physical properties + 3D assets<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.13618"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/FreedomIntelligence/TalkVid"><img src="https://img.shields.io/github/stars/FreedomIntelligence/TalkVid.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong, Shenzhen<br>
‚Ä¢ Dataset: TalkVid, Samples: None, Modality: RGB videos of talking heads<br>
‚Ä¢ Dataset: TalkVid-Bench, Samples: 500, Modality: RGB videos of talking heads<br>
‚Ä¢ Dataset: TalkVid-Core, Samples: None, Modality: RGB videos of talking heads<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.13488"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jarvisyjw/ROVER"><img src="https://img.shields.io/github/stars/jarvisyjw/ROVER.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CKS Robotics Institute, Hong Kong University of Science and Technology, Hong Kong SAR, China<br>
‚Ä¢ Dataset: Cross-floor, Samples: None, Modality: LiDAR (Livox Mid360), RGB-D + IMU (Intel RealSense D435i), Visual-Inertial (Intel RealSense T265), and ground truth trajectory<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.13007"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://url.fzi.de/SlimComm"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: FZI Research Center for Information Technology<br>
‚Ä¢ Dataset: OPV2V-R, Samples: , Modality: LiDAR, 4D Radar (with per-point Doppler), RGB cameras, GPS/IMU<br>
‚Ä¢ Dataset: Adver-City-R, Samples: , Modality: LiDAR, 4D Radar (with per-point Doppler), RGB cameras, GPS/IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.12644"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Intelligence and Computing, Tianjin University, Tianjin 300350, China<br>
‚Ä¢ Dataset: VirtualCrowd, Samples: 931 motion sequences, Modality: RGB videos + 3D annotations (2D/3D joints, 3D positions, SMPL-X parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.12610"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/qianchen214/OpenMoCap"><img src="https://img.shields.io/github/stars/qianchen214/OpenMoCap.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: CMU-Occlu, Samples: 5k synthetic MoCap sequences, Modality: MoCap markers<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Express4D: Expressive, Friendly, and Extensible 4D Facial Motion Generation Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.12438"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jaron1990.github.io/Express4D/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tel Aviv University<br>
‚Ä¢ Dataset: Express4D, Samples: 1205, Modality: ARKit blendshape coefficients<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.12349"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/IRMVLab/EgoLoc"><img src="https://img.shields.io/github/stars/IRMVLab/EgoLoc.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IRMV Lab, the Department of Automation, Shanghai Jiao Tong University, Shanghai 200240, China<br>
‚Ä¢ Dataset: DeskTIL, Samples: 150, Modality: RGB-D videos<br>
‚Ä¢ Dataset: ManiTIL, Samples: 200, Modality: RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.12330"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yuvalHG/LRRSim"><img src="https://img.shields.io/github/stars/yuvalHG/LRRSim.svg?style=social&label=Star"></a><br><a href="https://yuvalhg.github.io/DoppDrive/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: General Motors, Technical Center Israel<br>
‚Ä¢ Dataset: LRR-Sim, Samples: 50, Modality: Radar point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.11988"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universidad de Buenos Aires, Facultad de Ciencias Exactas y Naturales, Argentina; CONICET-UBA, Instituto de Ciencias de la Computacion (ICC), Argentina<br>
‚Ä¢ Dataset: Unnamed multi-modal micro-expression dataset, Samples: 504, Modality: RGB videos, Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.11255"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fantasy-amap.github.io/fantasy-talking2/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AMAP, Alibaba Group<br>
‚Ä¢ Dataset: Talking-NSQ, Samples: 410000, Modality: Portrait animation video preference pairs with motion naturalness, lip-sync, and visual quality annotations<br>
‚Ä¢ Dataset: (unnamed) Multidimensional Reward Data, Samples: 10000, Modality: Real and synthetic portrait animation videos with human preference annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>ViPE: Video Pose Engine for 3D Geometric Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.10934"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://research.nvidia.com/labs/toronto-ai/vipe/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA<br>
‚Ä¢ Dataset: Dynpose-100K++, Samples: 99501 videos, Modality: RGB videos + camera poses + dense depth maps<br>
‚Ä¢ Dataset: Wild-SDG-1M, Samples: 1000000 videos, Modality: AI-generated videos + camera poses + dense depth maps<br>
‚Ä¢ Dataset: Web360, Samples: 2000 videos, Modality: Panoramic videos + camera poses + distance maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.10771"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/Clarifiedfish/AEGIS"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National University of Singapore<br>
‚Ä¢ Dataset: AEGIS, Samples: 10470, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.10522"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zquang2202.github.io/SkeletonMamba/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: FPT Software AI Center<br>
‚Ä¢ Dataset: EgoAIST++, Samples: 3.9M frames, Modality: SMPL poses + egocentric videos + music<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Stress Detection from Multimodal Wearable Sensor Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.10468"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/paulvincenz/VitaStress"><img src="https://img.shields.io/github/stars/paulvincenz/VitaStress.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Data Engineering
Helmut-Schmidt University
Hamburg, Germany<br>
‚Ä¢ Dataset: VitaStress, Samples: 21, Modality: three-axis accelerometer, electrodermal activity, photoplethysmography, heart rate, RR-intervals, respiration rate, skin temperature<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.10427"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Turing Inc.<br>
‚Ä¢ Dataset: STRIDE-QA, Samples: 285 K frames, Modality: multi-view RGB videos + LiDAR point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.09818"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, AIUB, Dhaka, Bangladesh<br>
‚Ä¢ Dataset: VIMOS, Samples: 427200, Modality: motion sequences + RGB videos + text annotations (captions and Q&A)<br>
‚Ä¢ Dataset: ViMoNet-Bench, Samples: None, Modality: motion sequences + RGB videos + text annotations (Q&A)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.09811"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vLAR-group/TRACE"><img src="https://img.shields.io/github/stars/vLAR-group/TRACE.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: vLAR Group, The Hong Kong Polytechnic University<br>
‚Ä¢ Dataset: Dynamic Multipart dataset, Samples: 4, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.09709"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CyberAgent<br>
‚Ä¢ Dataset: Unity-test200, Samples: 200, Modality: RGB image pairs + line art<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.09404"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/GuangxunZhu/Waymo-3DSkelMo"><img src="https://img.shields.io/github/stars/GuangxunZhu/Waymo-3DSkelMo.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Glasgow<br>
‚Ä¢ Dataset: Waymo-3DSkelMo, Samples: 2438145, Modality: 3D skeletal motions from LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>AirSignatureDB: Exploring In-Air Signature Biometrics in the Wild and its Privacy Concerns</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.08502"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://play.google.com/store/apps/details?id=com.bida.behavepassdbuam"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Biometrics and Data Pattern Analytics Lab, Universidad Autonoma de Madrid, Spain<br>
‚Ä¢ Dataset: AirSignatureDB, Samples: , Modality: inertial sensor data (accelerometer, linear accelerometer, gyroscope)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.08269"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://emg2tendon.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Independent Researcher<br>
‚Ä¢ Dataset: emg2tendon, Samples: 25,254 sessions, Modality: sEMG signals + MoCap hand poses + tendon control signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Joint Transcription of Acoustic Guitar Strumming Directions and Chords</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.07973"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Klangio GmbH, Karlsruhe, Germany<br>
‚Ä¢ Dataset: Real-world Guitar Strumming Dataset (not explicitly named), Samples: 90 minutes of recordings, Modality: ESP32 smartwatch motion sensor (3-axis accelerometer), guitar pickup audio, microphone audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.07863"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://beingbeyond.github.io/Being-M0.5"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CASIA<br>
‚Ä¢ Dataset: HuMo100M, Samples: 5 million motion sequences, Modality: SMPL parameters, RGB videos, text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Noise-Aware Generative Microscopic Traffic Simulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.07453"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ct135.github.io/i24-msd/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Massachusetts Institute of Technology, Cambridge, MA, USA<br>
‚Ä¢ Dataset: I-24 MOTION Scenario Dataset (I24-MSD), Samples: 570000, Modality: Vehicle trajectories from infrastructure cameras + vectorized road maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>CharacterShot: Controllable and Consistent 4D Character Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.07409"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Jeoyal/CharacterShot"><img src="https://img.shields.io/github/stars/Jeoyal/CharacterShot.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tongji University<br>
‚Ä¢ Dataset: Character4D, Samples: 13115, Modality: multi-view videos of animated 3D characters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Understanding Dynamic Scenes in Ego Centric 4D Point Clouds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.07251"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: EgoDynamic4D, Samples: 275, Modality: RGB-D video, camera poses, instance masks, 4D bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.07003"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Chensiyu00/EGS-SLAM"><img src="https://img.shields.io/github/stars/Chensiyu00/EGS-SLAM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: EventReplica, Samples: 7, Modality: Synthetic blurred RGB-D, Event streams, Ground-truth poses<br>
‚Ä¢ Dataset: DEVD, Samples: 8, Modality: Real-world RGB images, Event streams, Depth, Motion capture poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.06757"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yashgarg98/VOccl3D-dataset"><img src="https://img.shields.io/github/stars/yashgarg98/VOccl3D-dataset.svg?style=social&label=Star"></a><br><a href="https://yashgarg98.github.io/VOccl3D-dataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Riverside<br>
‚Ä¢ Dataset: VOccl3D, Samples: 400 video sequences, Modality: RGB videos + 3D pose/shape (SMPL-X) annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>PA-HOI: A Physics-Aware Human and Object Interaction Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.06205"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/RayZuo123/pa-hoi-dataset"><img src="https://img.shields.io/github/stars/RayZuo123/pa-hoi-dataset.svg?style=social&label=Star"></a><br><a href="https://rayzuo123.github.io/pa-hoi-dataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: PA-HOI, Samples: 562, Modality: MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.05465"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/paulili08/F2PASeg"><img src="https://img.shields.io/github/stars/paulili08/F2PASeg.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: Pituitary Anatomy Segmentation (PAS), Samples: 120, Modality: RGB videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.05205"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Beryl2000/EndoMatcher"><img src="https://img.shields.io/github/stars/Beryl2000/EndoMatcher.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, and also with the School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China<br>
‚Ä¢ Dataset: Endo-Mix6, Samples: 1200000, Modality: RGB image pairs + correspondences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>X-MoGen: Unified Motion Generation across Humans and Animals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.05162"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: UniMo4D, Samples: 118663, Modality: 3D joint positions and velocities<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Benchmarking Shortcutting Techniques for Multi-Robot-Arm Motion Planning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.05027"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/philip-huang/mr-shortcut"><img src="https://img.shields.io/github/stars/philip-huang/mr-shortcut.svg?style=social&label=Star"></a><br><a href="https://philip-huang.github.io/mr-shortcut/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics Institute, Carnegie Mellon University<br>
‚Ä¢ Dataset: multi-robot motion planning instances, Samples: 1034, Modality: robot kinematic trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.04681"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://liangxuy.github.io/InterVLA/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: InterVLA, Samples: 3906, Modality: Egocentric and exocentric RGB videos, language commands, MoCap-based human (SMPL) and object motions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.03578"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jonasmueler/RadProPoserCode.git"><img src="https://img.shields.io/github/stars/jonasmueler/RadProPoserCode.git.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.14738837"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Machine Learning and Data Analytics Lab, Friedrich-Alexander-Universit√§t Erlangen-N√ºrnberg, Germany<br>
‚Ä¢ Dataset: RadProPoser Dataset, Samples: None, Modality: Raw radar tensors, Optical Motion Capture (OMC) 3D keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>AID4AD: Aerial Image Data for Automated Driving Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.02140"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DriverlessMobility/AID4AD"><img src="https://img.shields.io/github/stars/DriverlessMobility/AID4AD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Electrical Engineering, Technical University of Applied Sciences Augsburg, Germany<br>
‚Ä¢ Dataset: AID4AD, Samples: None, Modality: High-resolution aerial imagery<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&A</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.01984"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LUNAProject22/IMoRe"><img src="https://img.shields.io/github/stars/LUNAProject22/IMoRe.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of High-Performance Computing, Agency for Science, Technology and Research, Singapore<br>
‚Ä¢ Dataset: HuMMan-QA, Samples: 1311, Modality: SMPL motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>CVD-SfM: A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.01936"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/RobustFieldAutonomyLab/CVD-SfM"><img src="https://img.shields.io/github/stars/RobustFieldAutonomyLab/CVD-SfM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stevens Institute of Technology<br>
‚Ä¢ Dataset: SIT Campus, Samples: 365, Modality: RGB images (ground, aerial, satellite) + GPS poses<br>
‚Ä¢ Dataset: Raritan Bay Park, Samples: 313, Modality: RGB images (ground, aerial, satellite) + GPS poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>VLH: Vision-Language-Haptics Foundation Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.01361"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Skolkovo Institute of Science and Technology<br>
‚Ä¢ Dataset: VLH Drone Flight and Haptic Interaction Dataset, Samples: 50, Modality: Drone flight trajectories (Vicon motion capture), action commands, haptic signals, RGB videos (real & VR), language commands<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.01126"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://chaitanya100100.github.io/UniEgoMotion/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: EE4D-Motion, Samples: 143K training samples (110+ hours), Modality: egocentric RGB videos + pseudo-ground-truth 3D motion (SMPL-X)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.00748"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/BiDAlab/GAGAvatar-Benchmark"><img src="https://img.shields.io/github/stars/BiDAlab/GAGAvatar-Benchmark.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Biometrics and Data Pattern Analytics Lab, Universidad Autonoma de Madrid, Spain<br>
‚Ä¢ Dataset: GAGAvatar-Benchmark, Samples: None, Modality: Avatar videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.00589"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://iv.ee.hm.edu/contextmotionclip/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Munich University of Applied Sciences, Intelligent Vehicles Lab (IVL), 80335 Munich, Germany<br>
‚Ä¢ Dataset: WayMoCo, Samples: 27466, Modality: SMPL sequences, video frames, text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.00088"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.freedesktop.org/mateosss/xrtslam-metrics"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich<br>
‚Ä¢ Dataset: Monado SLAM Dataset, Samples: 64, Modality: Visual-Inertial (Monochrome Video + IMU) + Ground-truth pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2025</td>
  <td style="width:70%;"><strong>Punching Bag vs. Punching Person: Motion Transferability in Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2508.00085"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/raiyaan-abdullah/Motion-Transfer"><img src="https://img.shields.io/github/stars/raiyaan-abdullah/Motion-Transfer.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Research in Computer Vision, University of Central Florida<br>
‚Ä¢ Dataset: Syn-TA, Samples: 10000, Modality: Synthetic RGB videos<br>
‚Ä¢ Dataset: Kinetics400-TA, Samples: 139883, Modality: RGB videos<br>
‚Ä¢ Dataset: Something-Something-v2-TA, Samples: 162215, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding Space</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.23188"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southern University of Science and Technology, and jointly with Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China<br>
‚Ä¢ Dataset: Augmented KIT-ML with Audio, Samples: 3911, Modality: MoCap joints + Synthesized Audio<br>
‚Ä¢ Dataset: Augmented HumanML3D with Audio, Samples: 14616, Modality: MoCap joints + Synthesized Audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>GestureHYDRA: Semantic Co-speech Gesture Synthesis via Hybrid Modality Diffusion Transformer and Cascaded-Synchronized Retrieval-Augmented Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.22731"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mumuwei.github.io/GestureHYDRA/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: Streamer, Samples: 20969, Modality: monocular videos, reconstructed SMPL-X parameters, speech audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Recognizing Actions from Robotic View for Natural Human-Robot Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.22522"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View"><img src="https://img.shields.io/github/stars/wangzy01/ACTIVE-Action-from-Robotic-View.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School<br>
‚Ä¢ Dataset: ACTIVE, Samples: 46868, Modality: RGB+Point Cloud<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Estimating 2D Camera Motion with Hybrid Motion Basis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.22480"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lhaippp/CamFlow"><img src="https://img.shields.io/github/stars/lhaippp/CamFlow.svg?style=social&label=Star"></a><br><a href="https://lhaippp.github.io/CamFlow/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: GHOF-Cam, Samples: 256, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>MOVE: Motion-Guided Few-Shot Video Object Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.22061"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://henghuiding.com/MOVE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University, China<br>
‚Ä¢ Dataset: MOVE, Samples: 4300, Modality: RGB videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.21069"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ai-for-sensor-data-analytics-ulm/aisd_ortho_ki_dataset"><img src="https://img.shields.io/github/stars/ai-for-sensor-data-analytics-ulm/aisd_ortho_ki_dataset.svg?style=social&label=Star"></a><br><a href="[LINK EINF√úGEN]"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AI for Sensor Data Analytics Research Group, Ulm University of Applied Sciences, Ulm, 89081, Germany<br>
‚Ä¢ Dataset: GAITEX, Samples: 19 participants, Modality: IMU, MoCap markers, RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Deep Learning for Skeleton Based Human Motion Rehabilitation Assessment: A Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.21018"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MSD-IRIMAS/DeepRehabPile"><img src="https://img.shields.io/github/stars/MSD-IRIMAS/DeepRehabPile.svg?style=social&label=Star"></a><br><a href="https://msd-irimas.github.io/pages/DeepRehabPile"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IRIMAS, Universite de Haute-Alsace, France<br>
‚Ä¢ Dataset: Rehab-Pile, Samples: None, Modality: Skeleton sequences derived from 9 public repositories (e.g., from Kinect, OpenPose)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.20987"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/deepreasonings/WholeBodyBenchmark"><img src="https://img.shields.io/github/stars/deepreasonings/WholeBodyBenchmark.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Independent Researcher, China<br>
‚Ä¢ Dataset: Joint Whole-Body Talking Avatar and Speech Generation Version I (JWB-DH-V1), Samples: 2 million, Modality: RGB videos + body segmentation + pose landmarks + bounding boxes + motion text + speech transcriptions + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Event-Based De-Snowing for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.20901"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics and Perception Group, Department of Informatics, University of Zurich, Switzerland<br>
‚Ä¢ Dataset: DSEC-Snow, Samples: 250, Modality: RGB videos, Event streams, Ground truth images<br>
‚Ä¢ Dataset: Slider-Snow, Samples: None, Modality: RGB videos, Event streams, Ground truth images<br>
‚Ä¢ Dataset: SnowDriving, Samples: None, Modality: RGB videos, Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>MagicAnime: A Hierarchically Annotated, Multimodal and Multitasking Dataset with Benchmarks for Cartoon Animation Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.20368"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Centre for Computer Animation, Bournemouth University<br>
‚Ä¢ Dataset: MagicAnime, Samples: 50000, Modality: RGB videos + full-body keypoints + text<br>
‚Ä¢ Dataset: MagicAnime, Samples: 12080, Modality: RGB videos + facial keypoints<br>
‚Ä¢ Dataset: MagicAnime, Samples: 3000, Modality: RGB videos + audio + facial keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.19924"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dejian-lc.github.io/humansam/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National University of Defense Technology<br>
‚Ä¢ Dataset: Human-centric Forgery Video (HFV) dataset, Samples: 7390, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>FineMotion: A Dataset and Benchmark with both Spatial and Temporal Annotation for Fine-grained Motion Generation and Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.19850"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/CVI-SZU/FineMotion"><img src="https://img.shields.io/github/stars/CVI-SZU/FineMotion.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science & Software Engineering, Shenzhen University<br>
‚Ä¢ Dataset: FineMotion, Samples: 14616, Modality: SMPL pose parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>TransFlow: Motion Knowledge Transfer from Video Diffusion Models to Video Salient Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.19789"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/suhwan-cho/TransFlow"><img src="https://img.shields.io/github/stars/suhwan-cho/TransFlow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: GenGenAI<br>
‚Ä¢ Dataset: DUTS-Video, Samples: 218008, Modality: RGB image + optical flow + saliency mask<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.19684"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/Rosie-Lab/compas3d"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computing Science, Simon Fraser University<br>
‚Ä¢ Dataset: CoMPAS3D, Samples: 72 sequences (3 hours), Modality: MoCap (SMPL-X parameters, 3D joint trajectories), Synchronized audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>RISEE: A Highly Interactive Naturalistic Driving Trajectories Dataset with Human Subjective Risk Perception and Eye-tracking Information</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.19490"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ivtest-lab.github.io/RISEE dataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Automotive Studies, Tongji University<br>
‚Ä¢ Dataset: RISEE, Samples: 179, Modality: Vehicle trajectories, FPV videos, Eye-tracking data, Subjective risk ratings<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>GS-Occ3D: Scaling Vision-only Occupancy Reconstruction with Gaussian Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.19451"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IIIS, THU; Shanghai Qi Zhi Institute<br>
‚Ä¢ Dataset: GS-Occ3D Vision-only Occupancy Labels, Samples: 802 scenes (637 training, 165 validation), Modality: 3D occupancy grids<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Extreme Cardiac MRI Analysis under Respiratory Motion: Results of the CMRxMotion Challenge</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.19165"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/CMRxMotion"><img src="https://img.shields.io/github/stars/github.com/CMRxMotion.svg?style=social&label=Star"></a><br><a href="http://cmr.miccai.cloud"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Digital Medical Research Center, School of Basic Medical Sciences, Fudan University, Shanghai, Shanghai 200032, China<br>
‚Ä¢ Dataset: CMRxMotion, Samples: 320, Modality: CMR cine series<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.18155"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hahminlew.github.io/geoavatar"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Klleon AI Research<br>
‚Ä¢ Dataset: DynamicFace, Samples: 10, Modality: monocular RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>SV3.3B: A Sports Video Understanding Model for Action Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.17844"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/sportsvision/nsva_subset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sports Vision, Inc.<br>
‚Ä¢ Dataset: NSVA Subset, Samples: 1315, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.17664"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://talk2event.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NUS<br>
‚Ä¢ Dataset: Talk2Event, Samples: 5567, Modality: Event streams + RGB frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.17445"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Turku Intelligent Embedded and Robotics Systems lab, Faculty of Technology, University of Turku, Finland<br>
‚Ä¢ Dataset: Custom hybrid indoor dataset, Samples: approx. 6500 frames, Modality: LiDAR point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Physics-based Human Pose Estimation from a Single Moving RGB Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.17406"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/aidilayce/physdynpose"><img src="https://img.shields.io/github/stars/aidilayce/physdynpose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics<br>
‚Ä¢ Dataset: MoviCam, Samples: 7, Modality: RGB videos, SMPL poses, camera trajectories, scene geometry, contact labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>ResKACNNet: A Residual ChebyKAN Network for Inertial Odometry</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.16865"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the paper<br>
‚Ä¢ Dataset: TLIOv2, Samples: None, Modality: IMU data (acceleration and angular velocity)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>SPACT18: Spiking Human Action Recognition Benchmark Dataset with Complementary RGB and Thermal Modalities</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.16151"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yasser-Ashraf-Saleh/SPACT18-Dataset-Benchmark"><img src="https://img.shields.io/github/stars/Yasser-Ashraf-Saleh/SPACT18-Dataset-Benchmark.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Machine Learning
Mohamed bin Zayed University of Artificial Intelligence<br>
‚Ä¢ Dataset: SPACT18, Samples: 3168, Modality: Spike camera videos, RGB videos, Thermal videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Discovering and using Spelke segments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.16038"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/neuroailab/spelke-net"><img src="https://img.shields.io/github/stars/neuroailab/spelke-net.svg?style=social&label=Star"></a><br><a href="https://neuroailab.github.io/spelke_net"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: SpelkeBench, Samples: 500, Modality: RGB images + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.15597"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/beingbeyond/Being-H0"><img src="https://img.shields.io/github/stars/beingbeyond/Being-H0.svg?style=social&label=Star"></a><br><a href="https://beingbeyond.github.io/Being-H0"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: UniHand, Samples: 444100, Modality: RGB videos + MANO hand pose parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.15292"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/szu-pc/EndoControlMag"><img src="https://img.shields.io/github/stars/szu-pc/EndoControlMag.svg?style=social&label=Star"></a><br><a href="https://szupc.github.io/EndoControlMag/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electronic Engineering, The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: EndoVMM24, Samples: 24, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Music-Aligned Holistic 3D Dance Generation via Hierarchical Motion Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.14915"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://xjli360.github.io/SoulDance"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen International Graduate School, Tsinghua University<br>
‚Ä¢ Dataset: SoulDance, Samples: 284, Modality: MoCap data (BVH files, SMPL-X body models, FLAME face parameters) of holistic dance (body, hands, face) paired with music.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>SenseSeek Dataset: Multimodal Sensing to Study Information Seeking Behaviors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.14792"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ADMSCentre/SenseSeek-Dataset-code"><img src="https://img.shields.io/github/stars/ADMSCentre/SenseSeek-Dataset-code.svg?style=social&label=Star"></a><br><a href="https://osf.io/waunb/?view_only=94756f9d2c7a49e094ae42d494c9516a"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: RMIT University, Australia<br>
‚Ä¢ Dataset: SenseSeek, Samples: 940, Modality: MOTION (wrist accelerometer, head accelerometer, head magnetometer, head quaternions), EDA, EEG, PUPIL, GAZE<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>AthleticsPose: Authentic Sports Motion Dataset on Athletic Field and Evaluation of Monocular 3D Pose Estimation Ability</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.12905"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SZucchini/AthleticsPose"><img src="https://img.shields.io/github/stars/SZucchini/AthleticsPose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nagoya University<br>
‚Ä¢ Dataset: AthleticsPose, Samples: 62500, Modality: RGB videos + 3D joint positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.12463"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://MMHU-Benchmark.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Texas A&M University<br>
‚Ä¢ Dataset: MMHU, Samples: 57000, Modality: RGB videos, SMPL parameters, 3D trajectories, text descriptions, VQA pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.12095"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/aimagelab/brum"><img src="https://img.shields.io/github/stars/aimagelab/brum.svg?style=social&label=Star"></a><br><a href="https://aimagelab.ing.unimore.it/go/brum"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Modena and Reggio Emilia<br>
‚Ä¢ Dataset: BRUM-dataset, Samples: 12, Modality: Synthetic scenes (RGB images, depth maps, camera poses) and real-world scenes (360¬∞ videos of buses, provided as masked frames).<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>MOSPA: Human Motion Generation Driven by Spatial Audio</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.11949"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: Spatial Audio-Driven Human Motion (SAM), Samples: 3000, Modality: SMPL-X motion + Binaural audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.11931"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: Real-world event-based low-light radiance field reconstruction dataset, Samples: 6, Modality: Paired low-light and bright-light frames, event streams, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.11642"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Indian Institute of Technology Kanpur<br>
‚Ä¢ Dataset: Cricket Shot Intent Dataset (CSID), Samples: 2611, Modality: Pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.11037"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/ljw285/FootGait3D"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Biomedical Engineering, Fudan University<br>
‚Ä¢ Dataset: FootGait3D, Samples: 8403, Modality: point clouds from depth sensors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.10334"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peter Faber Business School, Australian Catholic University (ACU), Sydney, Australia<br>
‚Ä¢ Dataset: MoCap-Impute, Samples: 53, Modality: IMU-based kinematic variables<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.09672"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/CarmenQing/VST-Pose"><img src="https://img.shields.io/github/stars/CarmenQing/VST-Pose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Information Engineering, South China University of Technology, China<br>
‚Ä¢ Dataset: self-collected 2D pose dataset for smart home care scenarios, Samples: 3300, Modality: WiFi CSI signals + 2D keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>SnapMoGen: Human Motion Generation from Expressive Texts</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.09122"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://snap-research.github.io/SnapMoGen/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Snap Inc.<br>
‚Ä¢ Dataset: SnapMoGen, Samples: 20450, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>AirScape: An Aerial Generative World Model with Motion Controllability</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.08885"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://embodiedcity.github.io/AirScape/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: Dataset for Aerial World Model, Samples: 11000, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>MUVOD: A Novel Multi-view Video Object Segmentation Dataset and A Benchmark for 3D Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.07519"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://volumetric-repository.labs.b-com.com/#/muvod"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Research and Technology b-com, Cesson-S√©vign√©, France and Univ Rennes, INSA Rennes, CNRS, IETR - UMR 6164, F-35000 Rennes, France<br>
‚Ä¢ Dataset: MUVOD, Samples: 17, Modality: multi-view RGB videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.07394"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Wangxuan Institute of Computer Technology, Peking University<br>
‚Ä¢ Dataset: DeformingThings4D-skl, Samples: 787 4D motion sequences, Modality: 4D motion sequences with skeletal rigging<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Generating Moving 3D Soundscapes with Latent Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.07318"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://intellisys.haow.us/spatial-audio-project/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stevens Institute of Technology<br>
‚Ä¢ Dataset: None, Samples: 1025548, Modality: FOA audio + sound source trajectories (azimuth, elevation, speed)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.07095"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VankouF/MotionMillion-Codes"><img src="https://img.shields.io/github/stars/VankouF/MotionMillion-Codes.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: MotionMillion, Samples: 2000000, Modality: SMPL parameters<br>
‚Ä¢ Dataset: MotionMillion-Eval, Samples: 126, Modality: text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Democratizing High-Fidelity Co-Speech Gesture Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.06812"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mpi-lab.github.io/Democratizing-CSG/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: South China University of Technology<br>
‚Ä¢ Dataset: CSG-405, Samples: 147550, Modality: RGB videos + 2D skeletons<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.06530"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, BRAC University, Dhaka, Bangladesh<br>
‚Ä¢ Dataset: Sign3D-WLASL, Samples: 1983, Modality: 3D skeletal keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.06405"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DFKI, RPTU, Kaiserslautern, Germany<br>
‚Ä¢ Dataset: ImpAct, Samples: None, Modality: bio-impedance, IMU, video, 3D pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.06400"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VranLee/SU-T"><img src="https://img.shields.io/github/stars/VranLee/SU-T.svg?style=social&label=Star"></a><br><a href="https://vranlee.github.io/SU-T/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: China Agricultural University<br>
‚Ä¢ Dataset: Multiple Fish Tracking Dataset 2025 (MFT25), Samples: 15, Modality: RGB videos with annotated bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Learning to Track Any Points from Human Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.06233"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST AI<br>
‚Ä¢ Dataset: Anthro-LD, Samples: 1400, Modality: RGB videos + 2D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.05698"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mohsij/space-event-rgb-fusion"><img src="https://img.shields.io/github/stars/mohsij/space-event-rgb-fusion.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AI for Space Group, The University of Adelaide, Australia<br>
‚Ä¢ Dataset: FRESH, Samples: 24, Modality: RGB frames + event data + 6DoF poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Neural-Driven Image Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.05397"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://loongx1.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NUS<br>
‚Ä¢ Dataset: L-Mind, Samples: 23928, Modality: EEG, fNIRS, PPG, head motion (6-axis IMU), speech, image pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Beyond Features: How Dataset Design Influences Multi-Agent Trajectory Prediction Performance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.05098"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robert Bosch GmbH, Stuttgart, Germany<br>
‚Ä¢ Dataset: L4 Motion Forecasting dataset, Samples: 90k, Modality: LiDAR, cameras, radars<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.04955"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xinyueli2896/Expotion.git"><img src="https://img.shields.io/github/stars/xinyueli2896/Expotion.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates<br>
‚Ä¢ Dataset: Expotion Dataset, Samples: 7 hours of video, Modality: RGB videos of facial expressions and upper-body gestures<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for Particle Image Velocimetry</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.04750"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: International School, Beijing University of Posts and Telecommunications<br>
‚Ä¢ Dataset: Comprehensive PIV Benchmark Dataset, Samples: 19500, Modality: Synthetic particle image pairs + ground-truth velocity fields<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Grounded Gesture Generation: Language, Motion, and Space</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.04522"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://groundedgestures.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: Grounded Gestures, Samples: 6250, Modality: MoCap (HumanML3D format), Speech, 3D Scene Info<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.02948"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Westlake University, Zhejiang University<br>
‚Ä¢ Dataset: DriveMRP-10K, Samples: 10000, Modality: multimodal dataset comprising scene images (front-view, BEV), motion trajectories (waypoints), and textual annotations (VQA pairs, risk labels)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.02565"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.buzhenhuang.com/works/CloseApp.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southeast University, National University of Singapore<br>
‚Ä¢ Dataset: WildCHI, Samples: 100, Modality: RGB videos + pseudo ground-truth SMPL<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.02479"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/loseevaya/CrowdTrack"><img src="https://img.shields.io/github/stars/loseevaya/CrowdTrack.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: CrowdTrack, Samples: 33, Modality: RGB videos + bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.02200"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/ESTR-CoT"><img src="https://img.shields.io/github/stars/Event-AHU/ESTR-CoT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei 230601, China<br>
‚Ä¢ Dataset: CoT_ESTR, Samples: 16222, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.00660"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/crs524/MTCNet"><img src="https://img.shields.io/github/stars/crs524/MTCNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Medical Ultrasound Image Computing (MUSIC) Lab, School of Biomedical Engineering, Medical School, Shenzhen University, Shenzhen, China<br>
‚Ä¢ Dataset: 4D MV dataset, Samples: 160, Modality: 4D Ultrasound<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.00525"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://djamahl99.github.io/qaymo-pages/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Queensland, Brisbane, Australia<br>
‚Ä¢ Dataset: Box-QAymo, Samples: 13714, Modality: Camera images with box-referenced Q&A pairs derived from 3D object trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2025</td>
  <td style="width:70%;"><strong>Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2507.00339"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/Amar-S/MOVi-MC-AC"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Lawrence Livermore National Laboratory<br>
‚Ä¢ Dataset: MOVi-MC-AC, Samples: 2041, Modality: Multi-camera RGB videos, Depth masks, Modal/Amodal segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.24074"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DurrLab/C3VD"><img src="https://img.shields.io/github/stars/DurrLab/C3VD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Biomedical Engineering, Johns Hopkins University<br>
‚Ä¢ Dataset: C3VDv2, Samples: 192, Modality: RGB videos + depth + surface normals + optical flow + 6-DoF camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23957"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sinoyou.github.io/gavs"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich, Switzerland<br>
‚Ä¢ Dataset: Repurposed DeepFused dataset, Samples: 15, Modality: RGB videos + 3D camera poses + dynamic object masks + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23759"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, NUS, Singapore<br>
‚Ä¢ Dataset: Hyst-YT, Samples: 1980, Modality: RGB surgical videos with part-level segmentation masks<br>
‚Ä¢ Dataset: Lob-YT, Samples: 203, Modality: RGB surgical videos with part-level segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23690"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lucaria-academy.github.io/SynMotion/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ant Group<br>
‚Ä¢ Dataset: MotionBench, Samples: 96-160, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Event-based Tiny Object Detection: A Benchmark Dataset and Baseline</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23575"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ChenYichen9527/Ev-UAV"><img src="https://img.shields.io/github/stars/ChenYichen9527/Ev-UAV.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not found in the provided document<br>
‚Ä¢ Dataset: EV-UAV, Samples: 147, Modality: Event camera stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>DexH2R: A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.23152"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dexh2r.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: DexH2R, Samples: 4282, Modality: multi-view RGB-D streams, 3D annotations, robot kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.22718"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of Minnesota<br>
‚Ä¢ Dataset: Partial-RoboArt, Samples: None, Modality: 4D point clouds<br>
‚Ä¢ Dataset: Occluded-RoboArt, Samples: None, Modality: 4D point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.22554"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/seamless_interaction"><img src="https://img.shields.io/github/stars/facebookresearch/seamless_interaction.svg?style=social&label=Star"></a><br><a href="https://huggingface.co/datasets/facebook/seamless-interaction"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta<br>
‚Ä¢ Dataset: Seamless Interaction Dataset, Samples: 64739, Modality: RGB videos, Audio, SMPL-H poses, facial expression codes, text transcripts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Generating Attribute-Aware Human Motions from Textual Prompt</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.21912"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: HumanAttr, Samples: 18199, Modality: SMPL parameters from MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.21765"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/QiLi111/tus-rec-challenge_baseline"><img src="https://img.shields.io/github/stars/QiLi111/tus-rec-challenge_baseline.svg?style=social&label=Star"></a><br><a href="https://github-pages.ucl.ac.uk/tus-rec-challenge/TUS-REC2024/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UCL Hawkes Institute, Department of Medical Physics and Biomedical Engineering, University College London, London, WC1E 6BT, U.K.<br>
‚Ä¢ Dataset: TUS-REC2024, Samples: 2040, Modality: 2D ultrasound videos + 6-DoF pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.21680"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vinayak-vg/PhotonSplat"><img src="https://img.shields.io/github/stars/vinayak-vg/PhotonSplat.svg?style=social&label=Star"></a><br><a href="https://vinayak-vg.github.io/PhotonSplat/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Indian Institute of Technology, Madras, India<br>
‚Ä¢ Dataset: PhotonScenes, Samples: 9, Modality: multi-view SPAD binary images + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.20936"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://physrig.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Illinois Urbana Champaign<br>
‚Ä¢ Dataset: PhysRig Synthetic Dataset, Samples: 120, Modality: Simulated 3D mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.20550"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Tuebingen<br>
‚Ä¢ Dataset: BOAT360, Samples: None, Modality: Fisheye RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.20103"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: BrokenVideos, Samples: 3254, Modality: RGB videos + pixel-level masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.19851"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anima-x.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beihang University, China<br>
‚Ä¢ Dataset: None, Samples: 161023, Modality: Rigged 3D animation sequences (processed into multi-view videos and corresponding 2D pose maps)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>The Starlink Robot: A Platform and Dataset for Mobile Satellite Communication</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.19781"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/StarlinkRobot"><img src="https://img.shields.io/github/stars/github.com/StarlinkRobot.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University College London<br>
‚Ä¢ Dataset: Starlink Robot Dataset, Samples: None, Modality: robot kinematics, pose trajectories, LiDAR point clouds, fisheye camera images, IMU, GPS, communication metrics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Deblurring in the Wild: A Real-World Dataset from Smartphone High-Speed Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.19445"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Dhaka<br>
‚Ä¢ Dataset: SloMoDeblur, Samples: 42045, Modality: blur-sharp image pairs from high-speed videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EvDetMAV: Generalized MAV Detection from Moving Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.19416"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WindyLab/EvDetMAV"><img src="https://img.shields.io/github/stars/WindyLab/EvDetMAV.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology, Zhejiang University<br>
‚Ä¢ Dataset: EventMAV, Samples: 25335, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Matrix-Game: Interactive World Foundation Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.18701"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SkyworkAI/Matrix-Game"><img src="https://img.shields.io/github/stars/SkyworkAI/Matrix-Game.svg?style=social&label=Star"></a><br><a href="https://matrix-game-homepage.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Skywork AI<br>
‚Ä¢ Dataset: Matrix-Game-MC, Samples: 2,700 hours of unlabeled video clips, >1,000 hours of labeled video clips, Modality: RGB videos + action labels (keyboard, mouse) + agent kinematics (position, velocity, orientation)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.18443"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZzhYgwh/TwistEstimator"><img src="https://img.shields.io/github/stars/ZzhYgwh/TwistEstimator.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Automation, Northwestern Polytechnical University, Xi‚Äôan, Shaanxi, 710129 P.R. China.<br>
‚Ä¢ Dataset: None, Samples: 10, Modality: Event camera data, 4D mmWave radar data, IMU data, RTK-GPS ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.17590"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/taco-group/DRAMA-X"><img src="https://img.shields.io/github/stars/taco-group/DRAMA-X.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Texas A&M University<br>
‚Ä¢ Dataset: DRAMA-X, Samples: 5686, Modality: RGB video clips + object trajectories + intent labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.17332"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="P2MFDS Github Repository"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Northwestern Polytechnical University<br>
‚Ä¢ Dataset: None, Samples: 18000, Modality: mmWave 3D point cloud + 3D vibration data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.16856"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/little-snail-f/ParkFormer"><img src="https://img.shields.io/github/stars/little-snail-f/ParkFormer.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Automation, Chinese Academy of Sciences,Beijing 100190, China<br>
‚Ä¢ Dataset: ParkFormer Dataset, Samples: 272, Modality: RGB images, depth maps, ego-motion states (velocity, acceleration), pedestrian trajectories, control commands<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EchoShot: Multi-Shot Portrait Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.15838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://johnneywang.github.io/EchoShot-webpage"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xi‚Äôan Jiaotong University<br>
‚Ä¢ Dataset: PortraitGala, Samples: 650000, Modality: RGB videos + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Toward Rich Video Human-Motion2D Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.14428"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/FooAuto/Toward-Rich-Video-Human-Motion2D-Generation"><img src="https://img.shields.io/github/stars/FooAuto/Toward-Rich-Video-Human-Motion2D-Generation.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Technology, Tongji University, China<br>
‚Ä¢ Dataset: Motion2D-Video-150K, Samples: 150000, Modality: 2D skeleton sequences with textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Action Dubber: Timing Audible Actions via Inflectional Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.13320"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WenlongWan/Audible623"><img src="https://img.shields.io/github/stars/WenlongWan/Audible623.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Engineering, South China University of Technology<br>
‚Ä¢ Dataset: Audible 623, Samples: 623, Modality: RGB videos + frame-level annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>MAMMA: Markerless & Automatic Multi-Person Motion Action Capture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.13040"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, Germany<br>
‚Ä¢ Dataset: MAMMASyn, Samples: 2.8k sequences / 2.5M samples, Modality: Synthetic multi-view RGB video + SMPL-X annotations + dense 2D landmarks + segmentation masks<br>
‚Ä¢ Dataset: Latin-Dance, Samples: 10 sequences, Modality: Vicon MoCap data<br>
‚Ä¢ Dataset: Interacting Couples, Samples: 48 sequences, Modality: Vicon MoCap data<br>
‚Ä¢ Dataset: MAMMAEval-Singles, Samples: 22 sequences, Modality: Multi-view RGB video + Vicon MoCap data + SMPL-X annotations<br>
‚Ä¢ Dataset: MAMMAEval-Dance, Samples: 17 sequences, Modality: Multi-view RGB video + Vicon MoCap data + SMPL-X annotations<br>
‚Ä¢ Dataset: MAMMAEval-Extra, Samples: 16 sequences, Modality: Multi-view RGB video + Vicon MoCap data (standard and extra markers) + SMPL-X annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Multiple Object Tracking in Video SAR: A Benchmark and Tracking Baseline</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.12105"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/softwarePupil/VSMB"><img src="https://img.shields.io/github/stars/softwarePupil/VSMB.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Information Engineering, Beihang University<br>
‚Ä¢ Dataset: Video SAR MOT Benchmark (VSMB), Samples: 45, Modality: Video SAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.11774"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Indian Institute of Technology Kanpur<br>
‚Ä¢ Dataset: Isometric-Multiclass Dataset (IMCD), Samples: 4339, Modality: 2D pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>RationalVLA: A Rational Vision-Language-Action Model with Dual System</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.10826"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://irpn-eai.github.io/RationalVLA/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: RAtional MAnipulation (RAMA), Samples: 14412, Modality: Language instructions + RGB images + robot arm kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial Motion Capture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.10580"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZuoCX1996/TIC"><img src="https://img.shields.io/github/stars/ZuoCX1996/TIC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xiamen University, China<br>
‚Ä¢ Dataset: DSTIC, Samples: 1.04M sequences, Modality: Optical motion capture (body pose, absolute IMU orientation and acceleration), raw IMU readings<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.10353"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://Motion-R1.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: GigaAI<br>
‚Ä¢ Dataset: MotionCoT Data Engine, Samples: None, Modality: Chain-of-Thought (CoT) annotations paired with existing motion descriptions and motion sequences (MoCap)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.09997"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta, UC Merced<br>
‚Ä¢ Dataset: Customized Kubric dataset, Samples: 40000, Modality: Synthetic multi-view videos + 3D scene flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.09663"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Anhui University<br>
‚Ä¢ Dataset: RS-Art, Samples: 18 objects (6 categories, 3 instances each), each with 7 distinct articulation states and over 400 observations per instance., Modality: RGB-D captures, camera poses (intrinsics/extrinsics), reverse-engineered 3D models (USD, URDF, PLY) with part meshes, textures, joint definitions, and physics properties.<br>
‚Ä¢ Dataset: PartNet-Mobility (extended), Samples: Extended with 2-3 new instances per category across 8 categories, plus 4 objects with three movable parts., Modality: Synthetic 3D models with articulation data.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Synthetic Human Action Video Data Generation with Pose Transfer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.09411"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="synthetic-human-action.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SSPS<br>
‚Ä¢ Dataset: RANDOM People, Samples: 3600, Modality: Synthetic RGB videos + 3D Gaussian avatars + source identity videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>iTACO: Interactable Digital Twins of Articulated Objects from Casually Captured RGBD Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.08334"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://3dlg-hcvc.github.io/video2articulation/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Simon Fraser University<br>
‚Ä¢ Dataset: None, Samples: 784, Modality: RGBD videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Audio-Sync Video Generation with Multi-Stream Temporal Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.08003"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hjzheng.net/projects/MTV/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing Academy of Artificial Intelligence<br>
‚Ä¢ Dataset: DEMIX, Samples: 392000, Modality: RGB videos + demixed audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07865"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vLAR-group/FreeGave"><img src="https://img.shields.io/github/stars/vLAR-group/FreeGave.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: vLAR Group, The Hong Kong Polytechnic University<br>
‚Ä¢ Dataset: FreeGave-GoPro Dataset, Samples: 6, Modality: multi-view RGB videos with camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07860"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/uzh-rpg/event_based_ping_pong_ball_trajectory_prediction"><img src="https://img.shields.io/github/stars/uzh-rpg/event_based_ping_pong_ball_trajectory_prediction.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics and Perception Group, University of Zurich, Switzerland<br>
‚Ä¢ Dataset: Egocentric Event-Based Ping Pong Trajectories, Samples: 30, Modality: 3D ground-truth ball trajectories, event streams, RGB videos, eye-tracking, IMU, audio, SLAM poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07603"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: SurgBench, Samples: 225250, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>OpenDance: Multimodal Controllable 3D Dance Generation Using Large-scale Internet Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07565"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://open-dance.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: OpenDance5D, Samples: 41000, Modality: RGB video, audio waveform, 2D skeletal keypoints, 3D SMPL motion, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Improving Traffic Signal Data Quality for the Waymo Open Motion Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.07150"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/michigan-traffic-lab/WOMD-Traffic-Signal-Data-Improvement"><img src="https://img.shields.io/github/stars/michigan-traffic-lab/WOMD-Traffic-Signal-Data-Improvement.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Civil and Environmental Engineering, University of Michigan, Ann Arbor, MI, USA<br>
‚Ä¢ Dataset: Waymo Open Motion Dataset - Improved Traffic Signal Data, Samples: over 360,000 scenarios, Modality: Vehicle trajectories with imputed traffic signal states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.06596"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://fulmen67.github.io/EV-LayerSegNet"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Advanced Research and Innovation Center, Khalifa University<br>
‚Ä¢ Dataset: Not explicitly named in the paper, Samples: 1025, Modality: simulated event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>(LiFT) Lightweight Fitness Transformer: A language-vision model for Remote Monitoring of Physical Training</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.06480"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Electrical and Computer Engineering University of California, San Diego<br>
‚Ä¢ Dataset: Olympia, Samples: 7618, Modality: RGB videos + 3D skeletal keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.06318"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southeast University<br>
‚Ä¢ Dataset: GyroPeak-100, Samples: None, Modality: IMU signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>ChronoTailor: Harnessing Attention Guidance for Fine-Grained Video Virtual Try-On</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.05858"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Communication University of China<br>
‚Ä¢ Dataset: StyleDress, Samples: 12500, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Gen4D: Synthesizing Humans and Scenes in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.05397"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Vision and Image Processing Lab, University of Waterloo, Canada<br>
‚Ä¢ Dataset: SportPAL, Samples: 2012, Modality: Synthetic images with 2D/3D human pose annotations and SMPLX parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>FreeTimeGS: Free Gaussian Primitives at Anytime and Anywhere for Dynamic Scene Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.05348"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zju3dv.github.io/freetimegs/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: SelfCap, Samples: 8, Modality: multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>FRED: The Florence RGB-Event Drone Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.05163"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://miccunifi.github.io/FRED/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Florence<br>
‚Ä¢ Dataset: FRED (Florence RGB-Event Drone dataset), Samples: 4620, Modality: RGB videos and event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.04224"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://oxdan.active.vision/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Oxford<br>
‚Ä¢ Dataset: Oxford Day-and-Night, Samples: 491750, Modality: Egocentric RGB/grayscale video + IMU + 6DoF camera poses + 3D point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EV-Flying: an Event-based Dataset for In-The-Wild Recognition of Flying Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.04048"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Florence<br>
‚Ä¢ Dataset: EV-Flying, Samples: 3206, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Controllable Human-centric Keyframe Interpolation with Generative Prior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.03119"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gseancdat.github.io/projects/PoseFuse3D_KI"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: CHKI-Video, Samples: 2614, Modality: RGB videos + 2D poses + 3D SMPL-X parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.03107"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ByteDance-Seed/BM-code"><img src="https://img.shields.io/github/stars/ByteDance-Seed/BM-code.svg?style=social&label=Star"></a><br><a href="https://boese0601.github.io/bytemorph"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ByteDance Seed<br>
‚Ä¢ Dataset: ByteMorph-6M, Samples: 6450000, Modality: image pairs + text instructions<br>
‚Ä¢ Dataset: ByteMorph-Bench, Samples: 613, Modality: image pairs + text instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.02845"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LEI-QI-233/HAR-in-Space"><img src="https://img.shields.io/github/stars/LEI-QI-233/HAR-in-Space.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Karlsruhe Institute of Technology<br>
‚Ä¢ Dataset: MicroG-4M, Samples: 4759, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.02733"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/LecterF/LinkTo-Anime"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Macau University of Science and Technology, Macau, China<br>
‚Ä¢ Dataset: LinkTo-Anime, Samples: 395, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>InterRVOS: Interaction-aware Referring Video Object Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.02356"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cvlab-kaist.github.io/InterRVOS"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST AI<br>
‚Ä¢ Dataset: InterRVOS-8K, Samples: 8738, Modality: RGB videos + language expressions + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>FreeTacMan: Robot-free Visuo-Tactile Data Collection System for Contact-rich Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.01941"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://opendrivelab.com/FreeTacMan"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Innovation Institute<br>
‚Ä¢ Dataset: FreeTacMan Dataset, Samples: 10k demonstration trajectories, Modality: visuo-tactile images + end-effector poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.01674"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nju-pcalab.github.io/projects/MotionSight"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: MotionVid-QA, Samples: 40000, Modality: RGB videos + QA pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.01608"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/amathislab/EPFL-Smart-Kitchen"><img src="https://img.shields.io/github/stars/amathislab/EPFL-Smart-Kitchen.svg?style=social&label=Star"></a><br><a href="https://github.com/amathislab/EPFL-Smart-Kitchen"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: √âcole Polytechnique F√©d√©rale de Lausanne (EPFL), Lausanne<br>
‚Ä¢ Dataset: EPFL-Smart-Kitchen-30, Samples: 60189, Modality: RGB-D videos, IMU, eye gaze, 3D hand poses, 3D body poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.00411"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: LoHoSet, Samples: 23000, Modality: RGB-D images, language instructions, robot actions (end-effector Cartesian positions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>Sequence-Based Identification of First-Person Camera Wearers in Third-Person Views</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.00394"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Indiana University Bloomington<br>
‚Ä¢ Dataset: TF2025, Samples: None, Modality: Synchronized first- and third-person videos with segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>MotionPersona: Characteristics-aware Locomotion Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.00173"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motionpersona25.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: MotionPersona, Samples: 3150, Modality: MoCap joints (SMPL-X)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2025</td>
  <td style="width:70%;"><strong>From Motion to Behavior: Hierarchical Modeling of Humanoid Generative Behavior Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2506.00043"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sun Yat-sen University<br>
‚Ä¢ Dataset: GBC-100K, Samples: 123700, Modality: RGB videos + SMPL parameters + hierarchical textual annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Spatiotemporal Analysis of Forest Machine Operations Using 3D Video Classification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.24375"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.nibio.no/maciekwielgosz/forest_video_classification"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Norwegian Institute of Bioeconomy Research (NIBIO)<br>
‚Ä¢ Dataset: Forest Machine Operations Video Dataset, Samples: 349, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.24249"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China<br>
‚Ä¢ Dataset: bronchoscopy localization dataset, Samples: 66, Modality: Bronchoscopy videos + annotated 6-DOF camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Autoregressive Meta-Actions for Unified Controllable Trajectory Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.23612"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://arma-traj.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: Waymo Motion Dataset with frame-level meta-action annotations, Samples: None, Modality: Agent trajectories and map data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.23525"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xyz123xyz456/hallo4"><img src="https://img.shields.io/github/stars/xyz123xyz456/hallo4.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: Audio-Driven Portrait DPO Dataset, Samples: 20000, Modality: RGB video pairs with human preference labels (motion-video alignment, portrait fidelity)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Semantics-Aware Human Motion Generation from Audio Instructions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.23465"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Chinese Academy of Sciences, China<br>
‚Ä¢ Dataset: Oral Dataset (from HumanML3D), Samples: 87384, Modality: audio instructions + MoCap-based pose sequences<br>
‚Ä¢ Dataset: Oral Dataset (from KIT-ML), Samples: 12696, Modality: audio instructions + MoCap-based pose sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.22977"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vivocameraresearch.github.io/hypermotion/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Centre for Computer Animation, Bournemouth University, UK<br>
‚Ä¢ Dataset: Open-HyperMotionX Dataset, Samples: 19597, Modality: RGB videos + 2D pose annotations<br>
‚Ä¢ Dataset: HyperMotionX Bench, Samples: 100, Modality: RGB videos + 2D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.22859"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://muskie82.github.io/4dtam/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dyson Robotics Laboratory, Imperial College London<br>
‚Ä¢ Dataset: Sim4D, Samples: 50, Modality: RGB images, depth, surface normals, foreground masks, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.22769"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of St Andrews<br>
‚Ä¢ Dataset: MotionGaze, Samples: 803K+ IMU readings, Modality: IMU (accelerometer, gyroscope), RGB images, Gaze coordinates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.22335"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://aczheng-cai.github.io/up_slam.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Chinese Academy of Sciences<br>
‚Ä¢ Dataset: Bonn RGB-D Dynamic Dataset masks, Samples: None, Modality: Dynamic object masks for RGB-D sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.21653"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bwgzk-keke.github.io/DiffPhy/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Johns Hopkins University<br>
‚Ä¢ Dataset: PhyHQ, Samples: 8000, Modality: RGB videos + text captions + physical phenomena labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Frame In-N-Out: Unbounded Controllable Image-to-Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.21491"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://uva-computer-vision-lab.github.io/Frame-In-N-Out/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Virginia<br>
‚Ä¢ Dataset: Frame In-N-Out Training Dataset, Samples: 72300, Modality: RGB videos with processed metadata including motion trajectories, text prompts, bounding boxes, and identity reference images<br>
‚Ä¢ Dataset: Frame In-N-Out Evaluation Benchmark, Samples: 372, Modality: RGB videos with processed metadata including motion trajectories, text prompts, bounding boxes, and identity reference images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>RefAV: Towards Planning-Centric Scenario Mining</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.20981"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="cainand.github.io/RefAV/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: RefAV, Samples: 1000, Modality: LiDAR, 360¬∞ ring cameras, HD maps, 3D track annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.20460"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: VCIP, CS, Nankai University, Horizon Robotics<br>
‚Ä¢ Dataset: PM-X, Samples: 600, Modality: Dual-state rendered images, URDF annotations, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>HAODiff: Human-Aware One-Step Diffusion via Dual-Prompt Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.19742"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gobunu/HAODiff"><img src="https://img.shields.io/github/stars/gobunu/HAODiff.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: MPII-Test, Samples: 5427, Modality: RGB images with human motion blur<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.19386"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nate-gillman/force-prompting"><img src="https://img.shields.io/github/stars/nate-gillman/force-prompting.svg?style=social&label=Star"></a><br><a href="https://force-prompting.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: Global wind force dataset, Samples: 15000, Modality: Synthetic RGB videos + Force vectors<br>
‚Ä¢ Dataset: Local point force dataset, Samples: 23000, Modality: Synthetic RGB videos + Force vectors + Application points<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>PosePilot: An Edge-AI Solution for Posture Correction in Physical Exercises</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.19186"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gadhvirushiraj/PosePilot"><img src="https://img.shields.io/github/stars/gadhvirushiraj/PosePilot.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: HTI Lab, Plaksha University, Mohali, India<br>
‚Ä¢ Dataset: In-house Dataset, Samples: 336, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>EventEgoHands: Event-based Egocentric 3D Hand Mesh Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.19169"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Keio University<br>
‚Ä¢ Dataset: N-HOT3D, Samples: 447704, Modality: synthetic event streams + 3D hand mesh annotations (MANO parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>BiomechGPT: Towards a Biomechanically Fluent Multimodal Foundation Model for Clinically Relevant Motion Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.18465"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Neuroscience, Northwestern University; Shirley Ryan AbilityLab<br>
‚Ä¢ Dataset: Not explicitly named, Samples: 27000 motion-question-answer pairs, Modality: Biomechanical trajectories (joint angles and velocities)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.17677"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ophnet-3d.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Monash University<br>
‚Ä¢ Dataset: OphNet-3D, Samples: 41, Modality: Multi-view RGB-D videos with annotated MANO hand meshes and 6-DoF instrument poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.17201"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Informatics Institute, University of Amsterdam<br>
‚Ä¢ Dataset: Fish Data, Samples: 3796, Modality: RGB videos + MOT annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.17114"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/BASHLab/RAVEN"><img src="https://img.shields.io/github/stars/BASHLab/RAVEN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical & Computer Engineering
Worcester Polytechnic Institute<br>
‚Ä¢ Dataset: AVS-QA, Samples: 300000, Modality: RGB video + audio + IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.16602"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Peking University<br>
‚Ä¢ Dataset: MEgoHand Unified Dataset, Samples: 24000, Modality: RGB-D frames + MANO hand parameters + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.16249"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China.<br>
‚Ä¢ Dataset: DOFS (Deformable Object with Full Spatial information), Samples: 120 episodes (70 simulation, 50 real-world), Modality: multi-view RGB-D images, robot gripper poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.15287"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://intothemild.github.io/GS2E.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Computer Engineering, Peking University<br>
‚Ä¢ Dataset: GS2E, Samples: 1150, Modality: RGB frames, event streams, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Intentional Gesture: Deliver Your Intentions with Gestures for Speech</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.15197"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://andypinxinliu.github.io/Intentional-Gesture"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Rochester<br>
‚Ä¢ Dataset: InG (Intention-Grounded Gestures), Samples: 47913, Modality: SMPL-based gesture data + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.14866"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nisarganc.github.io/UPTor-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Bosch Corporate Research, Robert Bosch GmbH, Stuttgart, Germany, University of Technology Nuremberg (UTN), Germany<br>
‚Ä¢ Dataset: DARKO, Samples: 508, Modality: 3D Poses from RGB<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Learning collision risk proactively from naturalistic driving data at scale</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.13556"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yiru-Jiao/GSSM"><img src="https://img.shields.io/github/stars/Yiru-Jiao/GSSM.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.15787/VTT1/EFYEJR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Delft University of Technology<br>
‚Ä¢ Dataset: Bird‚Äôs eye view trajectory reconstruction of naturalistic crashes and near-crashes in the SHRP2 NDS, Samples: 6664, Modality: Bird's eye view trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>eStonefish-scenes: A synthetically generated dataset for underwater event-based optical flow prediction tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.13309"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/CIRS-Girona/estonefish-scenes"><img src="https://img.shields.io/github/stars/CIRS-Girona/estonefish-scenes.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/15130453"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Vision and Robotics Research Institute (ViCOROB), Universitat de Girona (UdG), Spain<br>
‚Ä¢ Dataset: eStonefish-scenes, Samples: 8, Modality: event streams, grayscale images, ground truth optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Event-Driven Dynamic Scene Depth Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.13279"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NUS<br>
‚Ä¢ Dataset: EventDC-Real, Samples: 15845, Modality: Color images, LiDAR, Event streams<br>
‚Ä¢ Dataset: EventDC-SemiSyn, Samples: 9307, Modality: Color images, LiDAR, Event streams<br>
‚Ä¢ Dataset: EventDC-FullSyn, Samples: 21500, Modality: Color images, Depth data, Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>FlowCut: Unsupervised Video Instance Segmentation via Temporal Mask Matching</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.13174"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Vision Group, University of Bern, Switzerland<br>
‚Ä¢ Dataset: FlowCut pseudo-labeled video dataset, Samples: 167365, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.12774"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Western Australia<br>
‚Ä¢ Dataset: UniHM Dataset, Samples: 44962, Modality: SMPL motion + text descriptions + scene voxels + object point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Event-based Star Tracking under Spacecraft Jitter: the e-STURT Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.12588"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zenodo.org/records/14031911"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Australian Institute for Machine Learning<br>
‚Ä¢ Dataset: e-STURT (Event-based Star Tracking Under Jitter), Samples: 200, Modality: event streams + piezoelectric actuator telemetry<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.11920"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University<br>
‚Ä¢ Dataset: H2R-UR5-SSv2-1M-Gripper, Samples: 62500, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-SSv2-1M-Leaphand, Samples: 62500, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-SSv2-1M-Mix, Samples: 62500, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-Franka-SSv2-1M, Samples: 62500, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-Ego4D-1M-Gripper, Samples: 117624, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-Ego4D-1M-Leaphand, Samples: 117624, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-UR5-Ego4D-1M-Mix, Samples: 117624, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
‚Ä¢ Dataset: H2R-Franka-Ego4D-1M, Samples: 117624, Modality: Augmented RGB videos, 3D hand keypoints, camera parameters, robot joint configurations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.11868"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National University of Defense Technology<br>
‚Ä¢ Dataset: MonoMobility Dataset, Samples: 26, Modality: monocular videos, 3D point clouds, motion part segmentation, motion attribute annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ElderFallGuard: Real-Time IoT and Computer Vision-Based Fall Detection System for Elderly Safety</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.11845"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information and Communicaton Technology, Bangladesh University of Engineering Technology, Dhaka, bangladesh<br>
‚Ä¢ Dataset: ElderFallGuard dataset, Samples: 7200, Modality: RGB images + 2D pose landmarks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.11282"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="github.com/shrutarv/MTevent_toolkit"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Dortmund<br>
‚Ä¢ Dataset: MTevent, Samples: 75, Modality: Stereo event camera streams, RGB videos, MoCap poses, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Robust 2D lidar-based SLAM in arboreal environments without IMU/GNSS</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.10847"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Pontificia Universidad Cat ¬¥olica de Chile<br>
‚Ä¢ Dataset: Pullally, Samples: None, Modality: LiDAR, RTK-GNSS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.10696"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tartanair.org/tartanground"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotic Systems Lab, ETH Zurich, Zurich, Switzerland<br>
‚Ä¢ Dataset: TartanGround, Samples: 910 trajectories / 1.5 million samples, Modality: RGB stereo video, depth, optical flow, stereo disparity, LiDAR point clouds, ground truth poses, semantic segmented images, occupancy maps, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.10205"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universitat de Barcelona, Spain<br>
‚Ä¢ Dataset: Foodkit, Samples: 21, Modality: RGB videos, image sequences, camera pose trajectories, segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.09694"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AgibotTech/EWMBench"><img src="https://img.shields.io/github/stars/AgibotTech/EWMBench.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AgiBot, HIT<br>
‚Ä¢ Dataset: EWMBENCH Dataset, Samples: 100, Modality: RGB videos, robot end-effector trajectories, textual instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.08986"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Arkansas, Fayetteville, AR, USA<br>
‚Ä¢ Dataset: ChicGrasp, Samples: 50, Modality: RGB video, robot proprioception (joint positions, joint velocities), binary gripper states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>RDD: Robust Feature Detector and Descriptor using Deformable Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.08013"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://xtcpete.github.io/rdd/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Creative Technologies, University of Southern California<br>
‚Ä¢ Dataset: MegaDepth-View, Samples: 1487, Modality: RGB images + camera poses + depth maps<br>
‚Ä¢ Dataset: Air-to-Ground, Samples: 1500, Modality: RGB images + camera poses + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>BETTY Dataset: A Multi-modal Dataset for Full-Stack Autonomy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.07266"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pitt-mit-iac.github.io/betty-dataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics Institute, Carnegie Mellon University<br>
‚Ä¢ Dataset: BETTY Dataset, Samples: None, Modality: Camera, LiDAR, Radar, IMU, GNSS, Odometry, Planned Trajectories, Actuation Commands (Throttle, Brake, Steering), Tire State (temperature, pressure, speed, torque, slip angle), Suspension<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.07007"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zyzhangUstc/MELLM"><img src="https://img.shields.io/github/stars/zyzhangUstc/MELLM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, University of Science and Technology of China<br>
‚Ä¢ Dataset: Instruction-following MEU Dataset, Samples: 4793, Modality: Instruction-description pairs and motion-enhanced color maps derived from optical flow of video frames (based on the DFME dataset).<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.06537"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence, Beihang University; The Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: MRFashion-7K, Samples: 7335, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.05001"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nie-lang/StabStitch2"><img src="https://img.shields.io/github/stars/nie-lang/StabStitch2.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Science, Beijing Jiaotong University<br>
‚Ä¢ Dataset: StabStitch-D, Samples: over 100 video pairs, Modality: RGB video pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.04974"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wengwanjiang.github.io/ReAlign-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Engineering, Southeast University, Nanjing, China<br>
‚Ä¢ Dataset: BiHumanML3D, Samples: 13312, Modality: 3D motion sequences + bilingual (English/Chinese) text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.04203"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Qzping/ELGAR"><img src="https://img.shields.io/github/stars/Qzping/ELGAR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Central Conservatory of Music, China and Tsinghua University, China<br>
‚Ä¢ Dataset: SPD-GEN, Samples: 81, Modality: MoCap joint rotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>A Dataset and Toolkit for Multiparameter Cardiovascular Physiology Sensing on Rings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.04172"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/thuhci/RingTool"><img src="https://img.shields.io/github/stars/thuhci/RingTool.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: ùúè-Ring, Samples: 28.21 hours of raw data from 34 subjects, Modality: PPG (infrared and red channels), 3-axis accelerometer data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.04002"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mshoe/PARC"><img src="https://img.shields.io/github/stars/mshoe/PARC.svg?style=social&label=Star"></a><br><a href="https://github.com/mshoe/PARC"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Simon Fraser University, Canada<br>
‚Ä¢ Dataset: Initial Parkour Dataset, Samples: None, Modality: MoCap joints with contact labels<br>
‚Ä¢ Dataset: PARC Generated Dataset, Samples: approximately 3000 sequences, Modality: Physics-corrected kinematic motions with contact labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03738"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://amo-humanoid.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC San Diego<br>
‚Ä¢ Dataset: AMO dataset, Samples: None, Modality: Dynamically-feasible whole-body reference joint angles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>A Unit Enhancement and Guidance Framework for Audio-Driven Avatar Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03603"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: CNAS (Chinese News Anchor Speech Dataset), Samples: 1473, Modality: RGB videos + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>AquaticVision: Benchmarking Visual SLAM in Underwater Environment with Events and Frames</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03448"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/aquaticvision-lias"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Data Science, The Chinese University of Hong Kong, Shenzhen, P. R. China<br>
‚Ä¢ Dataset: AquaticVision, Samples: 9, Modality: Stereo event streams, stereo grayscale frames, IMU, motion capture trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03154"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Simon Fraser University, Canada<br>
‚Ä¢ Dataset: BrokenAMASS, Samples: over 40 hours, Modality: MoCap skeleton<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.03116"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Lab of Multispectral Information Intelligent Processing Technology School of Artificial Intelligence and Automation, Huazhong University of Science and Technology<br>
‚Ä¢ Dataset: Complex, High-speed Motion (CHMD), Samples: None, Modality: RGB videos + Events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.01838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://kevinlee09.github.io/research/MVHumanNet++/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen<br>
‚Ä¢ Dataset: MVHumanNet++, Samples: 60000, Modality: Multi-view RGB videos, human masks, camera parameters, 2D/3D keypoints, SMPL/SMPLX parameters, normal maps, depth maps, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.01746"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mattie-e.github.io/Co3/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: GES-Inter, Samples: 27390, Modality: 3D human postures (SMPL-X) + separated audio + text transcripts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.00866"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kocurvik/rdnet"><img src="https://img.shields.io/github/stars/kocurvik/rdnet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Mathematics, Physics and Informatics, Comenius University Bratislava, Bratislava, Slovakia.<br>
‚Ä¢ Dataset: ROTUNDA and CATHEDRAL, Samples: 2891, Modality: RGB images + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>SOTA: Spike-Navigated Optimal TrAnsport Saliency Region Detection in Composite-bias Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.00394"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lwxfight/sota"><img src="https://img.shields.io/github/stars/lwxfight/sota.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory for Multimedia Information Processing, Peking University; Hubei Key Laboratory of Transportation Internet of Things, Wuhan University of Technology<br>
‚Ä¢ Dataset: SPIKE-DAVIS, Samples: None, Modality: Synthetic spike streams from interpolated videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2025</td>
  <td style="width:70%;"><strong>Fine-grained spatial-temporal perception for gas leak segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2505.00295"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of British Columbia - Okanagan<br>
‚Ä¢ Dataset: GasVid, Samples: 187, Modality: infrared videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.21718"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing University of Posts and Telecommunications, Beijing, China<br>
‚Ä¢ Dataset: ListenerX, Samples: 6683, Modality: 3D parametric head model (FLAME) sequences, speaker audio, text descriptions, emotion intensity tags<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.20808"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bit-bots.github.io/SoccerDiffusion"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Hamburg<br>
‚Ä¢ Dataset: SoccerDiffusion Dataset, Samples: 88, Modality: RGB images, IMU rotations, joint states, joint commands, game state<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.20234"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UP-COUNT/tracking"><img src="https://img.shields.io/github/stars/UP-COUNT/tracking.svg?style=social&label=Star"></a><br><a href="https://up-count.github.io/tracking"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Robotics and Machine Intelligence, Pozna ¬¥n University of Technology<br>
‚Ä¢ Dataset: UP-COUNT-TRACK, Samples: 31, Modality: RGB videos + trajectory annotations + sensor metadata<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.19654"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Loughborough University, Epinal Way, Loughborough, LE11 3TU, Leicestershire, United Kingdom<br>
‚Ä¢ Dataset: DRL-generated 2D SLAM error dataset (unnamed in paper), Samples: 75000, Modality: 2D Occupancy Grid Maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.19514"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Lappeenranta-Lahti University of Technology LUT, Finland<br>
‚Ä¢ Dataset: FSAnno, Samples: 783, Modality: Motion data, skeleton data, RGB video, audio, text annotations<br>
‚Ä¢ Dataset: FSBench, Samples: 783, Modality: Motion data, skeleton data, RGB video, audio, text annotations (QA pairs, comments, scores)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.18864"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: Particle Scenes with Spike and Displacement (PSSD), Samples: 30000 sample sequences (10,000 for each of 3 sub-datasets), Modality: Spike streams + ground truth displacement fields + images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>E-VLC: A Real-World Dataset for Event-based Visible Light Communication And Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.18521"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://woven-visionai.github.io/evlc-dataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Woven by Toyota<br>
‚Ä¢ Dataset: E-VLC, Samples: 110, Modality: Event camera streams, Frame-based videos, Ground-truth camera poses, LED marker positions, Bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Dynamic Camera Poses and Where to Find Them</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17788"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://research.nvidia.com/labs/dir/dynpose-100k"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA, University of Michigan<br>
‚Ä¢ Dataset: DynPose-100K, Samples: 100131, Modality: RGB videos + camera poses<br>
‚Ä¢ Dataset: Lightspeed, Samples: 36, Modality: Synthetic videos + ground truth camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Casual3DHDR: Deblurring High Dynamic Range 3D Gaussian Splatting from Casually Captured Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17728"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Westlake University, Hangzhou, China; Wuhan University, Wuhan, China<br>
‚Ä¢ Dataset: Synthetic Dataset (Blender), Samples: 4, Modality: Synthetic RGB videos with motion blur from continuous camera trajectories<br>
‚Ä¢ Dataset: CasualVideo, Samples: 6, Modality: Real-world RGB videos with auto-exposure, motion blur, and Vicon motion capture poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17414"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://2y7c3.github.io/3DV-TON/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DAMO Academy, Alibaba Group; Hupan Lab<br>
‚Ä¢ Dataset: HR-VVT, Samples: 130, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17371"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://deepscenario.github.io/DSC3D/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DeepScenario, TU Munich, Munich Center for Machine Learning<br>
‚Ä¢ Dataset: DeepScenario Open 3D Dataset (DSC3D), Samples: 175000, Modality: 6DoF bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Contrastive Learning for Continuous Touch-Based Authentication</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.17271"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: North China University of Technology<br>
‚Ä¢ Dataset: Ffinger, Samples: interaction data from 29 participants, Modality: Multi-channel time series of finger trajectories (coordinates, timestamp, pressure, area, velocity, direction)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Advancing Radar Hand Gesture Recognition: A Hybrid Spectrum Synthetic Framework Merging Simulation with Neural Networks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.16423"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Institute of Health Data Science, Peking University<br>
‚Ä¢ Dataset: unnamed radar-vision HGR dataset, Samples: None, Modality: mmWave radar signals, 3D hand joint coordinates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>SILM: A Subjective Intent Based Low-Latency Framework for Multiple Traffic Participants Joint Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.16377"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory of General Artificial Intelligence, Key Laboratory of Machine Perception (MoE), School of Intelligence Science and Technology, Peking University, Beijing 100871, China.<br>
‚Ä¢ Dataset: CAIC-TP, Samples: more than 25000 sequences, Modality: Trajectory sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.15786"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gdaosu.github.io/sat2groundscape"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Ohio State University<br>
‚Ä¢ Dataset: Sat2GroundScape, Samples: 45000 sequences (estimated from 90 scenes * ~500 sequences/scene); containing 25,000+ panoramic pairs and 100,000+ perspective pairs, Modality: Satellite images, ground-view panoramic/perspective images, depth maps, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Towards Understanding Camera Motions in Any Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.15376"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/linzhiqiu/camerabench"><img src="https://img.shields.io/github/stars/linzhiqiu/camerabench.svg?style=social&label=Star"></a><br><a href="https://linzhiqiu.github.io/papers/camerabench"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CMU<br>
‚Ä¢ Dataset: CameraBench, Samples: 3381, Modality: RGB videos + camera motion labels + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.14602"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://k2muse.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; University of Chinese Academy of Sciences, Beijing, China<br>
‚Ä¢ Dataset: K2MUSE, Samples: None, Modality: kinematics (MoCap markers), kinetics (force plates), amplitude-mode ultrasound (AUS), surface electromyography (sEMG)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>MILUV: A Multi-UAV Indoor Localization dataset with UWB and Vision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.14376"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/decargroup/miluv"><img src="https://img.shields.io/github/stars/decargroup/miluv.svg?style=social&label=Star"></a><br><a href="https://decargroup.github.io/miluv/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: McGill University<br>
‚Ä¢ Dataset: MILUV, Samples: 36, Modality: UWB, stereo vision, monocular vision, IMU, laser rangefinder, magnetometer, MoCap poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Adversarial Locomotion and Motion Imitation for Humanoid Policy Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.14305"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://almi-humanoid.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Artificial Intelligence (TeleAI), China Telecom<br>
‚Ä¢ Dataset: ALMI-X, Samples: 81,549 trajectories, Modality: Robot kinematic trajectories (states, actions, joint angles, etc.) with language descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>SLAM&Render: A Benchmark for the Intersection Between Neural Rendering, Gaussian Splatting and SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.13713"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://samuel-cerezo.github.io/SLAM&Render"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Departamento de Inform√°tica e Ingenier√≠a de Sistemas, Universidad de Zaragoza, 50018 Zaragoza, Spain<br>
‚Ä¢ Dataset: SLAM&Render, Samples: 40, Modality: RGB-D images, IMU readings, robot kinematic data, ground-truth pose streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.12284"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bit.ly/LatentAct"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Illinois Urbana-Champaign<br>
‚Ä¢ Dataset: HoloAssist with 3D hand motion and contact annotations, Samples: 15000, Modality: 3D hand pose trajectories + contact maps (from egocentric RGB videos)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Coding-Prior Guided Diffusion Network for Video Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.12222"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="Not provided in the paper."><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the paper.<br>
‚Ä¢ Dataset: GoPro dataset augmented with coding priors, Samples: 3214 image pairs, Modality: RGB videos + Motion Vectors + Coding Residuals<br>
‚Ä¢ Dataset: DVD dataset augmented with coding priors, Samples: 71 videos, Modality: RGB videos + Motion Vectors + Coding Residuals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>CodingHomo: Bootstrapping Deep Homography With Video Coding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.12165"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/liuyike422/CodingHomo"><img src="https://img.shields.io/github/stars/liuyike422/CodingHomo.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information and Communication Engineering, University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: CA-unsup, Samples: 446200, Modality: Image pairs + motion vectors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Action Anticipation from SoccerNet Football Video Broadcasts</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.12021"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MohamadDalal/FAANTRA"><img src="https://img.shields.io/github/stars/MohamadDalal/FAANTRA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Aalborg University<br>
‚Ä¢ Dataset: SoccerNet Ball Action Anticipation dataset, Samples: 12433, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>RESPLE: Recursive Spline Estimation for LiDAR-Based Odometry</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.11580"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ASIG-X/RESPLE"><img src="https://img.shields.io/github/stars/ASIG-X/RESPLE.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Link√∂ping University, Sweden<br>
‚Ä¢ Dataset: HelmDyn, Samples: 10, Modality: LiDAR, IMU, MoCap<br>
‚Ä¢ Dataset: R-Campus, Samples: 1, Modality: LiDAR, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.11521"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: INTER DRIVE, Samples: 150000, Modality: Agent trajectories + natural language annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.11326"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pvuw.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Big Data, Fudan University<br>
‚Ä¢ Dataset: MOSE (extended test set), Samples: None, Modality: RGB videos + segmentation masks<br>
‚Ä¢ Dataset: MeViS (extended test set), Samples: None, Modality: RGB videos + language expressions + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.10905"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: InterHF, Samples: 90000, Modality: annotated video clips<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>SeeTree -- A modular, open-source system for tree detection and orchard localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.10764"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Jostan86/pf_orchard_localization"><img src="https://img.shields.io/github/stars/Jostan86/pf_orchard_localization.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Collaborative Robotics and Intelligent Systems (CoRIS) Institute, Oregon State University<br>
‚Ä¢ Dataset: SeeTree Dataset, Samples: 55, Modality: IMU, GNSS, RGB-D, wheel odometry<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>HUMOTO: A 4D Dataset of Mocap Human Object Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.10414"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jiaxin-lu.github.io/humoto/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Texas at Austin, Adobe Research<br>
‚Ä¢ Dataset: HUMOTO, Samples: 736, Modality: MoCap (full-body and hands), RGB-D videos, 3D object models, text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>RGB-Event based Pedestrian Attribute Recognition: A Benchmark Dataset and An Asymmetric RWKV Fusion Framework</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.10018"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/OpenPAR"><img src="https://img.shields.io/github/stars/Event-AHU/OpenPAR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: EventPAR, Samples: 100000, Modality: RGB frames and event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-wave Point Cloud Sequence</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.09862"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University, Bytedance Research<br>
‚Ä¢ Dataset: virtual radar-text dataset, Samples: 13308, Modality: mmWave radar point cloud sequence + text<br>
‚Ä¢ Dataset: real test dataset, Samples: 375, Modality: mmWave radar point cloud sequence + text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>CamMimic: Zero-Shot Image To Camera Motion Personalized Video Generation Using Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.09472"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cammimic.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland College Park<br>
‚Ä¢ Dataset: None, Samples: 680, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Low-Light Image Enhancement using Event-Based Illumination Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.09379"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: INSAIT, Sofia University<br>
‚Ä¢ Dataset: EvLowLight, Samples: 60, Modality: RGB images + temporal-mapping events + motion events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>A Constrained Optimization Approach for Gaussian Splatting from Coarsely-posed Images and Noisy Lidar Point Clouds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.09129"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: dConstruct Robotics<br>
‚Ä¢ Dataset: Multi-camera SLAM Dataset, Samples: 4, Modality: RGB images, IMU, Lidar, Pose Trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.08222"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/F3Set/F3Set"><img src="https://img.shields.io/github/stars/F3Set/F3Set.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ningbo University<br>
‚Ä¢ Dataset: F3Set (tennis), Samples: 11584, Modality: RGB videos + event annotations<br>
‚Ä¢ Dataset: F3Set (badminton), Samples: 112, Modality: RGB videos + event annotations<br>
‚Ä¢ Dataset: F3Set (table tennis), Samples: 42, Modality: RGB videos + event annotations<br>
‚Ä¢ Dataset: F3Set (tennis doubles), Samples: 78, Modality: RGB videos + event annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>RealCam-Vid: High-resolution Video Dataset with Dynamic Scenes and Metric-scale Camera Movements</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.08212"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZGCTroy/RealCam-Vid"><img src="https://img.shields.io/github/stars/ZGCTroy/RealCam-Vid.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science Zhejiang University<br>
‚Ä¢ Dataset: RealCam-Vid, Samples: , Modality: High-resolution videos + metric-scale camera parameters + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Multi-person Physics-based Pose Estimation for Combat Sports</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.08175"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ¬¥Ecole de technologie sup ¬¥erieure, Montreal, Canada<br>
‚Ä¢ Dataset: Elite Boxing Footage, Samples: Over 20 minutes of video footage, Modality: Multi-view RGB videos<br>
‚Ä¢ Dataset: Supplementary Dataset, Samples: None, Modality: Multi-view RGB videos + MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Towards Unconstrained 2D Pose Estimation of the Human Spine</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.08110"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://saifkhichi96.github.io/research/spinepose/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: German Research Center for Artificial Intelligence (DFKI)<br>
‚Ä¢ Dataset: SpineTrack, Samples: 58766, Modality: RGB images + 2D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>FMNV: A Dataset of Media-Published News Videos for Fake News Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.07687"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DennisIW/FMNV"><img src="https://img.shields.io/github/stars/DennisIW/FMNV.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the paper<br>
‚Ä¢ Dataset: FMNV, Samples: 2393, Modality: RGB videos, audio, text titles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>S2R-HDR: A Large-Scale Rendered Dataset for HDR Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.07667"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://openimaginglab.github.io/S2R-HDR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory<br>
‚Ä¢ Dataset: S2R-HDR, Samples: 1000, Modality: HDR image sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.07375"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/IRMVLab/MMTwin"><img src="https://img.shields.io/github/stars/IRMVLab/MMTwin.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IRMV Lab, the Department of Automation, Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: Self-recorded HTP benchmark (unnamed in paper), Samples: 1200, Modality: egocentric RGB videos + point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.07083"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://kszpxxzmc.github.io/GenDoP/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: DataDoP, Samples: 29000, Modality: camera trajectories, RGBD images, textual captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Uncertainty-Aware Hybrid Machine Learning in Virtual Sensors for Vehicle Sideslip Angle Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.06105"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MB-Team-THI/UAHL-RevStED"><img src="https://img.shields.io/github/stars/MB-Team-THI/UAHL-RevStED.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CARISSMA Institute of Automated Driving, Technische Hochschule Ingolstadt, Germany<br>
‚Ä¢ Dataset: ReV-StED (Real-world Vehicle State Estimation Dataset), Samples: 900000, Modality: Vehicle dynamics sensor data from onboard sensors (CAN), GNSS/Inertial System, and optical speed sensor.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>UVG-VPC: Voxelized Point Cloud Dataset for Visual Volumetric Video-based Coding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.05888"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ultravideo.fi/UVG-VPC/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ultra Video Group, Tampere University, Tampere, Finland<br>
‚Ä¢ Dataset: UVG-VPC, Samples: 12, Modality: Voxelized point cloud sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Event-based Civil Infrastructure Visual Defect Detection: ev-CIVIL Dataset and Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.05679"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gwgknudayanga/evCIVIL"><img src="https://img.shields.io/github/stars/gwgknudayanga/evCIVIL.svg?style=social&label=Star"></a><br><a href="https://figshare.com/s/825aec2714266fa40d29"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Photonics Engineering, Technical University of Denmark, Denmark<br>
‚Ä¢ Dataset: ev-CIVIL, Samples: 680, Modality: DVS event streams + Grayscale frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>From Sparse Signal to Smooth Motion: Real-Time Motion Generation with Rolling Prediction Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.05265"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/barquerogerman/RPM"><img src="https://img.shields.io/github/stars/barquerogerman/RPM.svg?style=social&label=Star"></a><br><a href="https://barquerogerman.github.io/RPM/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta Reality Labs, Universitat de Barcelona, Computer Vision Center<br>
‚Ä¢ Dataset: GORP, Samples: >14 hours from 28 participants, Modality: VR tracking signals (head, hands/controllers) + MoCap ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.05046"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nju-cite-mocaphumanoid.github.io/MotionPRO/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Science and Engineering, Nanjing University, Nanjing, China<br>
‚Ä¢ Dataset: MotionPRO, Samples: 729, Modality: Pressure data, RGB videos, Optical MoCap data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Inter-event Interval Microscopy for Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.04924"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory for Multimedia Information Processing, Peking University, Beijing 100871, China<br>
‚Ä¢ Dataset: IEIMat, Samples: None, Modality: Event streams and fluorescence microscopy images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>OmniCam: Unified Multimodal Video Generation via Camera Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.02312"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: OmniTr, Samples: 1000, Modality: RGB videos, camera trajectories, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Beyond Static Scenes: Camera-controllable Background Generation for Human Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.02004"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yaomingshuai.github.io/Beyond-Static-Scenes.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Harbin Institute of Technology, Harbin, China<br>
‚Ä¢ Dataset: DynaScene, Samples: 200000, Modality: RGB videos + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Dynamic Initialization for LiDAR-inertial SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.01451"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lian-yue0515/D-LI-Init"><img src="https://img.shields.io/github/stars/lian-yue0515/D-LI-Init.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: school of Mechanical Engineering, Shandong University, Jinan 250061, China and Key Laboratory of High Efficiency and Clean Mechanical Manufacture, Ministry of Education, Jinan 250061, China<br>
‚Ä¢ Dataset: D-LI-Init test datasets, Samples: 49, Modality: LiDAR, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2025</td>
  <td style="width:70%;"><strong>Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2504.00438"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LannnSun/a-real-life-flexiwear-bodynet-dataset"><img src="https://img.shields.io/github/stars/LannnSun/a-real-life-flexiwear-bodynet-dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Key Laboratory of Navigation and Location-based Services, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, 200240<br>
‚Ä¢ Dataset: a real-life flexiwear-bodynet-dataset, Samples: 429, Modality: IMU data from smartphone, smartwatch, and headphones; Ground truth trajectories from VIO<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.24379"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sqwu.top/Any2Cap/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kuaishou Technology, National University of Singapore<br>
‚Ä¢ Dataset: Any2CapIns, Samples: 44644, Modality: RGB videos + human pose sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR) Challenge</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.24306"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/athaddius/STIRMetrics"><img src="https://img.shields.io/github/stars/athaddius/STIRMetrics.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/14803158"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Intuitive Surgical Inc.<br>
‚Ä¢ Dataset: STIR Challenge 2024 dataset (STIRC2024), Samples: 60, Modality: Stereo RGB videos + point trajectories from IR tattoos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.24026"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://humandreamer.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: GigaAI<br>
‚Ä¢ Dataset: MotionVid, Samples: 1270321, Modality: text + 2D pose sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Can Visuo-motor Policies Benefit from Random Exploration Data? A Case Study on Stacking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.23571"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cloudgripper.org"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: CloudGripper-Stack-750, Samples: 12400, Modality: RGB videos + proprioception states + actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>FRAME: Floor-aligned Representation for Avatar Motion from Egocentric Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.23094"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="vcai.mpi-inf.mpg.de/projects/FRAME"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: FRAME, Samples: 1600000, Modality: Stereo fisheye video, 6D head pose, Ground truth 3D joint positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>SocialGen: Modeling Multi-Human Social Interaction with Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.22906"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://socialgenx.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: SocialX, Samples: >40K, Modality: XH3D (SMPL-compatible pose data) + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.22397"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vadeli.github.io/GAITGen/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto, Computer Science Department, Vector Institute, KITE Research Institute, UHN<br>
‚Ä¢ Dataset: PD-GaM, Samples: 1701, Modality: 3D mesh dataset (SMPL parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.22394"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/Endo-TTAP-36E5"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Mechatronics and Engineering, Shenzhen University, Shenzhen, China<br>
‚Ä¢ Dataset: Endo-TAPC5, Samples: 40, Modality: RGB videos + point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.21860"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://maniptrans.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of General Artificial Intelligence, BIGAI<br>
‚Ä¢ Dataset: DEXMANIP NET, Samples: 3.3K episodes, Modality: simulated robotic manipulation trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.21268"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.lidarhumanmotion.net/climbingcap/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University; National Institute for Data Science in Health and Medicine, Xiamen University; Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University<br>
‚Ä¢ Dataset: AscendMotion, Samples: 412000, Modality: RGB, LiDAR, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>SpikeDerain: Unveiling Clear Videos from Rainy Sequences Using Color Spike Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.20315"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Wuhan University of Technology<br>
‚Ä¢ Dataset: RAIN100C, Samples: 100, Modality: RGB videos + color spike streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EGVD: Event-Guided Video Diffusion Model for Physically Realistic Large-Motion Frame Interpolation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.20268"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OpenImagingLab/EGVD"><img src="https://img.shields.io/github/stars/OpenImagingLab/EGVD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University, Shanghai AI Laboratory<br>
‚Ä¢ Dataset: DJI 30fps, Samples: 191 sequences, Modality: RGB videos + simulated event data<br>
‚Ä¢ Dataset: Comprehensive Training Dataset (Prophesee, BS-ERGB, DJI 30fps, GOPRO 240fps), Samples: 400 sequences, Modality: RGB videos + real/simulated event data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EBS-EKF: Accurate and High Frequency Event-based Star Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.20101"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.kitware.com/nest-public/kwebsstartracking"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kitware<br>
‚Ä¢ Dataset: EBS-EKF Star Tracking Dataset, Samples: 14, Modality: Event streams + 3D rotational trajectories (quaternions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.19913"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/c7w/PartRM"><img src="https://img.shields.io/github/stars/c7w/PartRM.svg?style=social&label=Star"></a><br><a href="https://PartRM.c7w.tech/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: PartDrag-4D, Samples: 20548, Modality: Multi-view images, animated meshes, point clouds, and 2D drag vectors for articulated objects<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.19355"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ikodoh.github.io/ST-VLM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea University<br>
‚Ä¢ Dataset: STKit, Samples: 116000, Modality: RGB videos with kinematic instruction tuning data (QA pairs)<br>
‚Ä¢ Dataset: STKit-Bench, Samples: 1400, Modality: RGB videos with kinematic instruction tuning data (QA pairs)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>TrackRAD2025 challenge dataset: Real-time tumor tracking for MRI-guided radiotherapy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.19119"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LMUK-RADONC-PHYS-RES/trackrad2025/"><img src="https://img.shields.io/github/stars/trackrad2025/.svg?style=social&label=Star"></a><br><a href="https://trackrad2025.grand-challenge.org/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Radiation Oncology, Radiation Oncology Key Laboratory of Sichuan Province, Sichuan Clinical Research Center for Cancer, Sichuan Cancer Hospital & Institute, Sichuan Cancer Center, University of Electronic Science and Technology of China, Chengdu, China<br>
‚Ä¢ Dataset: TrackRAD2025, Samples: 585, Modality: 2D cine MRIs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Target-Aware Video Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18950"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://taeksuu.github.io/tavid/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Seoul National University<br>
‚Ä¢ Dataset: unnamed, Samples: 1290, Modality: RGB videos + segmentation masks + text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Predicting the Road Ahead: A Knowledge Graph based Foundation Model for Scene Understanding in Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18730"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/boschresearch/fm4su"><img src="https://img.shields.io/github/stars/boschresearch/fm4su.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Bosch Corporate Research, Renningen, Germany, University of Stuttgart, Stuttgart, Germany<br>
‚Ä¢ Dataset: BEV symbolic scene representation dataset from nuScenesKG, Samples: 30000, Modality: Symbolic Bird's-Eye-View (BEV) representation from Knowledge Graph<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18552"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://potentialming.github.io/projects/EvAnimate"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Sydney<br>
‚Ä¢ Dataset: EvTikTok, Samples: 350, Modality: simulated event streams<br>
‚Ä¢ Dataset: EvHumanMotion, Samples: 113, Modality: RGB videos + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Human-Object Interaction via Automatically Designed VLM-Guided Motion Policy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18349"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vlm-rmd.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Interplay, Samples: 1210, Modality: Interaction plans, 3D scene layouts, text instructions, and top-view images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>AGIR: Assessing 3D Gait Impairment with Reasoning based on LLMs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.18141"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/w/AGIR-7BF7/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ICube laboratory, University of Strasbourg, CNRS, France<br>
‚Ä¢ Dataset: Enhanced Parkinson's Disease (PD) gait dataset, Samples: 883, Modality: 3D joint data + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>TransAnimate: Taming Layer Diffusion to Generate RGBA Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17934"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Clemson University<br>
‚Ä¢ Dataset: Animate Dataset, Samples: 3000, Modality: RGBA videos<br>
‚Ä¢ Dataset: Foreground Object Videos Dataset, Samples: 7000, Modality: RGBA videos<br>
‚Ä¢ Dataset: Synthesized Transparent Motion Videos, Samples: 20000, Modality: RGBA videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Event-Based Crossing Dataset (EBCD)</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17499"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/joeduman/Thresholded event-based-crossing-dataset"><img src="https://img.shields.io/github/stars/joeduman/Thresholded event-based-crossing-dataset.svg?style=social&label=Star"></a><br><a href="https://ieee-dataport.org/documents/event-based-crossing-dataset-ebcd"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UMBC<br>
‚Ä¢ Dataset: Event-Based Crossing Dataset (EBCD), Samples: 33, Modality: Event-based images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17358"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Oxford, Department of Computer Science<br>
‚Ä¢ Dataset: Synthetic Motion-Blurred Image Dataset (from ScanNet++v2), Samples: 121200, Modality: Synthesized motion-blurred RGB images, depth maps, optical flow fields, camera poses<br>
‚Ä¢ Dataset: Real-world Motion-Blurred Image Dataset, Samples: 10000, Modality: Real-world motion-blurred RGB images, ARKit poses, IMU measurements<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17132"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: FallingDetection-CeleX, Samples: 875, Modality: Event-camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>ColabSfM: Collaborative Structure-from-Motion by Point Cloud Registration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.17093"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/EricssonResearch/ColabSfM"><img src="https://img.shields.io/github/stars/EricssonResearch/ColabSfM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Link√∂ping University<br>
‚Ä¢ Dataset: ColabSfM SfM Registration Dataset, Samples: 22000, Modality: Pairs of 3D SfM point clouds with normals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Pedestrians and Robots: A Novel Dataset for Learning Distinct Social Navigation Forces</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.16481"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Humanoid Robots Lab, University of Bonn, Germany<br>
‚Ä¢ Dataset: robot-pedestrian influence (RPI) dataset, Samples: 18669, Modality: 2D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.16421"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://quanhaol.github.io/magicmotion-site/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: MagicData, Samples: 51000, Modality: RGB videos with text and trajectory (mask, bounding box) annotations<br>
‚Ä¢ Dataset: MagicBench, Samples: 600, Modality: RGB videos with trajectory (mask, bounding box) annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>PoseTraj: Pose-Aware Trajectory Control in Video Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.16068"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://robingg1.github.io/Pose-Traj/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Edinburgh<br>
‚Ä¢ Dataset: PoseTraj-10K, Samples: 10000, Modality: RGB videos + trajectories + 3D bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.15835"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vulab-ai.github.io/BARD-GS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Case Western Reserve University<br>
‚Ä¢ Dataset: Real-world Captured Blurry Dataset, Samples: 12, Modality: Paired blurry (24 FPS) and sharp (240 FPS) RGB videos of dynamic scenes from synchronized cameras<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Controlling Avatar Diffusion with Learnable Gaussian Embedding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.15809"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ustc3dv.github.io/Learn2Control/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: Synthetic Head Dataset, Samples: 10000, Modality: Synthetic images + FLAME parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14935"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://favor-bench.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: FAVOR-Bench, Samples: 1776, Modality: RGB videos<br>
‚Ä¢ Dataset: FAVOR-Train, Samples: 17152, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14919"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: Unified and Extended Motion Dataset, Samples: 48251, Modality: MoCap joints + Text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Matching Skeleton-based Activity Representations with Heterogeneous Signals for HAR</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14547"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Shuheng-Li/SKELAR"><img src="https://img.shields.io/github/stars/Shuheng-Li/SKELAR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, San Diego<br>
‚Ä¢ Dataset: MASD (Multimodal Activity Sensing Dataset), Samples: 540, Modality: IMU, WiFi, skeleton<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14247"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/NSN-Hello/GeoFlow-SLAM"><img src="https://img.shields.io/github/stars/NSN-Hello/GeoFlow-SLAM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Horizon Robotics<br>
‚Ä¢ Dataset: Go2 D435i dataset, Samples: 4, Modality: RGB-D, IMU, LiDAR, Legged Odometry<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.14229"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ha-vln-project.vercel.app/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Washington<br>
‚Ä¢ Dataset: HAPS 2.0, Samples: 486, Modality: SMPL mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>8-Calves Image dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.13777"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/tonyFang04/8-calves"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Bristol<br>
‚Ä¢ Dataset: 8-Calves dataset, Samples: 1, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Multi-Platform Teach-and-Repeat Navigation by Visual Place Recognition Based on Deep-Learned Local Features</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.13090"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://imr.ciirc.cvut.cz/Datasets/TaR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Czech Institute of Informatics, Robotics and Cybernetics, Czech technical University in Prague; Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague<br>
‚Ä¢ Dataset: TaR, Samples: 3, Modality: RGB images + pose transformations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Beyond Role-Based Surgical Domain Modeling: Generalizable Re-Identification in the Operating Room</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.13028"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wngTn/orreid"><img src="https://img.shields.io/github/stars/wngTn/orreid.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Chair for Computer Aided Medical Procedures, Technical University of Munich, Boltzmannstra√üe 3, 85748, Garching, Germany<br>
‚Ä¢ Dataset: ORReID13, Samples: 6358, Modality: 3D point clouds + multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>GIFT: Generated Indoor video frames for Texture-less point tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12944"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/GIFT-6D02/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southern University of Science and Technology<br>
‚Ä¢ Dataset: GIFT, Samples: 1800, Modality: RGB videos + point trajectories + optical flow + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>AUTV: Creating Underwater Video Datasets with Pixel-wise Annotations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12828"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: UTV, Samples: 2000, Modality: RGB videos + fine-grained text annotations (including motion/behavior)<br>
‚Ä¢ Dataset: SUTV, Samples: 10000, Modality: RGB videos + pixel-wise segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Stereo Event-based, 6-DOF Pose Tracking for Uncooperative Spacecraft</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12732"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Zibin6/SE6PT"><img src="https://img.shields.io/github/stars/Zibin6/SE6PT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Aerospace Science and Engineering, National University of Defense Technology<br>
‚Ä¢ Dataset: Stereo Event-based Uncooperative Spacecraft Motion Dataset, Samples: 17, Modality: Stereo event streams + 6-DOF pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12527"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Intelligence Science and Technology, National University of Defense Technology, China.<br>
‚Ä¢ Dataset: In-house dataset, Samples: 17, Modality: RGB-D videos + IMU data + pose ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12419"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/3190105222/EgoEv_Gesture"><img src="https://img.shields.io/github/stars/3190105222/EgoEv_Gesture.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang University, China<br>
‚Ä¢ Dataset: EgoEvGesture, Samples: 5419, Modality: event camera streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>M2UD: A Multi-model, Multi-scenario, Uneven-terrain Dataset for Ground Robot with Localization and Mapping Evaluation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.12387"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yaepiii.github.io/M2UD/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Robotics at Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China<br>
‚Ä¢ Dataset: M2UD, Samples: 58, Modality: LiDAR, RGB-D Camera, IMU, GNSS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Bring Your Rear Cameras for Egocentric 3D Human Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.11652"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/EgoRear/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, SIC<br>
‚Ä¢ Dataset: Ego4View-Syn, Samples: 8372, Modality: synthetic RGB videos (fisheye) + SMPL parameters<br>
‚Ä¢ Dataset: Ego4View-RW, Samples: 478, Modality: real-world RGB videos (fisheye) + MoCap joints + SMPL parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>EMoTive: Event-guided Trajectory Modeling for 3D Motion Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.11371"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: USTC<br>
‚Ä¢ Dataset: CarlaEvent3D, Samples: 22125, Modality: Events + Images + 3D Motion Labels (Optical Flow, Motion in Depth, Scene Flow)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Enhancing Hand Palm Motion Gesture Recognition by Eliminating Reference Frame Bias via Frame-Invariant Similarity Measures</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.11352"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.15020057"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering and Flanders Make at KU Leuven<br>
‚Ä¢ Dataset: Hand Palm Motion (HPM) dataset, Samples: 420, Modality: Motion capture trajectories (3D position + quaternion)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>6D Object Pose Tracking in Internet Videos for Robotic Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.10307"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague<br>
‚Ä¢ Dataset: New dataset of instructional videos, Samples: 32, Modality: RGB videos + 6D object pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>RMG: Real-Time Expressive Motion Generation with Self-collision Avoidance for 6-DOF Companion Robotic Arms</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.09959"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: robotic arm expressive motion dataset, Samples: over 10,000, Modality: Robotic arm joint trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Unified Dense Prediction of Video Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.09344"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Virginia<br>
‚Ä¢ Dataset: Panda-Dense, Samples: 300000, Modality: RGB videos + entity segmentation + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>SwapAnyone: Consistent and Realistic Video Synthesis for Swapping Any Person into Any Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.09154"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/PKU-YuanGroup/SwapAnyone"><img src="https://img.shields.io/github/stars/PKU-YuanGroup/SwapAnyone.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: HumanAction-32K, Samples: 32000, Modality: RGB videos + pose keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Ev-Layout: A Large-scale Event-based Multi-modal Dataset for Indoor Layout Estimation and Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.08370"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software, Shandong University, China<br>
‚Ä¢ Dataset: Ev-Layout, Samples: 2500, Modality: RGB images, event streams, IMU data, illuminance sensor data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HERO: Human Reaction Generation from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.08270"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JackYu6/HERO"><img src="https://img.shields.io/github/stars/JackYu6/HERO.svg?style=social&label=Star"></a><br><a href="https://jackyu6.github.io/HERO"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: ViMo, Samples: 3500, Modality: RGB videos + 3D human motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Depth-Assisted Network for Indiscernible Marine Object Counting with Adaptive Motion-Differentiated Feature Encoding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.08152"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OUCVisionGroup/VIMOC-Net"><img src="https://img.shields.io/github/stars/OUCVisionGroup/VIMOC-Net.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Engineering, Ocean University of China<br>
‚Ä¢ Dataset: VIMOC Dataset, Samples: 50, Modality: RGB videos + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>AG-VPReID: A Challenging Large-Scale Benchmark for Aerial-Ground Video-based Person Re-Identification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.08121"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Vi-Sion/AG-VPReID-Net"><img src="https://img.shields.io/github/stars/Vi-Sion/AG-VPReID-Net.svg?style=social&label=Star"></a><br><a href="https://github.com/Vi-Sion/AG-VPReID-Net"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical Engineering and Robotics, Queensland University of Technology<br>
‚Ä¢ Dataset: AG-VPReID, Samples: 32321, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HumanMM: Global Human Motion Recovery from Multi-shot Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07597"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zhangyuhong01.github.io/HumanMM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University, IDEA Research<br>
‚Ä¢ Dataset: ms-Motion, Samples: 600, Modality: RGB videos + 3D human motion (SMPL) + camera parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>AthletePose3D: A Benchmark Dataset for 3D Human Pose Estimation and Kinematic Validation in Athletic Movements</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07499"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/calvinyeungck/AthletePose3D"><img src="https://img.shields.io/github/stars/calvinyeungck/AthletePose3D.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Graduate School of Informatics, Nagoya University, Nagoya, Japan<br>
‚Ä¢ Dataset: AthletePose3D, Samples: 165000, Modality: RGB videos + 3D poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>PersonaBooth: Personalized Text-to-Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07390"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://boeun-kim.github.io/page-PersonaBooth"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Birmingham, Korea Electronics Technology Institute, Dankook University<br>
‚Ä¢ Dataset: PerMo, Samples: 6,610 clips, Modality: MoCap (optical markers, skeleton, SMPL-H mesh), text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs and Chain-of-Thought Prompting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07234"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau<br>
‚Ä¢ Dataset: Highway-Text, Samples: 6606, Modality: Text descriptions of traffic scenarios<br>
‚Ä¢ Dataset: Urban-Text, Samples: 5431, Modality: Text descriptions of traffic scenarios<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>YOLOMG: Vision-based Drone-to-Drone Detection with Appearance and Pixel-Level Motion Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07115"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Irisky123/YOLOMG"><img src="https://img.shields.io/github/stars/Irisky123/YOLOMG.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Artificial Intelligence, Westlake University, Hangzhou, China<br>
‚Ä¢ Dataset: ARD100, Samples: 100, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07020"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University at Buffalo, SUNY<br>
‚Ä¢ Dataset: DriveLM-Deficit, Samples: 53895, Modality: RGB video clips<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HybridReg: Robust 3D Point Cloud Registration with Hybrid Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.07019"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hxwork/HybridReg"><img src="https://img.shields.io/github/stars/hxwork/HybridReg.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: HybridMatch, Samples: 50600, Modality: Point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Motion Anything: Any to Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.06955"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://steve-zeyu-zhang.github.io/MotionAnything"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ANU<br>
‚Ä¢ Dataset: Text-Music-Dance (TMD), Samples: 2153, Modality: text, music, and dance motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Sign Language Translation using Frame and Event Stream: Benchmark Dataset and Algorithms</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.06484"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/OpenESL"><img src="https://img.shields.io/github/stars/Event-AHU/OpenESL.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: VECSL, Samples: 15676, Modality: RGB frames + Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.06053"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dropletx.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IEIT System Co., Ltd.<br>
‚Ä¢ Dataset: DropletVideo-10M, Samples: 10000000, Modality: RGB videos + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>A Helping (Human) Hand in Kinematic Structure Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.05301"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics and Biology Laboratory, Technische Universit√§t Berlin; Science of Intelligence, Research Cluster of Excellence, Berlin<br>
‚Ä¢ Dataset: unnamed, Samples: 30, Modality: RGB-D videos + MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.05231"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="Search for 'Kaiwu' on ScienceDB"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the text<br>
‚Ä¢ Dataset: Kaiwu, Samples: 11664, Modality: Motion Capture (3D skeleton ground truth), Multi-view RGB-D videos, Audio, IMU, EMG, Eye Gaze (first-person video), Hand Pose (data glove), Tactile/Force (data glove)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Combined Physics and Event Camera Simulator for Slip Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tub-rip/event-slip"><img src="https://img.shields.io/github/stars/tub-rip/event-slip.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit ¬®at Berlin, and Robotics Institute Germany<br>
‚Ä¢ Dataset: Simple Set, Samples: 192, Modality: Event camera data, RGB frames, Object/Gripper orientations (quaternions)<br>
‚Ä¢ Dataset: Complex Set, Samples: 1200, Modality: Event camera data, RGB frames, Object/Gripper orientations (quaternions)<br>
‚Ä¢ Dataset: Real Set, Samples: 5, Modality: Event camera data, RGB frames, Robot kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>What Are You Doing? A Closer Look at Controllable Human Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04666"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/google-deepmind/wyd-benchmark"><img src="https://img.shields.io/github/stars/google-deepmind/wyd-benchmark.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google DeepMind<br>
‚Ä¢ Dataset: What Are You Doing? (WYD), Samples: 1544, Modality: RGB videos + captions + video segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>3HANDS Dataset: Learning from Humans for Generating Naturalistic Handovers with Supernumerary Robotic Limbs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04635"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hci.cs.uni-saarland.de/projects/3hands/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Saarland University<br>
‚Ä¢ Dataset: 3HANDS, Samples: 946, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Omnidirectional Multi-Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04565"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xifen523/OmniTrack"><img src="https://img.shields.io/github/stars/xifen523/OmniTrack.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hunan University<br>
‚Ä¢ Dataset: QuadTrack, Samples: 32, Modality: panoramic image sequences + 2D bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.04257"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Seoul National University<br>
‚Ä¢ Dataset: Truebones Zoo dataset (annotated), Samples: 1000+, Modality: skeletal motion + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Supervised Visual Docking Network for Unmanned Surface Vehicles Using Auto-labeling in Real-world Water Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.03282"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/usv-docking/home"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information Science and Engineering, Hebei University of Science and Technology, 26 Yuxiang Street, Yuhua District, Shijiazhuang, Heibei Province, 050018, P.R. China<br>
‚Ä¢ Dataset: USV Visual Docking Dataset, Samples: 2000, Modality: Fisheye images + relative pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Monocular Person Localization under Camera Ego-motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.02916"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MEDLAR-T-Rex/rpf_quadruped"><img src="https://img.shields.io/github/stars/MEDLAR-T-Rex/rpf_quadruped.svg?style=social&label=Star"></a><br><a href="https://medlartea.github.io/rpf-quadruped/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Key Laboratory of Robotics and Computer Vision, Southern University of Science and Technology (SUSTech), and the Department of Electronic and Electrical Engineering, SUSTech.<br>
‚Ä¢ Dataset: Our Dataset, Samples: None, Modality: RGB videos (pin-hole, fisheye, equirectangular), UWB distance measurements, motion capture poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.02572"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://racevla.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Intelligent Space Robotics Laboratory, Skolkovo Institute of Science and Technology<br>
‚Ä¢ Dataset: RaceVLA dataset, Samples: 200, Modality: Vicon MoCap (position, velocity, yaw), RGB images, natural language instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>BdSLW401: Transformer-Based Word-Level Bangla Sign Language Recognition Using Relative Quantization Encoding (RQE)</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.02360"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the paper<br>
‚Ä¢ Dataset: BdSLW401, Samples: 102176, Modality: Pose landmarks (MediaPipe)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>One-Step Event-Driven High-Speed Autofocus</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.01214"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang University, China<br>
‚Ä¢ Dataset: PSF-based Focus Event Synthetic Dataset, Samples: 84, Modality: Synthetic focus event stacks with simulated motion<br>
‚Ä¢ Dataset: DAVIS346 Autofocus Dataset, Samples: 28, Modality: Time-synchronized event streams and grayscale images<br>
‚Ä¢ Dataset: Prophesee EVK4 Autofocus Dataset, Samples: 28, Modality: Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>HiMo: High-Speed Objects Motion Compensation in Point Clouds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.00803"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/KTH-RPL/HiMo"><img src="https://img.shields.io/github/stars/KTH-RPL/HiMo.svg?style=social&label=Star"></a><br><a href="https://kin-zhang.github.io/HiMo"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Division of Robotics, Perception, and Learning (RPL), KTH Royal Institute of Technology; Autonomous Transport Solutions Lab, Scania Group<br>
‚Ä¢ Dataset: Scania, Samples: 500 sequences, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.00495"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://xuanchenli.github.io/TexTalk/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: TexTalk4D, Samples: 100, Modality: audio-synced 3D mesh sequences with 8K dynamic textures<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>High Dynamic Range Video Compression: A Large-Scale Benchmark Dataset and A Learned Bit-depth Scalable Compression Algorithm</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.00410"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sdkinda/HDR-Learned-Video-Coding"><img src="https://img.shields.io/github/stars/sdkinda/HDR-Learned-Video-Coding.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shangha University<br>
‚Ä¢ Dataset: HDRVD2K, Samples: 2200, Modality: HDR videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2025</td>
  <td style="width:70%;"><strong>BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2503.00389"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Keio University<br>
‚Ä¢ Dataset: AMPL (Acoustic Music-based PoseLearning dataset), Samples: None, Modality: Mocap joints + Acoustic signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Towards long-term player tracking with graph hierarchies and domain-specific features</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.21242"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mkoshkina/sports-SUSHI"><img src="https://img.shields.io/github/stars/mkoshkina/sports-SUSHI.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: York University<br>
‚Ä¢ Dataset: Hockey Tracking Dataset, Samples: 20, Modality: RGB videos with MOT annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>egoPPG: Heart Rate Estimation from Eye-Tracking Cameras in Egocentric Systems to Benefit Downstream Vision Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.20879"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, ETH Zurich<br>
‚Ä¢ Dataset: egoPPG-DB, Samples: 150, Modality: Eye-tracking videos + POV RGB videos + IMU + PPG + ECG<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>EyEar: Learning Audio Synchronized Human Gaze Trajectory Based on Physics-Informed Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.20858"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XiaochuanLiu-ruc/EyEar"><img src="https://img.shields.io/github/stars/XiaochuanLiu-ruc/EyEar.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Gaoling School of Artifcial Intelligence, Renmin University of China, Beijing, China<br>
‚Ä¢ Dataset: EyEar-20k, Samples: 20000, Modality: Gaze trajectories + images + synchronized audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.20824"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Texas at Austin<br>
‚Ä¢ Dataset: Handheld Burst Motion Dataset, Samples: 102, Modality: Homography matrices from multi-frame captures<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>FuseGrasp: Radar-Camera Fusion for Robotic Grasping of Transparent Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.20037"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Information Engineering, The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: self-built RGB-D-Radar transparent object dataset, Samples: 600, Modality: RGB images + depth images + mmWave radar images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>C-Drag: Chain-of-Thought Driven Motion Controller for Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.19868"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WesLee88524/C-Drag-Official-Repo"><img src="https://img.shields.io/github/stars/WesLee88524/C-Drag-Official-Repo.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northwestern Polytechnical University<br>
‚Ä¢ Dataset: VOI, Samples: 72, Modality: RGB videos with bounding box and trajectory annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>TransVDM: Motion-Constrained Video Diffusion Model for Transparent Video Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.19454"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Alibaba Group<br>
‚Ä¢ Dataset: None, Samples: 10000, Modality: RGB-alpha video clips + bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.18373"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://siplab.org/projects/EgoSim"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich<br>
‚Ä¢ Dataset: MultiEgoView, Samples: 119.4 hours (synthetic) + 5 hours (real), Modality: RGB videos (from 6 body-worn cameras), ground-truth 3D body poses, activity annotations, simulated IMU data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Impact of Object Weight in Handovers: Inspiring Robotic Grip Release and Motion from Human Handovers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.17834"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/paragkhanna1/RPL Khanna Human Handover Datasets"><img src="https://img.shields.io/github/stars/paragkhanna1/RPL Khanna Human Handover Datasets.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Division of Robotics, Perception and Learning (RPL), EECS, KTH Royal Institute of Technology, Sweden<br>
‚Ä¢ Dataset: Handovers@RPL-2.0, Samples: 3235, Modality: MoCap joints, Forces<br>
‚Ä¢ Dataset: YCB-Handovers, Samples: 2771, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>V-HOP: Visuo-Haptic 6D Object Pose Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.17434"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lhy.xyz/projects/v-hop/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: V-HOP Multi-embodied Dataset, Samples: 1550000, Modality: RGB-D videos, robot kinematics (joint positions), tactile sensor data, 6D object pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>A dataset of high-resolution plantar pressures for gait analysis across varying footwear and walking speeds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.17244"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UNB-StepUP/StepUP-P150"><img src="https://img.shields.io/github/stars/UNB-StepUP/StepUP-P150.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.20383/103.01285"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of New Brunswick, Institute of Biomedical Engineering<br>
‚Ä¢ Dataset: UNB StepUP-P150, Samples: over 200,000 footsteps, Modality: Plantar pressure<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Human2Robot: Learning Robot Actions from Paired Human-Robot Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.16587"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: H&R, Samples: 2600, Modality: paired human hand and robot arm RGB videos, robot arm kinematics (position, rotation, joint velocity), human hand kinematics (position, rotation, key points)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>DeProPose: Deficiency-Proof 3D Human Pose Estimation via Adaptive Multi-View Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.16419"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WUJINHUAN/DeProPose"><img src="https://img.shields.io/github/stars/WUJINHUAN/DeProPose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence, Xidian University, China<br>
‚Ä¢ Dataset: Deficiency-Aware 3D Pose Estimation (DA-3DPE) dataset, Samples: 575689, Modality: multi-view RGB videos + 3D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.14917"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not specified in text<br>
‚Ä¢ Dataset: VQA driving instruction dataset, Samples: None, Modality: multi-view scene videos, BEV map images, QA annotations, vehicle motion trajectories, and control signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.14795"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Milab, Westlake University<br>
‚Ä¢ Dataset: Humanoid-VLA motion-language interleaved dataset, Samples: 929000 clips, Modality: motion sequences + text annotations<br>
‚Ä¢ Dataset: Humanoid-S, Samples: 4646 video clips, Modality: human pose + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Exploiting Deblurring Networks for Radiance Fields</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.14454"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KT<br>
‚Ä¢ Dataset: BlurRF-Synth, Samples: 150, Modality: Synthesized RGB images (motion & defocus blurred)<br>
‚Ä¢ Dataset: BlurRF-Real, Samples: 5, Modality: Real-world RGB images (motion blurred)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.14004"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen University<br>
‚Ä¢ Dataset: Inter3D, Samples: 4, Modality: RGB images + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.13859"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: MSVCOD, Samples: 162, Modality: RGB videos with pixel-level annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Event-Based Video Frame Interpolation With Cross-Modal Asymmetric Bidirectional Motion Fields</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.13716"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/intelpro/CBMNet"><img src="https://img.shields.io/github/stars/intelpro/CBMNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea Advanced Institute of Science and Technology<br>
‚Ä¢ Dataset: ERF-X170FPS, Samples: 140, Modality: RGB videos + Event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>MoVer: Motion Verification for Motion Graphics Animations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.13372"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mover-dsl.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University, USA<br>
‚Ä¢ Dataset: MoVer Test Dataset, Samples: 5600, Modality: Text prompts paired with ground truth MoVer verification programs for 2D SVG motion graphics animations.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>iMOVE: Instance-Motion-Aware Video Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.11594"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kuaishou Technology, Zhejiang University<br>
‚Ä¢ Dataset: iMOVE-IT, Samples: 114000, Modality: RGB videos with bounding box motion trajectories and dynamic captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>AdaManip: Adaptive Articulated Object Manipulation Environments and Policy Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.11124"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://adamanip.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center on Frontiers of Computing Studies, School of Computer Science, Peking University<br>
‚Ä¢ Dataset: AdaManip Adaptive Demonstrations, Samples: 5540 simulated sequences, Modality: Robot end-effector pose trajectories, 3D point clouds, robot proprioception states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>E-3DGS: Event-Based Novel View Rendering of Large-Scale Scenes Using 3D Gaussian Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.10827"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/E3DGS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Saarland University, MPI for Informatics, SIC<br>
‚Ä¢ Dataset: E-3DGS-Real, Samples: None, Modality: color event stream, RGB images, camera poses<br>
‚Ä¢ Dataset: E-3DGS-Synthetic, Samples: 3, Modality: simulated color event stream, rendered RGB frames, camera poses<br>
‚Ä¢ Dataset: E-3DGS-Synthetic-Hard, Samples: 3, Modality: simulated color event stream, rendered RGB frames, noisy camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.09533"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University of Science and Technology<br>
‚Ä¢ Dataset: TalkingFace-Wild, Samples: 31300, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>A Deep Inverse-Mapping Model for a Flapping Robotic Wing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.09378"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Hadar933/AdaptiveSpectrumLayer"><img src="https://img.shields.io/github/stars/Hadar933/AdaptiveSpectrumLayer.svg?style=social&label=Star"></a><br><a href="https://www.beatus-lab.org"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Engineering, The Institute of Life Sciences, Center for Bioengineering, The Hebrew University of Jerusalem<br>
‚Ä¢ Dataset: None, Samples: 153, Modality: 3D wing kinematics (Euler angles) from stereo high-speed cameras, synchronized with aerodynamic force sensor data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>EventSTR: A Benchmark Dataset and Baselines for Event Stream based Scene Text Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.09020"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/EventSTR"><img src="https://img.shields.io/github/stars/Event-AHU/EventSTR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University<br>
‚Ä¢ Dataset: EventSTR, Samples: 9928, Modality: Event stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Measuring Anxiety Levels with Head Motion Patterns in Severe Depression Population</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.08813"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Univ. Lille, CNRS, Centrale Lille, Institut Mines-T ¬¥el¬¥ecom, UMR 9189 CRIStAL, F-59000 Lille, France; Univ. Lille, Inserm, CHU Lille, U1172 - LilNCog - Lille Neuroscience & Cognition, F-59000 Lille, France<br>
‚Ä¢ Dataset: CALYPSO Depression Dataset, Samples: 32, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.08639"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cinemaster-dev.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dalian University of Technology<br>
‚Ä¢ Dataset: Unnamed 3D box and camera pose video dataset, Samples: 156000, Modality: RGB videos + 3D bounding boxes + 3D camera trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.07869"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://eventego3d.mpi-inf.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Computing and Artificial Intelligence, Max Planck Institute for Informatics, SIC, Saarbr√ºcken, Germany; Augmented Vision, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany.<br>
‚Ä¢ Dataset: EE3D-R, Samples: 12 sequences, ~4.64e5 poses, Modality: egocentric event streams, 3D body joints, allocentric RGB streams, SMPL body parameters<br>
‚Ä¢ Dataset: EE3D-W, Samples: 9 sequences, ~4.18e5 poses, Modality: egocentric event streams, 3D body joints, allocentric RGB streams, SMPL body parameters<br>
‚Ä¢ Dataset: EE3D-S, Samples: 946 motion sequences, ~6.34e6 3D human poses, Modality: synthetic egocentric event streams, 3D body joints, SMPL body models<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.07531"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: Camera Motion Control Dataset, Samples: 62000, Modality: RGB videos + camera trajectories<br>
‚Ä¢ Dataset: Object Motion Control Dataset, Samples: 60000, Modality: RGB videos + dense/sparse object trajectories + optical flow<br>
‚Ä¢ Dataset: VideoLightingDirection (VLD) Dataset, Samples: 57600, Modality: Synthetic RGB videos + camera trajectories + lighting direction<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>CT-UIO: Continuous-Time UWB-Inertial-Odometer Localization Using Non-Uniform B-spline with Fewer Anchors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.06287"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JasonSun623/CT-UIO"><img src="https://img.shields.io/github/stars/JasonSun623/CT-UIO.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Engineering Research Center of Robot Visual Perception and Control Technology, Hunan University, Changsha 410012, China<br>
‚Ä¢ Dataset: Corridor Dataset, Samples: 6, Modality: UWB, IMU, Odometer, LIDAR<br>
‚Ä¢ Dataset: Exhibition Hall Dataset, Samples: 6, Modality: UWB, IMU, Odometer, LIDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.05979"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology, China<br>
‚Ä¢ Dataset: Open-VFX, Samples: 675, Modality: RGB videos + text prompts + instance segmentation masks + start/end timestamps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.04847"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://agnjason.github.io/HumanDiT-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University, ByteDance<br>
‚Ä¢ Dataset: Unnamed HumanDiT dataset, Samples: 4500000, Modality: RGB videos + body pose sequences + background keypoint sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>High-Speed Dynamic 3D Imaging with Sensor Fusion Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.04630"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of North Carolina, Chapel Hill, USA<br>
‚Ä¢ Dataset: Synthetic High-Speed Dynamic Scenes, Samples: 3, Modality: RGB videos + Depth maps + Events<br>
‚Ä¢ Dataset: Real-World High-Speed Dynamic Scenes, Samples: 3, Modality: RGB videos + Depth maps + Events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>HD-EPIC: A Highly-Detailed Egocentric Video Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.04144"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://hd-epic.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Uni. of Bristol<br>
‚Ä¢ Dataset: HD-EPIC, Samples: 19900, Modality: Egocentric RGB videos, 6DoF camera trajectories, 3D digital twins, 3D object/hand masks, gaze, audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.03639"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://snap-research.github.io/PointVidGen/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Los Angeles<br>
‚Ä¢ Dataset: PointVid, Samples: 70000, Modality: RGB videos + 3D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Swarm Characteristic Classification using Robust Neural Networks with Optimized Controllable Inputs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.03619"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DWPeltier3/Swarm-NN-TSC"><img src="https://img.shields.io/github/stars/DWPeltier3/Swarm-NN-TSC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Naval Postgraduate School<br>
‚Ä¢ Dataset: Combined ND, Samples: 72000, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
‚Ä¢ Dataset: Combined DM, Samples: 24000, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
‚Ä¢ Dataset: Combined Noise, Samples: 244800, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
‚Ä¢ Dataset: Combined DM+, Samples: 200000, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
‚Ä¢ Dataset: Combined ND & DM, Samples: 240000, Modality: Simulated 2D swarm agent trajectories (positions and velocities)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>SKI Models: Skeleton Induced Vision-Language Embeddings for Understanding Activities of Daily Living</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.03459"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/thearkaprava/SKI-Models"><img src="https://img.shields.io/github/stars/thearkaprava/SKI-Models.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of North Carolina at Charlotte<br>
‚Ä¢ Dataset: NTU120 video-instruction pairs, Samples: 100K question-answer pairs, Modality: RGB videos + text instruction pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal Structures from Multi-view Joint Cloud</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.02936"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China<br>
‚Ä¢ Dataset: BUMocap-X, Samples: 1 sequence (120 seconds), Modality: MoCap joints from multi-view video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2025</td>
  <td style="width:70%;"><strong>Event-aided Semantic Scene Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2502.02334"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Pandapan01/EvSSC"><img src="https://img.shields.io/github/stars/Pandapan01/EvSSC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang University<br>
‚Ä¢ Dataset: DSEC-SSC, Samples: 12 sequences (3,488 frames), Modality: Event camera data + RGB images + LiDAR<br>
‚Ä¢ Dataset: SemanticKITTI-E, Samples: 4649 frames (3834 train, 815 val), Modality: RGB images + simulated event data + LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Every Image Listens, Every Image Dances: Music-Driven Image Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.18801"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stony Brook University<br>
‚Ä¢ Dataset: MuseDance, Samples: 2904, Modality: RGB videos + audio + text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>UDC-VIT: A Real-World Video Dataset for Under-Display Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.18545"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mcrl/UDC-VIT"><img src="https://img.shields.io/github/stars/mcrl/UDC-VIT.svg?style=social&label=Star"></a><br><a href="https://kyusuahn.github.io/UDC-VIT.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Data Science, Seoul National University, Seoul, Republic of Korea; Research Center, Samsung Display Co., Ltd., Yongin, Republic of Korea<br>
‚Ä¢ Dataset: UDC-VIT, Samples: 647, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via Multimodal Visual Feature Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.18124"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="remote-bmxs.netlify.app"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Academy for Engineering & Technology, Fudan University<br>
‚Ä¢ Dataset: NEPose, Samples: 50 videos, Modality: Binocular 4K endoscopic videos + pose trajectories from optical tracking system<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>VidSole: A Multimodal Dataset for Joint Kinetics Quantification and Disease Detection with Deep Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.17890"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: VidSole, Samples: 2632, Modality: instrumented insole forces and moments, 2-viewpoint RGB video, 3D motion capture, force plate data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Extending Information Bottleneck Attribution to Video Sequences</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.16889"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/anonrep/IBA-for-Video-Sequences"><img src="https://img.shields.io/github/stars/anonrep/IBA-for-Video-Sequences.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit ¬®at Berlin<br>
‚Ä¢ Dataset: deepfake detection dataset, Samples: 378, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.14319"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Xiaohao-Xu/SLAM-under-Perturbation"><img src="https://img.shields.io/github/stars/Xiaohao-Xu/SLAM-under-Perturbation.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Michigan, Ann Arbor<br>
‚Ä¢ Dataset: Robust-Ego3D, Samples: 1000, Modality: RGB-D videos + pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Improving Video Generation with Human Feedback</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.13918"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gongyeliu.github.io/videoalign"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: Human-labeled video generation preference dataset, Samples: 108000, Modality: RGB videos + human preference annotations<br>
‚Ä¢ Dataset: VideoGen-RewardBench, Samples: 26500, Modality: RGB videos + human preference annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Multimodal Sensor Dataset for Monitoring Older Adults Post Lower-Limb Fractures in Community Settings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.13888"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/abedidev/maison-llf"><img src="https://img.shields.io/github/stars/abedidev/maison-llf.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/14597613"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KITE Research Institute, Toronto Rehabilitation Institute, University Health Network, Toronto, Canada<br>
‚Ä¢ Dataset: MAISON-LLF, Samples: 560, Modality: Accelerometer, GPS, step count, binary motion events, heart rate, sleep data, clinical questionnaires<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.13335"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan 430074, China; S-Lab for Advanced Intelligence, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: ZJU-MoCap-Blur, Samples: 6 sequences, Modality: Synthesized motion-blurred RGB videos + SMPL poses + foreground masks<br>
‚Ä¢ Dataset: Real-Human-Blur, Samples: None, Modality: Monocular motion-blurred RGB videos + SMPL poses + foreground masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Machine Learning Modeling for Multi-order Human Visual Motion Processing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.12810"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/anoymized/multi-order-motion-model"><img src="https://img.shields.io/github/stars/anoymized/multi-order-motion-model.svg?style=social&label=Star"></a><br><a href="https://anoymized.github.io/motion-model-website/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Graduate School of Informatics, Kyoto University, Kyoto, 606-8501, Japan.<br>
‚Ä¢ Dataset: Material-Controlled Motion Dataset, Samples: None, Modality: RGB videos + optical flow<br>
‚Ä¢ Dataset: Second-order Motion Benchmark, Samples: 40 scenes, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Int2Planner: An Intention-based Multi-modal Motion Planner for Integrated Prediction and Planning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.12799"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cxlz/Int2Planner"><img src="https://img.shields.io/github/stars/cxlz/Int2Planner.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence & Department of CSE & MoE Lab of AI, Shanghai Jiao Tong University; COWAROBOT Co. Ltd.<br>
‚Ä¢ Dataset: private dataset, Samples: 680964, Modality: trajectory data, localization data, route path information<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Interaction Dataset of Autonomous Vehicles with Traffic Lights and Signs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.12536"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://uwmadison.box.com/s/dbysk2jl15w0j56hd02rfaosuhvx3zu0"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Civil and Environmental Engineering, University of Wisconsin-Madison, Madison, WI 53706, United States<br>
‚Ä¢ Dataset: Interaction Dataset of Autonomous Vehicles with Traffic Lights and Signs, Samples: 82132, Modality: Vehicle trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>TOFFE -- Temporally-binned Object Flow from Events for High-speed and Energy-Efficient Object Detection and Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.12482"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Elmore Family School of Electrical and Computer Engineering, Purdue University<br>
‚Ä¢ Dataset: TOFFE dataset, Samples: None, Modality: Synthetic data from Gazebo simulator including RGB frames, depth, event camera data, 6-DoF object pose, and object velocity.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>TalkingEyes: Pluralistic Speech-Driven 3D Eye Gaze Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.09921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lkjkjoiuiu.github.io/TalkingEyes Home/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Informatics, Xiamen University, China<br>
‚Ä¢ Dataset: TalKingEyesDataset (TKED), Samples: 5982, Modality: Audio + 3D mesh motion sequences (FLAME parameters for eye gaze, head, and face)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>SMPLest-X: Ultimate Scaling for Expressive Human Pose and Shape Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.09782"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wqyin/SMPLest-X"><img src="https://img.shields.io/github/stars/wqyin/SMPLest-X.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Tokyo, SenseTime Research<br>
‚Ä¢ Dataset: SynHand, Samples: 462800, Modality: Synthetic RGB images + SMPL-X annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Robust Single Object Tracking in LiDAR Point Clouds under Adverse Weather Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.07133"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Mathematical Sciences, Dalian University of Technology, China<br>
‚Ä¢ Dataset: KITTI-A, Samples: 2730, Modality: LiDAR point clouds<br>
‚Ä¢ Dataset: nuScenes-A, Samples: 82770, Modality: LiDAR point clouds<br>
‚Ä¢ Dataset: CADC-SOT, Samples: 7375, Modality: LiDAR point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian Splatting Based Data Augmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.02845"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://iscas3dv.github.io/HOGSA/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Software, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: HOGSA, Samples: 2400000, Modality: RGB images + 3D hand/object poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal Conditions and Larger Scene</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.02807"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Computer Engineering, Peking University, China<br>
‚Ä¢ Dataset: AE-NeRF Synthetic Event Dataset, Samples: 8, Modality: Synthetic event streams, RGB images, ground truth camera poses, estimated camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>WorldPose: A World Cup Dataset for Global 3D Human Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.02771"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://eth-ait.github.io/WorldPoseDataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich<br>
‚Ä¢ Dataset: WorldPose, Samples: 88 sequences, Modality: RGB videos + 3D human poses (SMPL) + global trajectories + camera parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>JoyGen: Audio-Driven 3D Depth-Aware Talking-Face Video Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.01798"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JOY-MM/JoyGen"><img src="https://img.shields.io/github/stars/JOY-MM/JoyGen.svg?style=social&label=Star"></a><br><a href="https://joy-mm.github.io/JoyGen"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: JD.Com, Inc.<br>
‚Ä¢ Dataset: Chinese talking-face dataset, Samples: 1100, Modality: RGB videos + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>Free-Form Motion Control: A Synthetic Video Generation Dataset with Controllable Camera and Object Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.01425"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://henghuiding.github.io/SynFMC"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: SynFMC, Samples: 26000, Modality: Synthetic videos + 6D object poses + 6D camera poses + segmentation masks + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>L3D-Pose: Lifting Pose for 3D Avatars from a Single Camera in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.01174"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://soumyaratnadebnath.github.io/L3D-Pose"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IIT Gandhinagar, India<br>
‚Ä¢ Dataset: Deep Macaque, Samples: 8000, Modality: Synthetic renderings + 2D/3D pose data<br>
‚Ä¢ Dataset: Deep Horse, Samples: 6000, Modality: Synthetic renderings + 2D/3D pose data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2025</td>
  <td style="width:70%;"><strong>DynamicLip: Shape-Independent Continuous Authentication via Lip Articulator Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2501.01032"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xxxxx"><img src="https://img.shields.io/github/stars/github.com/xxxxx.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100085, China<br>
‚Ä¢ Dataset: Dynamic Lip Authentication Dataset, Samples: 50 subjects, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>T-DOM: A Taxonomy for Robotic Manipulation of Deformable Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.20998"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/t-dom"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institut de Rob√≤tica i Inform√†tica Industrial, CSIC-UPC, Barcelona, Spain<br>
‚Ä¢ Dataset: Deformable Object Manipulation Dataset, Samples: 10 tasks, Modality: RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>DAVE: Diverse Atomic Visual Elements Dataset with High Representation of Vulnerable Road Users in Complex and Unpredictable Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.20042"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: DA VE, Samples: 1231, Modality: RGB videos + GPS + Bounding Box/Action Annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>UniAvatar: Taming Lifelike Audio-Driven Talking Head Generation with Comprehensive Motion and Lighting Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.19860"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Li Auto<br>
‚Ä¢ Dataset: DH-FaceDrasMvVid-100, Samples: None, Modality: RGB videos<br>
‚Ä¢ Dataset: DH-FaceReliVid-200, Samples: None, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>RobotDiffuse: Motion Planning for Redundant Manipulator based on Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.19500"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ACRoboT-buaa/RobotDiffuse"><img src="https://img.shields.io/github/stars/ACRoboT-buaa/RobotDiffuse.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software, Beihang University, Beijing, China.<br>
‚Ä¢ Dataset: Robot-obtalcles-panda (ROP), Samples: 140000, Modality: Robot pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Learning Monocular Depth from Events via Egomotion Compensation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.19067"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not specified in the document<br>
‚Ä¢ Dataset: EventCitySim, Samples: 5, Modality: RGB images, depth maps, event data, IMU measurements, gyroscope data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Mimicking-Bench: A Benchmark for Generalizable Humanoid-Scene Interaction Learning via Human Mimicking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.17730"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mimicking-bench.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: Mimicking-Bench, Samples: 23490, Modality: Human skeleton motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>V$^2$-SfMLearner: Learning Monocular Depth and Ego-motion for Multimodal Wireless Capsule Endoscopy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.17595"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China<br>
‚Ä¢ Dataset: Multimodal-WCE-1 (MM-WCE-1), Samples: 11, Modality: RGB videos + Vibration signals + Depth maps + Ego-motion trajectories<br>
‚Ä¢ Dataset: Multimodal-WCE-2 (MM-WCE-2), Samples: 11, Modality: RGB videos + Vibration signals + Depth maps + Ego-motion trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>WildPPG: A Real-World PPG Dataset of Long Continuous Recordings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.17540"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://siplab.org/projects/WildPPG"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science ETH Z√ºrich, Switzerland<br>
‚Ä¢ Dataset: WildPPG, Samples: 216 hours, Modality: PPG, Accelerometer, ECG, Temperature, Altitude<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>InterDance:Reactive 3D Dance Generation with Realistic Duet Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.16982"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://inter-dance.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: InterDance, Samples: None, Modality: MoCap (SMPL-X format)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Long-Term Upper-Limb Prosthesis Myocontrol via High-Density sEMG and Incremental Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.16271"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DarioDiDomenico/IncrHDsEMG"><img src="https://img.shields.io/github/stars/DarioDiDomenico/IncrHDsEMG.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/record/10801000"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Rehab Technologies Lab, Istituto Italiano di Tecnologia (IIT), Genoa, Italy, DET, Politecnico di Torino, Turin, Italy<br>
‚Ä¢ Dataset: DELTA, Samples: 2940, Modality: High-Density surface electromyography (HD-sEMG) signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>AutoLife: Automatic Life Journaling with Smartphones and LLMs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.15714"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: A self-collected human life dataset for life journaling, Samples: 58, Modality: smartphone sensor data (accelerometer, gyroscope, barometer, GPS speed, GPS location, WiFi signals)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.15664"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://virtualhumans.mpi-inf.mpg.de/scenic/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: T√ºbingen AI Center, University of T√ºbingen; Max Planck Institute for Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: The SCENIC Dataset, Samples: 15000, Modality: SMPL motion + text annotations + terrain meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Event-assisted 12-stop HDR Imaging of Dynamic Scene</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.14705"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://openimaginglab.github.io/Event-Assisted-12stops-HDR/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory<br>
‚Ä¢ Dataset: ESHDR, Samples: None, Modality: Synchronized LDR images and event camera streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Learning from Massive Human Videos for Universal Humanoid Pose Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.14172"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://usc-gvl.github.io/UH-1"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Southern California<br>
‚Ä¢ Dataset: Humanoid-X, Samples: 163800, Modality: RGB videos, text descriptions, SMPL human poses, humanoid keypoints, humanoid target DoF positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>TH√ñR-MAGNI Act: Actions for Human Motion Modeling in Robot-Shared Industrial Spaces</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.13729"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tmralmeida/thor-magni-actions"><img src="https://img.shields.io/github/stars/tmralmeida/thor-magni-actions.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AASS, ¬®Orebro University<br>
‚Ä¢ Dataset: TH¬®OR-MAGNI Act, Samples: 8.3 hours of labeled actions, Modality: Action labels, MoCap trajectories, Egocentric video, Gaze data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Move-in-2D: 2D-Conditioned Human Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.13185"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hhsinping.github.io/Move-in-2D"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Adobe Research, University of California, Merced<br>
‚Ä¢ Dataset: Humans-in-Context Motion (HiC-Motion), Samples: 300000, Modality: RGB videos + SMPL motion + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Continuous Patient Monitoring with AI: Real-Time Analysis of Video in Hospital Care Settings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.13152"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lookdeep/ai-norms-2024"><img src="https://img.shields.io/github/stars/lookdeep/ai-norms-2024.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: LookDeep Health<br>
‚Ä¢ Dataset: ai-norms-2024, Samples: 1466 patient-days, Modality: Computer vision predictions (object detection, role classification, motion estimation) from RGB/NIR video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>A New Adversarial Perspective for LiDAR-based 3D Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.13017"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Informatics, Xiamen University, China; Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, China<br>
‚Ä¢ Dataset: ROLiD, Samples: 1964 water mist sequences, 664 smoke sequences, Modality: LiDAR point cloud sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Task-Parameter Nexus: Task-Specific Parameter Learning for Model-Based Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.12448"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ILLINOIS.EDU<br>
‚Ä¢ Dataset: Trajectory Bank, Samples: 1200, Modality: 2D quadrotor polynomial trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Can video generation replace cinematographers? Research on the cinematic language of generated video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.12223"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tongji University<br>
‚Ä¢ Dataset: Cinematic2K, Samples: 2000, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Instruction-based Image Manipulation by Watching How Things Move</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.12087"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo<br>
‚Ä¢ Dataset: InstructMove dataset, Samples: 6000000, Modality: RGB image pairs + text instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.11974"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/declare-lab/EMMA-X"><img src="https://img.shields.io/github/stars/declare-lab/EMMA-X.svg?style=social&label=Star"></a><br><a href="https://declare-lab.github.io/Emma-X/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Singapore University of Technology and Design<br>
‚Ä¢ Dataset: hierarchical embodiment dataset, Samples: 60000, Modality: Robot manipulation trajectories (7D actions) + images + textual annotations (grounded reasoning, 3D movement plans) + 2D gripper positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.11198"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vita-epfl/GEM"><img src="https://img.shields.io/github/stars/vita-epfl/GEM.svg?style=social&label=Star"></a><br><a href="https://vita-epfl.github.io/GEM.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: √âcole Polytechnique F√©d√©rale de Lausanne (EPFL)<br>
‚Ä¢ Dataset: GEM Curated and Pseudo-Labeled Dataset, Samples: None, Modality: RGB videos + pseudo-labeled depth, ego-trajectories, and human poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>A Pioneering Neural Network Method for Efficient and Robust Fluid Simulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.10748"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software Engineering, Xi‚Äôan Jiaotong University, Xi‚Äôan, 710049, China<br>
‚Ä¢ Dataset: Fueltank dataset, Samples: 320000, Modality: SPH fluid particle simulation<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SUGAR: Subject-Driven Video Customization in a Zero-Shot Manner</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.10533"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Adobe Research<br>
‚Ä¢ Dataset: Unnamed synthetic dataset for subject-driven video customization, Samples: 2500000, Modality: RGB videos + text + images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.09623"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lwq20020127.github.io/OmniDrag"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Computer Engineering, Peking University<br>
‚Ä¢ Dataset: Move360, Samples: 1580, Modality: Omnidirectional videos (ODV)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.09283"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/NJU-PCALab/InstanceCap"><img src="https://img.shields.io/github/stars/NJU-PCALab/InstanceCap.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: InstanceVid, Samples: 22000, Modality: RGB videos + structured captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SyncViolinist: Music-Oriented Violin Motion Generation Based on Bowing and Fingering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.08343"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Kakanat/SyncViolinist"><img src="https://img.shields.io/github/stars/Kakanat/SyncViolinist.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Waseda University<br>
‚Ä¢ Dataset: SyncViolinist Dataset, Samples: 61, Modality: MoCap joints, audio, MIDI, bowing/fingering annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Generative Zoo</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.08101"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://genzoo.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T ¬®ubingen, Germany<br>
‚Ä¢ Dataset: GenZoo, Samples: 1000000, Modality: RGB images + 3D pose and shape parameters<br>
‚Ä¢ Dataset: GenZoo-Felidae, Samples: 100, Modality: RGB images + 3D pose and shape parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Repurposing Pre-trained Video Diffusion Models for Event-based Video Interpolation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07761"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vdm-evfi.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, Park<br>
‚Ä¢ Dataset: Clear-Motion, Samples: 9 sequences, Modality: RGB videos + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07759"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://fuxiao0719.github.io/projects/3dtrajmaster"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: 360‚ó¶-Motion Dataset, Samples: 54000, Modality: RGB videos + 6DoF pose trajectories + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07755"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Boston University<br>
‚Ä¢ Dataset: SAT (Spatial Aptitude Training), Samples: 175000, Modality: Simulated 2D images + Question-Answer pairs about static and dynamic (ego/object motion) spatial reasoning<br>
‚Ä¢ Dataset: SAT real-image dynamic test set, Samples: 150, Modality: Real-world images + Question-Answer pairs about dynamic spatial reasoning<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>On Motion Blur and Deblurring in Visual Place Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07751"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/bferrarini/MotionBlurGenerator"><img src="https://img.shields.io/github/stars/bferrarini/MotionBlurGenerator.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronics and Computer Science, University of Southampton, SO17 1BJ Southampton, U.K.<br>
‚Ä¢ Dataset: Blurry Places benchmark, Samples: 9 traverses, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Benchmarking Vision-Based Object Tracking for USVs in Complex Maritime Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.07392"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Muhayyuddin/tracking"><img src="https://img.shields.io/github/stars/Muhayyuddin/tracking.svg?style=social&label=Star"></a><br><a href="https://muhayyuddin.github.io/tracking/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University, United Arab Emirates.<br>
‚Ä¢ Dataset: Real-world USV tracking dataset, Samples: 21, Modality: RGB videos<br>
‚Ä¢ Dataset: Simulated USV tracking dataset, Samples: 5, Modality: Simulated RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view RGB and Event Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.06770"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/4dqv/DynEventNeRF"><img src="https://img.shields.io/github/stars/4dqv/DynEventNeRF.svg?style=social&label=Star"></a><br><a href="https://4dqv.mpi-inf.mpg.de/DynEventNeRF/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Informatics, SIC; Saarland University<br>
‚Ä¢ Dataset: Dynamic EventNeRF Real Dataset, Samples: 16, Modality: multi-view (6) RGB and event streams<br>
‚Ä¢ Dataset: Dynamic EventNeRF Synthetic Dataset, Samples: 5, Modality: multi-view (5) synthetic RGB frames and event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Object Detection using Event Camera: A MoE Heat Conduction based Detector and A New Benchmark Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.06647"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/OpenEvDET1"><img src="https://img.shields.io/github/stars/Event-AHU/OpenEvDET1.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: EvDET200K, Samples: 10054, Modality: Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>doScenes: An Autonomous Driving Dataset with Natural Language Instruction for Human Interaction and Vision-Language Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05893"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.github.com/rossgreer/doScenes"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Merced<br>
‚Ä¢ Dataset: doScenes, Samples: 1000, Modality: vehicle trajectories, multimodal sensor data (cameras, LiDAR, radar), natural language instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>MotionShop: Zero-Shot Motion Transfer in Video Diffusion Models with Mixture of Score Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05355"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="motionshop-diffusion.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Virginia Tech<br>
‚Ä¢ Dataset: MotionBench, Samples: 1200, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>ACT-Bench: Towards Action Controllable World Models for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05337"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Turing Inc.<br>
‚Ä¢ Dataset: ACT-BENCH, Samples: 2286, Modality: RGB videos + trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Œª: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile Manipulation Robotics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05313"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="lambdabenchmark.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: LAMBDA (Œª), Samples: 571, Modality: RGB-D egocentric observations, segmentations, robot/object poses, discrete actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Text to Blind Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.05277"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://blindways.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Boston University<br>
‚Ä¢ Dataset: BlindWays, Samples: 1029, Modality: IMU-based 3D joints, text descriptions, synchronized RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Monocular Dynamic Gaussian Splatting: Fast, Brittle, and Scene Complexity Rules</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.04457"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: instructive synthetic dataset, Samples: 30, Modality: Synthetic RGB videos + camera poses<br>
‚Ä¢ Dataset: D-NeRF, Nerfies, HyperNeRF, NeRF-DS, iPhone dataset (extensions), Samples: 50, Modality: Segmentation masks and improved camera poses for existing RGB video datasets<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.04244"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ivl.cs.brown.edu/research/gigahands.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: GigaHands, Samples: 14000, Modality: Multi-view RGB videos, 3D hand poses (keypoints, MANO meshes), 3D object poses, text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.04037"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://grisoon.github.io/INFP/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Bytedance<br>
‚Ä¢ Dataset: DyConv, Samples: over 200 hours of video clips, Modality: Video clips of dyadic conversations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Dense Scene Reconstruction from Light-Field Images Affected by Rolling Shutter</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.03518"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ICB-Vision-AI/DenseRSLF"><img src="https://img.shields.io/github/stars/ICB-Vision-AI/DenseRSLF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universit ¬¥e de Bourgogne, CNRS UMR 6303 ICB; Universit ¬¥e de Franche-Comt ¬¥e, CNRS UMR 6174 FEMTO-ST<br>
‚Ä¢ Dataset: RSLF+, Samples: None, Modality: Light-field images + depth maps + visibility masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Video LLMs for Temporal Reasoning in Long Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02930"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.retrocausal.ai"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Retrocausal, Inc.<br>
‚Ä¢ Dataset: IndustryASM, Samples: 4803, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Adaptive LiDAR Odometry and Mapping for Autonomous Agricultural Mobile Robots in Unmanned Farms</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02899"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UCR-Robotics/AG-LOAM"><img src="https://img.shields.io/github/stars/UCR-Robotics/AG-LOAM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, University of California-Riverside<br>
‚Ä¢ Dataset: AG-LOAM dataset, Samples: 18, Modality: LiDAR point clouds, robot odometry, GPS-RTK<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>emg2pose: A Large and Diverse Benchmark for Surface Electromyographic Hand Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02725"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/emg2pose"><img src="https://img.shields.io/github/stars/facebookresearch/emg2pose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Reality Labs, Meta<br>
‚Ä¢ Dataset: emg2pose, Samples: 25253, Modality: sEMG + motion capture joint angles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02449"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://byencoder.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of Freiburg, 79110 Freiburg, Germany<br>
‚Ä¢ Dataset: BYE Dataset, Samples: 29, Modality: RGB-D images, instance masks, camera pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>It Takes Two: Real-time Co-Speech Two-person's Interaction Generation via Reactive Auto-regressive Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02419"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: InterAct++, Samples: 402, Modality: MoCap joints + Face + Audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Dual Exposure Stereo for Extended Dynamic Range 3D Imaging</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.02351"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: POSTECH<br>
‚Ä¢ Dataset: Real-world Dataset, Samples: 7432, Modality: stereo RGB videos + LiDAR point clouds<br>
‚Ä¢ Dataset: Synthetic Dataset, Samples: 1200, Modality: synthetic stereo videos + depth maps + disparity maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Continuous-Time Human Motion Field from Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.01747"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Pennsylvania, USA<br>
‚Ä¢ Dataset: Beam-splitter Event Agile Human Motion Dataset (BEAHM), Samples: 40, Modality: Events + RGB videos + 3D meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Articulate3D: Holistic Understanding of 3D Scenes as Universal Scene Description</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.01398"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: INSAIT, Sofia University ‚ÄúSt. Kliment Ohridski‚Äù<br>
‚Ä¢ Dataset: Articulate3D, Samples: 280, Modality: 3D scene scans with part-level articulation kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Object Agnostic 3D Lifting in Space and Time</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.01166"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Adelaide<br>
‚Ä¢ Dataset: AnimalSyn3D, Samples: 678, Modality: 3D skeleton motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>Human Action CLIPs: Detecting AI-generated Human Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.00526"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.huggingface.co/datasets/faridlab/deepaction"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google, Stanford University<br>
‚Ä¢ Dataset: DeepAction, Samples: 3200, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.00174"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://solami-ai.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SenseTime Research<br>
‚Ä¢ Dataset: SynMSI, Samples: 6300, Modality: SMPL-X joint rotations and synthesized speech<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>ETAP: Event-based Tracking of Any Point</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.00133"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tub-rip/ETAP"><img src="https://img.shields.io/github/stars/tub-rip/ETAP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit√§t Berlin<br>
‚Ä¢ Dataset: EventKubric, Samples: 10173, Modality: Events + RGB videos + point tracks + optical flow + depth + segmentations<br>
‚Ä¢ Dataset: EVIMO2, Samples: None, Modality: ground truth point tracks (new annotation)<br>
‚Ä¢ Dataset: E2D2, Samples: None, Modality: ground truth point tracks (new annotation)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2024</td>
  <td style="width:70%;"><strong>OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2412.00115"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fudan-generative-vision.github.io/OpenHumanVid"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: OpenHumanVid, Samples: 13200000, Modality: RGB videos + skeleton pose + speech audio + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>SIMS: Simulating Stylized Human-Scene Interactions with Retrieval-Augmented Script Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.19921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wenjiawang0312.github.io/projects/sims/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: ViconStyle, Samples: 415, Modality: MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>SkelMamba: A State Space Model for Efficient Skeleton Action Recognition of Neurological Disorders</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.19544"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Udine<br>
‚Ä¢ Dataset: Neurological Disorders (ND), Samples: 396, Modality: RGB videos + skeleton joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.19167"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/hand-tracking-toolkit"><img src="https://img.shields.io/github/stars/facebookresearch/hand-tracking-toolkit.svg?style=social&label=Star"></a><br><a href="https://facebookresearch.github.io/hot3d"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta Reality Labs<br>
‚Ä¢ Dataset: HOT3D, Samples: 3832, Modality: Multi-view egocentric RGB/monochrome videos + motion-capture poses (hands and objects) + eye gaze + SLAM point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Lifting Motion to the 3D World via 2D Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.18808"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: NicoleMove, Samples: None, Modality: RGB videos + 2D pose sequences<br>
‚Ä¢ Dataset: CatPlay, Samples: None, Modality: RGB videos + 2D keypoint sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>GaussianSpeech: Audio-Driven Gaussian Avatars</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.18675"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://shivangi-aneja.github.io/projects/gaussianspeech"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich<br>
‚Ä¢ Dataset: Multi-view Audio-Visual Dataset (unnamed), Samples: 2500, Modality: Multi-view RGB videos, audio, and 3D face trackings<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.18654"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Motion-AILab/AToM"><img src="https://img.shields.io/github/stars/Motion-AILab/AToM.svg?style=social&label=Star"></a><br><a href="https://atom-motion.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen International Graduate School, Tsinghua University<br>
‚Ä¢ Dataset: MotionPrefer, Samples: 47100, Modality: Generated 3D human motion sequences with text-based preference labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>MotionCharacter: Identity-Preserving and Motion Controllable Human Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.18281"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motioncharacter.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huazhong University of Science and Technology<br>
‚Ä¢ Dataset: Human-Motion, Samples: 106292, Modality: RGB videos + text captions + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Snake-Inspired Mobile Robot Positioning with Hybrid Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.17430"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ansfl/MoRPINet"><img src="https://img.shields.io/github/stars/ansfl/MoRPINet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hatter Department of Marine Technologies, University of Haifa, Israel<br>
‚Ä¢ Dataset: MoRPINet, Samples: 13, Modality: IMU + GNSS-RTK<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Enhancing Lane Segment Perception and Topology Reasoning with Crowdsourcing Trajectory Priors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.17161"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wowlza/TrajTopo"><img src="https://img.shields.io/github/stars/wowlza/TrajTopo.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Vehicle and Mobility, Tsinghua University<br>
‚Ä¢ Dataset: Supplementary Trajectory Dataset for OpenLane-V2, Samples: None, Modality: crowdsourcing trajectory data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Leveraging Foundation Models To learn the shape of semi-fluid deformable objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16802"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Alstom transports, IMVIA EA 7535 laboratory, university of Burgundy<br>
‚Ä¢ Dataset: Weld Pool Dataset, Samples: 9, Modality: RGB videos + keypoint labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Bundle Adjusted Gaussian Avatars Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16758"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Artificial Intelligence Laboratory, The University of Tokyo<br>
‚Ä¢ Dataset: Synthetic ZJU-MoCap Deblurring Dataset, Samples: 6, Modality: RGB videos + SMPL parameters<br>
‚Ä¢ Dataset: 360-degree Hybrid-Exposure Human Motion Dataset, Samples: 8, Modality: Multi-view RGB videos (blurry and sharp) + SMPL parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>DiM-Gestor: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16729"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zf223669/DiMGestures"><img src="https://img.shields.io/github/stars/zf223669/DiMGestures.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Media Engineering, Communication University of Zhejiang, China<br>
‚Ä¢ Dataset: Chinese Co-Speech Gestures (CCG) dataset, Samples: 391, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Deep Learning for Motion Classification in Ankle Exoskeletons Using Surface EMG and IMU Signals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16273"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Sr933/Exoskeleton-Data-Acquisition-and-Processing-Code"><img src="https://img.shields.io/github/stars/Sr933/Exoskeleton-Data-Acquisition-and-Processing-Code.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.17863/CAM.113504"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Electrical Engineering Division, Department of Engineering, University of Cambridge, Cambridge CB3 0FA, UK; School of Clinical Medicine, University of Cambridge, Cambridge CB2 0SP, UK<br>
‚Ä¢ Dataset: None, Samples: 1504, Modality: sEMG and IMU signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>SMGDiff: Soccer Motion Generation using diffusion probabilistic models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.16216"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Soccer-X, Samples: 2398, Modality: MoCap sequences (SMPL)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>KinMo: Kinematic-aware Human Motion Understanding and Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.15472"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://andypinxinliu.github.io/KinMo"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Irvine<br>
‚Ä¢ Dataset: KinMo, Samples: 14616, Modality: MoCap joints + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>A Benchmark Dataset for Collaborative SLAM in Service Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.14775"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vision3d-lab/CSE_Dataset"><img src="https://img.shields.io/github/stars/vision3d-lab/CSE_Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Artificial Intelligence Graduate School, UNIST, South Korea<br>
‚Ä¢ Dataset: C-SLAM dataset in Service Environments (CSE), Samples: 18, Modality: stereo RGB, stereo depth, IMU, GT pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>sEMG-based Gesture-Free Hand Intention Recognition: System, Dataset, Toolbox, and Benchmark Results</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.14131"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Intelligence Science and Technology, National University of Defense Technology, Changsha 410073, China<br>
‚Ä¢ Dataset: sEMG-based Gesture-Free Hand Intention Recognition Dataset, Samples: None, Modality: 8-channel sEMG signals, 3-channel IMU data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Sparse Input View Synthesis: 3D Representations and Reliable Priors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.13631"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nagabhushansn95.github.io/publications.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Electrical Communication Engineering, Indian Institute of Science<br>
‚Ä¢ Dataset: Indian Institute of Science Virtual Environment Exploration Dataset - Dynamic Scenes (IISc VEED-Dynamic), Samples: 800, Modality: RGB videos + depth + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>VioPose: Violin Performance 4D Pose Estimation by Hierarchical Audiovisual Inference</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.13607"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SeongJong-Yoo/VioPose"><img src="https://img.shields.io/github/stars/SeongJong-Yoo/VioPose.svg?style=social&label=Star"></a><br><a href="https://sj-yoo.info/viopose/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: VioDat, Samples: 639, Modality: 3D motion capture (kinematic joints), synchronized multi-view video (4 cameras), synchronized audio (4 microphones)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Can Reasons Help Improve Pedestrian Intent Estimation? A Cross-Modal Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.13302"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Visual Information Technology (CVIT) Lab, IIIT Hyderabad<br>
‚Ä¢ Dataset: PIE++, Samples: 1842, Modality: RGB videos + multi-label textual reason annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Enhancing Thermal MOT: A Novel Box Association Method Leveraging Thermal Identity and Motion Similarity</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.12943"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wassimea/thermalMOT"><img src="https://img.shields.io/github/stars/wassimea/thermalMOT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Ottawa<br>
‚Ä¢ Dataset: RGB-Thermal MOT dataset, Samples: 30, Modality: RGB videos + Thermal videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>EROAM: Event-based Camera Rotational Odometry and Mapping in Real-time</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.11004"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wlxing1901/EROAM"><img src="https://img.shields.io/github/stars/wlxing1901/EROAM.svg?style=social&label=Star"></a><br><a href="https://wlxing1901.github.io/eroam/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, The University of Hong Kong, Hong Kong SAR, China<br>
‚Ä¢ Dataset: EROAM-campus, Samples: 10, Modality: Event camera + LiDAR + 3DoF rotational motion trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>The Oxford Spires Dataset: Benchmarking Large-Scale LiDAR-Visual Localisation, Reconstruction and Radiance Field Methods</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.10546"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Oxford Robotics Inst., Dept. of Eng. Science, Univ. of Oxford, UK<br>
‚Ä¢ Dataset: Oxford Spires Dataset, Samples: 24, Modality: RGB images, LiDAR, IMU, Ground Truth Trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.10504"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chenkang455/USP-Gaussian"><img src="https://img.shields.io/github/stars/chenkang455/USP-Gaussian.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Peking University<br>
‚Ä¢ Dataset: Synthetic Spike Dataset (based on Deblur-NeRF), Samples: None, Modality: Spike streams<br>
‚Ä¢ Dataset: Real-world Spike Dataset, Samples: None, Modality: Spike streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Gait Kinematics in Healthy Participants: A Motion Capture Dataset Under Weight Load and Knee Brace Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.10485"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://figshare.com/articles/dataset/IMU-Based_Motion_Capture_Data_for_Various_Walking_Tasks/26090200"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Electrical Engineering, K. N. Toosi University of Technology, Tehran, 1631714191, Iran<br>
‚Ä¢ Dataset: IMU-Based Motion Capture Data for Various Walking Tasks, Samples: None, Modality: IMU data (raw sensor, processed, Euler angles, joint kinematics)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.09921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://groundmore.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CRCV, University of Central Florida<br>
‚Ä¢ Dataset: GROUND-MORE, Samples: 1715, Modality: RGB videos + text questions + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>SINETRA: a Versatile Framework for Evaluating Single Neuron Tracking in Behaving Animals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.09462"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/raphaelreme/SINETRA"><img src="https://img.shields.io/github/stars/raphaelreme/SINETRA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institut Pasteur, Universit ¬¥e de Paris-Cit ¬¥e, CNRS UMR 3691, BioImage Analysis Unit F-75015 Paris, France<br>
‚Ä¢ Dataset: SINETRA Synthetic Dataset, Samples: 15, Modality: Synthetic 2D/3D fluorescence videos with ground truth particle trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.08380"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://egovid.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Alibaba<br>
‚Ä¢ Dataset: EgoVid-5M, Samples: 5000000, Modality: RGB videos + kinematic control (VIO) + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.08279"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WU-CVGL/MBA-SLAM"><img src="https://img.shields.io/github/stars/WU-CVGL/MBA-SLAM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology at Zhejiang University and the School of Engineering at Westlake University, Hangzhou, China<br>
‚Ä¢ Dataset: real-world motion-blurred SLAM dataset, Samples: 3, Modality: RGB-D video + MoCap ground truth trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.06757"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/quzefan/LuSh-NeRF"><img src="https://img.shields.io/github/stars/quzefan/LuSh-NeRF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science City University of Hong Kong<br>
‚Ä¢ Dataset: LOL-BlurNeRF, Samples: 10, Modality: RGB videos + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>GraV: Grasp Volume Data for the Design of One-Handed XR Interfaces</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.05245"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HAL-UCSB/grav-sim"><img src="https://img.shields.io/github/stars/HAL-UCSB/grav-sim.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California Santa Barbara, CA, USA<br>
‚Ä¢ Dataset: GraV, Samples: 367, Modality: point clouds + motion cost<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Pose2Trajectory: Using Transformers on Body Pose to Predict Tennis Player's Trajectory</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.04501"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/alshami52/Pose2Trajectory.git"><img src="https://img.shields.io/github/stars/alshami52/Pose2Trajectory.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Science Department, University of Colorado, Colorado Springs<br>
‚Ä¢ Dataset: None, Samples: , Modality: 2D joint positions, bounding boxes, ball coordinates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>MA^2: A Self-Supervised and Motion Augmenting Autoencoder for Gait-Based Automatic Disease Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.03129"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/EchoItLiu/MA2-PyTorch"><img src="https://img.shields.io/github/stars/EchoItLiu/MA2-PyTorch.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information Technology and Media, Hexi University, Zhangye, 734000, P.R. China<br>
‚Ä¢ Dataset: tRGG, Samples: 101125, Modality: Ground Reaction Force (GRF)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Real-Time Detection for Small UAVs: Combining YOLO and Multi-frame Motion Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.02582"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Aerospace Engineering, Beijing Institute of Technology, Beijing 100081, China<br>
‚Ä¢ Dataset: Fixed-Wings Dataset, Samples: 13, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>GenXD: Generating Any 3D and 4D Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.02319"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gen-x-d.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National University of Singapore<br>
‚Ä¢ Dataset: CamVid-30K, Samples: 30000, Modality: videos with camera poses and object motion strength<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Object segmentation from common fate: Motion energy processing enables human-like zero-shot generalization to random dot stimuli</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.01505"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mtangemann/motion_energy_segmentation"><img src="https://img.shields.io/github/stars/mtangemann/motion_energy_segmentation.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of T√ºbingen, T√ºbingen AI Center<br>
‚Ä¢ Dataset: Synthetic video dataset for motion segmentation (unnamed), Samples: 1001, Modality: Synthetic RGB videos with ground truth masks and optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Nightbeat: Heart Rate Estimation From a Wrist-Worn Accelerometer During Sleep</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.00731"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/eth-siplab/Nightbeat"><img src="https://img.shields.io/github/stars/eth-siplab/Nightbeat.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, ETH Zurich, Z√ºrich, Switzerland<br>
‚Ä¢ Dataset: Nightbeat-DB, Samples: 42, Modality: 3-axis accelerometer signals + ECG signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2024</td>
  <td style="width:70%;"><strong>Muscles in Time: Learning to Understand Human Motion by Simulating Muscle Activations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2411.00128"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/simplexsigil/MusclesInTime"><img src="https://img.shields.io/github/stars/simplexsigil/MusclesInTime.svg?style=social&label=Star"></a><br><a href="https://simplexsigil.github.io/mint"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Karlsruhe Institute of Technology<br>
‚Ä¢ Dataset: Muscles in Time (MinT), Samples: None, Modality: Simulated muscle activations + pose sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Learning Video Representations without Natural Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.24213"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://unicorn53547.github.io/video_syn_rep/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Accelerating and transforming textures, Samples: 9537, Modality: Synthetic RGB videos<br>
‚Ä¢ Dataset: Accelerating and transforming StyleGAN crops, Samples: 9537, Modality: Synthetic RGB videos<br>
‚Ä¢ Dataset: Accelerating and transforming image crops, Samples: 9537, Modality: Synthetic RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Exploiting Information Theory for Intuitive Robot Programming of Manual Activities</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.23963"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.13846970"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Human-Robot Interfaces and Interaction, Istituto Italiano di Tecnologia, Genoa, Italy<br>
‚Ä¢ Dataset: HANDSOME (HAND Skills demOnstrated by Multi-subjEcts), Samples: 250, Modality: RGB videos + 6D poses (from ArUco markers)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided Mixture-of-Experts</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.23836"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: HDAV (High-definition Audio-Visual dataset), Samples: 2203, Modality: RGB videos + audio + 3D human template parameter annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>GS-Blur: A 3D Scene-Based Dataset for Realistic Image Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.23658"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dongwoohhh/GS-Blur"><img src="https://img.shields.io/github/stars/dongwoohhh/GS-Blur.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of ECE&ASRI, Seoul National University, Korea<br>
‚Ä¢ Dataset: GS-Blur, Samples: 752335, Modality: Paired blurry and sharp RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>bit2bit: 1-bit quanta video reconstruction via self-supervised photon prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.23247"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lyehe/ssunet"><img src="https://img.shields.io/github/stars/lyehe/ssunet.svg?style=social&label=Star"></a><br><a href="https://drive.google.com/drive/folders/1M5bsmsaLBkYmO7nMUjK5_m71RonOp-P9"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Case Western Reserve University<br>
‚Ä¢ Dataset: bit2bit SPAD video dataset, Samples: 7 real SPAD videos (100k-130k frames each) and 1 synthetic video (3990 frames), Modality: 1-bit SPAD high-speed videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>NYC-Event-VPR: A Large-Scale High-Resolution Event-Based Visual Place Recognition Dataset in Dense Urban Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.21615"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ai4ce/NYC-Event-VPR"><img src="https://img.shields.io/github/stars/ai4ce/NYC-Event-VPR.svg?style=social&label=Star"></a><br><a href="https://ai4ce.github.io/NYC-Event-VPR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: New York University, Brooklyn, NY 11201, USA<br>
‚Ä¢ Dataset: NYC-Event-VPR, Samples: None, Modality: event streams, RGB videos, GPS trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Skinned Motion Retargeting with Dense Geometric Interaction Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20986"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/abcyzj/MeshRet"><img src="https://img.shields.io/github/stars/abcyzj/MeshRet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Technology, BNRist, Tsinghua University; Key Laboratory of Pervasive Computing, Ministry of Education<br>
‚Ä¢ Dataset: ScanRet, Samples: 8298, Modality: Motion capture sequences on 3D scanned meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>CardiacNet: Learning to Reconstruct Abnormalities for Cardiac Disease Assessment from Echocardiogram Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20769"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xmed-lab/CardiacNet"><img src="https://img.shields.io/github/stars/xmed-lab/CardiacNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: CardiacNet-PAH, Samples: 496, Modality: Echocardiogram videos<br>
‚Ä¢ Dataset: CardiacNet-ASD, Samples: 231, Modality: Echocardiogram videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>YourSkatingCoach: A Figure Skating Video Benchmark for Fine-Grained Element Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20427"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Science, Academia Sinica<br>
‚Ä¢ Dataset: YourSkatingCoach, Samples: 454, Modality: RGB videos + 2D skeletons<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>NT-VOT211: A Large-Scale Benchmark for Night-time Visual Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20421"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/LiuYuML/NV-VOT211"><img src="https://img.shields.io/github/stars/LiuYuML/NV-VOT211.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xinjiang University<br>
‚Ä¢ Dataset: NT-VOT211, Samples: 211, Modality: RGB videos + bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>SFTrack: A Robust Scale and Motion Adaptive Algorithm for Tracking Small and Fast Moving Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.20079"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Immersive Media Engineering & Department of Computer Science Education, Sungkyunkwan University<br>
‚Ä¢ Dataset: Refined UAVDT, Samples: 55, Modality: RGB videos + bounding box trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>On Occlusions in Video Action Detection: Benchmark Datasets And Training Recipes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.19553"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rajatmodi62/OccludedActionBenchmark"><img src="https://img.shields.io/github/stars/rajatmodi62/OccludedActionBenchmark.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CRCV, University of Central Florida<br>
‚Ä¢ Dataset: O-UCF, Samples: 20306, Modality: RGB videos<br>
‚Ä¢ Dataset: OVIS-UCF, Samples: 20306, Modality: RGB videos<br>
‚Ä¢ Dataset: O-JHMDB, Samples: 5896, Modality: RGB videos<br>
‚Ä¢ Dataset: OVIS-JHMDB, Samples: 5896, Modality: RGB videos<br>
‚Ä¢ Dataset: Real-OUCF, Samples: 1743, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>x-RAGE: eXtended Reality -- Action & Gesture Events Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.19486"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.com/NVM_IITD_Research/xrage"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Indian Institute of Technology Delhi, New Delhi, India - 110016<br>
‚Ä¢ Dataset: X-RAGE, Samples: 8064, Modality: event camera<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>OVT-B: A New Large-Scale Benchmark for Open-Vocabulary Multi-Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.17534"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Coo1Sea/OVT-B-Dataset"><img src="https://img.shields.io/github/stars/Coo1Sea/OVT-B-Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software Technology, Zhejiang University<br>
‚Ä¢ Dataset: OVT-B, Samples: 1973, Modality: RGB videos + bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>E-3DGS: Gaussian Splatting with Exposure and Motion Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.16995"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MasterHow/E-3DGS"><img src="https://img.shields.io/github/stars/MasterHow/E-3DGS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, China<br>
‚Ä¢ Dataset: EME-3D, Samples: 9, Modality: Event data (motion and exposure), camera parameters, sparse point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Pedestrian motion prediction evaluation for urban autonomous driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.16864"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dmytrozabolotnii/autoware-mini"><img src="https://img.shields.io/github/stars/dmytrozabolotnii/autoware-mini.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Computer Science, University of Tartu<br>
‚Ä¢ Dataset: Tartu Autonomous Vehicle Dataset, Samples: 18 scenes, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.16695"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chyangyu/MPT.git"><img src="https://img.shields.io/github/stars/chyangyu/MPT.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology, Ocean University of China, Qingdao, China<br>
‚Ä¢ Dataset: MPT (Multiple Phytoplankton Tracking), Samples: 140, Modality: High-resolution videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MotionGlot: A Multi-Embodied Motion Generation Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.16623"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ivl.cs.brown.edu/research/motionglot.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University, USA<br>
‚Ä¢ Dataset: QUAD-LOCO, Samples: 48000 trajectories, Modality: Quadruped robot trajectories (velocities) + text annotations<br>
‚Ä¢ Dataset: QUES-CAP, Samples: 23000 prompts, Modality: Situational text prompts (questions) paired with human motion data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MSGField: A Unified Scene Representation Integrating Motion, Semantics, and Geometry for Robotic Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.15730"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="MSGField.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei 230026, China<br>
‚Ä¢ Dataset: MSGField Dataset, Samples: 4, Modality: RGB video sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MCCoder: Streamlining Motion Control with LLM-Assisted Code Generation and Rigorous Verification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.15154"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MCCodeAI/MCCoder"><img src="https://img.shields.io/github/stars/MCCodeAI/MCCoder.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Thrust of Data Science and Analytics, The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: MCEVAL, Samples: 186, Modality: Programming tasks (natural language instruction, canonical Python code) which generate motion trajectories for a soft-motion controller.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Quanta Video Restoration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.14994"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chennuriprateek/Quanta_Video_Restoration-QUIVER-"><img src="https://img.shields.io/github/stars/chennuriprateek/Quanta_Video_Restoration-QUIVER-.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Purdue University<br>
‚Ä¢ Dataset: I 2-2000FPS, Samples: 280, Modality: High-speed grayscale videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Unlabeled Action Quality Assessment Based on Multi-dimensional Adaptive Constrained Dynamic Time Warping</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.14161"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China<br>
‚Ä¢ Dataset: BGym, Samples: 153, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Skill Generalization with Verbs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.14118"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://rachelma80000.github.io/SkillGenVerbs/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University, Providence, RI, USA<br>
‚Ä¢ Dataset: No explicit name given, Samples: 41688, Modality: RGB image sequences of object trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.13830"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dreamvideo2.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: DreamVideo-2 Dataset, Samples: 230160, Modality: RGB videos with captions, frame-level masks, and bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MotionBank: A Large-scale Video Motion Benchmark with Disentangled Rule-based Annotations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.13790"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/liangxuy/MotionBank"><img src="https://img.shields.io/github/stars/liangxuy/MotionBank.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Eastern Institute of Technology, Ningbo<br>
‚Ä¢ Dataset: MotionBank, Samples: 1240000, Modality: SMPL parameters extracted from videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Arc-Length-Based Warping for Robot Skill Synthesis from Multiple Demonstrations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.13322"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AutoLabModena/AutoLab-Co-Manipulation-Dataset.git"><img src="https://img.shields.io/github/stars/AutoLabModena/AutoLab-Co-Manipulation-Dataset.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Italian Institute of Technology, Genova, Italy<br>
‚Ä¢ Dataset: AutoLab Co-Manipulation Dataset, Samples: 126, Modality: robot end-effector position recordings<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>QueensCAMP: an RGB-D dataset for robust Visual SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.12520"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/larocs/queenscamp-dataset"><img src="https://img.shields.io/github/stars/larocs/queenscamp-dataset.svg?style=social&label=Star"></a><br><a href="https://larocs.github.io/queenscamp-dataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Computing, Universidade de Campinas<br>
‚Ä¢ Dataset: QueensCAMP, Samples: 112, Modality: RGB-D videos + 6DoF pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.11838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hifi-diffusion.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google<br>
‚Ä¢ Dataset: LaMoR, Samples: 19, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Look Ma, no markers: holistic performance capture without the hassle</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.11520"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://aka.ms/SynthMoCap"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Microsoft<br>
‚Ä¢ Dataset: SynthBody, Samples: 100000, Modality: Synthetic RGB images + 2D landmarks + semantic segmentation + 3D joint locations + SMPL-H parameters<br>
‚Ä¢ Dataset: SynthFace, Samples: 100000, Modality: Synthetic RGB images + 2D landmarks + semantic segmentation + head pose<br>
‚Ä¢ Dataset: SynthHand, Samples: 100000, Modality: Synthetic RGB images + 2D/3D joint locations + MANO parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.10818"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://TemporalBench.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Wisconsin-Madison<br>
‚Ä¢ Dataset: TemporalBench, Samples: 2179, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>LVD-2M: A Long-take Video Dataset with Temporally Dense Captions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.10816"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SilentView/LVD-2M"><img src="https://img.shields.io/github/stars/SilentView/LVD-2M.svg?style=social&label=Star"></a><br><a href="https://silentview.github.io/LVD-2M/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: LVD-2M, Samples: 2000000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Self-Assessed Generation: Trustworthy Label Generation for Optical Flow and Stereo Matching in Real-world</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.10453"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HanLingsgjk/UnifiedGeneralization"><img src="https://img.shields.io/github/stars/HanLingsgjk/UnifiedGeneralization.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University of Science and Technology<br>
‚Ä¢ Dataset: GS58, Samples: 58800, Modality: RGB image pairs + optical flow labels / stereo disparity labels<br>
‚Ä¢ Dataset: NeRF58, Samples: 58800, Modality: RGB image pairs + optical flow labels / stereo disparity labels<br>
‚Ä¢ Dataset: 3D Flight Foreground Database, Samples: 250000, Modality: RGB image pairs + optical flow labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.09374"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/NAIL-HNU/ESVO2.git"><img src="https://img.shields.io/github/stars/NAIL-HNU/ESVO2.git.svg?style=social&label=Star"></a><br><a href="https://youtu.be/gmAU32Oeiv8"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Neuromorphic Automation and Intelligence Lab (NAIL) at School of Robotics, Hunan University, Changsha, China<br>
‚Ä¢ Dataset: hnu mapping and hnu tracking, Samples: 2, Modality: Stereo event camera, IMU, GNSS (RTK)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Enhanced Kalman with Adaptive Appearance Motion SORT for Grounded Generic Multiple Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.09243"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://UARK-AICV.github.io/G2MOT"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Arkansas<br>
‚Ä¢ Dataset: G2MOT, Samples: 253, Modality: RGB videos + bounding box tracks + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MMHead: Towards Fine-grained Multi-modal 3D Facial Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.07757"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wsj-sjtu.github.io/MMHead/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: MMHead, Samples: 35903, Modality: 3D facial motion (FLAME parameters) + speech audio + hierarchical text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.07718"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fudan-generative-vision.github.io/hallo2"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: Wild, Samples: 2019, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.07659"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northwestern University, Yellow.ai<br>
‚Ä¢ Dataset: WebVid-10M-recaptioned, Samples: 10000000, Modality: RGB videos + text captions<br>
‚Ä¢ Dataset: Curated YouTube-VOS for Sketch-guided Inpainting, Samples: None, Modality: RGB videos + binary masks + sketches + text captions<br>
‚Ä¢ Dataset: Curated DAVIS for Sketch-guided Inpainting, Samples: None, Modality: RGB videos + binary masks + sketches + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>CoPESD: A Multi-Level Surgical Motion Dataset for Training Large Vision-Language Models to Co-Pilot Endoscopic Submucosal Dissection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.07540"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gkw0010/CoPESD"><img src="https://img.shields.io/github/stars/gkw0010/CoPESD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China.<br>
‚Ä¢ Dataset: CoPESD, Samples: 88395, Modality: RGB images + bounding boxes + text-based motion instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Learning to Generate Diverse Pedestrian Movements from Web Videos with Noisy Labels</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.07500"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://genforce.github.io/PedGen/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of California, Los Angeles<br>
‚Ä¢ Dataset: CityWalkers, Samples: 120914, Modality: RGB videos + SMPL parameters + depth maps + semantic segmentation maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.06963"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://movin3d.github.io/ELMO_SIGASIA2024/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MOVIN Inc., South Korea<br>
‚Ä¢ Dataset: ELMO dataset, Samples: 20 subjects, Modality: LiDAR point clouds + Optical Motion Capture + Video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.06437"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kt2024-hal/LocoVR"><img src="https://img.shields.io/github/stars/kt2024-hal/LocoVR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California Santa Barbara, Toyota Motor North America<br>
‚Ä¢ Dataset: LocoVR, Samples: 7071, Modality: 3D motion capture trajectories (head, hands, waist, feet) in virtual reality<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Are Minimal Radial Distortion Solvers Necessary for Relative Pose Estimation?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.05984"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kocurvik/rd"><img src="https://img.shields.io/github/stars/kocurvik/rd.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague<br>
‚Ä¢ Dataset: ROTUNDA and CATHEDRAL Benchmark, Samples: 2891, Modality: RGB images + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly Detection in Surveillance Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.05900"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: VCA Group, Department of Electrical Engineering, Eindhoven University of Technology<br>
‚Ä¢ Dataset: Video Anomaly Detection Dataset (VADD), Samples: 2591, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>F√ºrElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.05791"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University, USA<br>
‚Ä¢ Dataset: F√ºrElise, Samples: 153, Modality: 3D hand motions + audio + MIDI<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>A Unified Framework for Motion Reasoning and Generation in Human Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.05628"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vim-motion-language.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea University<br>
‚Ä¢ Dataset: Inter-MT2, Samples: 153000, Modality: MoCap joints (SMPL-X) + Text Instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Real-Time Truly-Coupled Lidar-Inertial Motion Correction and Spatiotemporal Dynamic Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.05152"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://uts-ri.github.io/lidar inertial motion correction/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics Institute at the University of Technology Sydney, Australia<br>
‚Ä¢ Dataset: Techlab dataset, Samples: 2, Modality: LiDAR, IMU, MoCap<br>
‚Ä¢ Dataset: Newer College dataset (extended), Samples: 3, Modality: LiDAR, IMU, dynamicity labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Lost in Tracking: Uncertainty-guided Cardiac Cine MRI Segmentation at Right Ventricle Base</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.03320"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.tudelft.nl/yidongzhao/rvot_seg"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Imaging Physics, Delft University of Technology, Lorentzweg 1, 2628 CJ Delft, The Netherlands<br>
‚Ä¢ Dataset: ACDC dataset with refined RV base annotation, Samples: 150, Modality: Cardiac cine MRI + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Scaling Large Motion Models with Million-Level Human Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.03311"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://beingbeyond.github.io/Being-M0/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Renmin University of China<br>
‚Ä¢ Dataset: MotionLib, Samples: 1210000, Modality: 3D human motion (SMPL parameters) from RGB videos with hierarchical text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Autonomous Character-Scene Interaction Synthesis from Text Instruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.03187"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lingomotions.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for AI, Peking University, China and National Key Lab of General AI, BIGAI, China<br>
‚Ä¢ Dataset: LINGO, Samples: 16 hours of motion sequences, Modality: MoCap sequences (SMPL-X parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>AirLetters: An Open Video Dataset of Characters Drawn in the Air</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.02921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="developer.qualcomm.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto<br>
‚Ä¢ Dataset: AirLetters, Samples: 161652, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater Scene Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.01517"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WangHaoran16/UW-GS"><img src="https://img.shields.io/github/stars/WangHaoran16/UW-GS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, University of Bristol, Bristol, UK<br>
‚Ä¢ Dataset: S-UW, Samples: 4, Modality: RGB videos + dynamic object masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Learning Physics From Video: Unsupervised Physical Parameter Estimation for Continuous Dynamical Systems</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.01376"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Alejandro-neuro/Learning_physics_from_video"><img src="https://img.shields.io/github/stars/Alejandro-neuro/Learning_physics_from_video.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Delft University of Technology<br>
‚Ä¢ Dataset: Delfys75, Samples: 75, Modality: RGB videos + object masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>Towards Native Generative Model for 3D Head Avatar</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.01226"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory for Novel Software Technology, Nanjing University, China<br>
‚Ä¢ Dataset: SYNHEAD100, Samples: 5200, Modality: 3D mesh models with blendshape rigging<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.00371"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="aha-vlm.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA, University of Washington<br>
‚Ä¢ Dataset: AHAdataset, Samples: 49000, Modality: RGB images of robot failure trajectories + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2024</td>
  <td style="width:70%;"><strong>MM-Conv: A Multi-modal Conversational Dataset for Virtual Humans</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2410.00253"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mm-conv.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology, Stockholm, Sweden<br>
‚Ä¢ Dataset: MM-Conv, Samples: 6.7 hours of recordings, Modality: MoCap joints (50-marker skeleton), speech, gaze, facial expressions, scene graphs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>UniAff: A Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.20551"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SiyuanH-SJTu/UniAff"><img src="https://img.shields.io/github/stars/SiyuanH-SJTu/UniAff.svg?style=social&label=Star"></a><br><a href="https://sites.google.com/view/uni-aff/home"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University, China<br>
‚Ä¢ Dataset: UniAff Dataset, Samples: 63200, Modality: RGB-D images + 6D pose, joint axis, affordance annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Tracking Everything in Robotic-Assisted Surgery</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.19821"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zhanbh1019/SurgicalMotion"><img src="https://img.shields.io/github/stars/zhanbh1019/SurgicalMotion.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hamlyn Centre for Robotic Surgery, Imperial College London, SW7 2AZ, UK<br>
‚Ä¢ Dataset: SurgicalMotion, Samples: 20, Modality: RGB videos + 2D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>High Quality Human Image Animation using Regional Supervision and Motion Blur Condition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.19580"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Showlab, National University of Singapore<br>
‚Ä¢ Dataset: HumanDance, Samples: 3802, Modality: RGB videos + 2D keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>S2O: Static to Openable Enhancement for Articulated 3D Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.18896"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/3dlg-hcvc/s2o"><img src="https://img.shields.io/github/stars/3dlg-hcvc/s2o.svg?style=social&label=Star"></a><br><a href="https://3dlg-hcvc.github.io/s2o/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Simon Fraser University<br>
‚Ä¢ Dataset: Articulated Containers Dataset (ACD), Samples: 1350, Modality: 3D meshes with annotated part kinematics (part segmentation, motion type, motion axis, motion origin)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>EyeTrAES: Fine-grained, Low-Latency Eye Tracking via Adaptive Event Slicing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.18813"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/EyeTrAES"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Indian Institute of Technology Kharagpur, India<br>
‚Ä¢ Dataset: EyeTrAES, Samples: 40, Modality: Event camera data (events, grayscale frames) and eye tracker data (grayscale frames, point of gaze)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.17988"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wengflow.github.io/deblur-e-nerf"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The NUS Graduate School‚Äôs Integrative Sciences and Engineering Programme (ISEP)<br>
‚Ä¢ Dataset: Deblur e-NeRF Synthetic Event Dataset, Samples: None, Modality: Synthetic event streams with ground truth camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>BlinkTrack: Feature Tracking over 100 FPS via Events and Images</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.17981"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Lab of CAD&CG, Zhejiang University<br>
‚Ä¢ Dataset: MultiTrack, Samples: 2000, Modality: Synthetic images, events, and point trajectories with occlusion<br>
‚Ä¢ Dataset: EC-occ, Samples: None, Modality: RGB frames, events, and point trajectories with synthetic occlusions<br>
‚Ä¢ Dataset: EDS-occ, Samples: None, Modality: RGB frames, events, and point trajectories with synthetic occlusions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Subjective and Objective Quality-of-Experience Evaluation Study for Live Video Streaming</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.17596"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: TaoLive QoE Database, Samples: 1155, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>A vision-based framework for human behavior understanding in industrial assembly lines</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.17356"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zenodo.org/uploads/13370888"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Computer Science, Foundation for Research and Technology - Hellas, Greece<br>
‚Ä¢ Dataset: CarDA, Samples: 25, Modality: RGB-D videos, motion capture data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Adverse Weather Optical Flow: Cumulative Homogeneous-Heterogeneous Adaptation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.17001"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hyzhouboy/CH2DA-Flow"><img src="https://img.shields.io/github/stars/hyzhouboy/CH2DA-Flow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China<br>
‚Ä¢ Dataset: Real-Weather World, Samples: None, Modality: RGB videos + optical flow labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>TrackNetV4: Enhancing Fast Sports Object Tracking with Motion Attention Maps</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.14543"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arjunraj-09/TrackNetv4"><img src="https://img.shields.io/github/stars/arjunraj-09/TrackNetv4.svg?style=social&label=Star"></a><br><a href="https://sites.google.com/view/tracknetv4"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computing, Australian National University<br>
‚Ä¢ Dataset: Challenging Multi-ball Tracking Dataset, Samples: over 23,000 training frames and more than 1,000 testing frames, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>T2M-X: Learning Expressive Text-to-Motion Generation from Partially Annotated Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.13251"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Iowa State University<br>
‚Ä¢ Dataset: T2M-X Dataset, Samples: 61400, Modality: SMPL-X parametric model<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Infrared Small Target Detection in Satellite Videos: A New Dataset and A Novel Recurrent Feature Refinement Framework</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.12448"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XinyiYing/RFR"><img src="https://img.shields.io/github/stars/XinyiYing/RFR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Electronic Science and Technology, National University of Defense Technology<br>
‚Ä¢ Dataset: IRSatVideo-LEO, Samples: 200, Modality: Synthesized short-wave infrared (SWIR) videos with satellite motion and target trajectory data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>GS-Net: Generalizable Plug-and-Play 3D Gaussian Splatting Module</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.11307"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The School of Vehicle and Mobility, Tsinghua University, China<br>
‚Ä¢ Dataset: CARLA-NVS, Samples: 20 scenes (10 dynamic), 100 frames each, Modality: RGB images, Depth images, Semantic segmentation images, LiDAR point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>MotIF: Motion Instruction Fine-tuning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.10683"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motif-1k.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Massachusetts Institute of Technology<br>
‚Ä¢ Dataset: MotIF-1K, Samples: 1022, Modality: RGBD videos, joint states, language annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Dynamic Layer Detection of a Thin Materials using DenseTact Optical Tactile Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.09849"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://armlabstanford.github.io/dynamic-cloth-detection"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ARMLab in the Mechanical Engineering Department, Stanford University<br>
‚Ä¢ Dataset: None, Samples: 568, Modality: RGB videos, 6-axis wrench, joint states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>DreamMover: Leveraging the Prior of Diffusion Models for Image Interpolation with Large Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.09605"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dreamm0ver.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of AIA, Huazhong University of Science and Technology<br>
‚Ä¢ Dataset: InterpBench, Samples: 100, Modality: image pairs with large motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.08494"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/axle-lab/WheelPoser"><img src="https://img.shields.io/github/stars/axle-lab/WheelPoser.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: WheelPoser-IMU, Samples: 167 minutes of motion data, Modality: IMU sensor data + Motion Capture (SMPL)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>USTC-TD: A Test Dataset and Benchmark for Image and Video Coding in 2020s</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.08481"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/esakak/USTC-TD"><img src="https://img.shields.io/github/stars/esakak/USTC-TD.svg?style=social&label=Star"></a><br><a href="https://esakak.github.io/USTC-TD"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: the MOE Key Laboratory of Brain-Inspired Intelligent Perception and Cognition, University of Science and Technology of China, Hefei 230027, China<br>
‚Ä¢ Dataset: USTC-TD, Samples: 10, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>FORS-EMG: A Novel sEMG Dataset for Hand Gesture Recognition Across Multiple Forearm Orientations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.07484"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Engineering, Varendra University, Rajshahi, Bangladesh<br>
‚Ä¢ Dataset: FORS-EMG, Samples: 3420, Modality: sEMG signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>VMAS: Video-to-Music Generation via Semantic Alignment in Web Music Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.07450"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://genjib.github.io/project_page/VMAs/index.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UNC Chapel Hill<br>
‚Ä¢ Dataset: DISCO-MV, Samples: 2200000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>LSE-NeRF: Learning Sensor Modeling Errors for Deblured Neural Radiance Fields with RGB-Event Stereo</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.06104"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ubc-vision/LSENeRF"><img src="https://img.shields.io/github/stars/ubc-vision/LSENeRF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of British Columbia<br>
‚Ä¢ Dataset: LSE-NeRF Dataset, Samples: 10, Modality: blurry RGB videos + event streams + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>KRONC: Keypoint-based Robust Camera Optimization for 3D Car Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.05407"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Modena and Reggio Emilia<br>
‚Ä¢ Dataset: KRONC-dataset, Samples: 7, Modality: RGB images + camera poses + semantic keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>FacialFlowNet: Advancing Facial Optical Flow Estimation with a Diverse Dataset and a Decomposed Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.05396"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University<br>
‚Ä¢ Dataset: FacialFlowNet, Samples: 105970, Modality: RGB images + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>CD-NGP: A Fast Scalable Continual Representation for Dynamic Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.05166"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Automation, Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: long multi-view video dataset, Samples: 5, Modality: multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>HelmetPoser: A Helmet-Mounted IMU Dataset for Data-Driven Estimation of Human Head Motion in Diverse Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.05006"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lqiutong.github.io/HelmetPoser"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: China-Singapore International Joint Research Institute (CSIJRI) and School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: HelmetPoser, Samples: 30, Modality: IMU + VICON ground truth poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Heterogeneous LiDAR Dataset for Benchmarking Robust Localization in Diverse Degenerate Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.04961"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/PengYu-Team/GEODE_dataset"><img src="https://img.shields.io/github/stars/PengYu-Team/GEODE_dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sun Yat-sen University, The University of Hong Kong<br>
‚Ä¢ Dataset: GEODE, Samples: 64, Modality: LiDAR, Stereo Camera, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Synergy and Synchrony in Couple Dances</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.04440"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://von31.github.io/synNsync"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: 3D Human Couple Dancing Dataset, Samples: 30 hours of footage, Modality: RGB videos + 3D pseudo ground truth motion (SMPL parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>TP-GMOT: Tracking Generic Multiple Object by Textual Prompt with Motion-Appearance Cost (MAC) SORT</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.02490"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fsoft-aic.github.io/TP-GMOT"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: FPT Software AI Center, Vietnam<br>
‚Ä¢ Dataset: Refer-GMOT, Samples: 98, Modality: RGB videos + tracking annotations + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Snapshot: Towards Application-centered Models for Pedestrian Trajectory Prediction in Urban Traffic Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.01971"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/TUMFTM/Snapshot"><img src="https://img.shields.io/github/stars/TUMFTM/Snapshot.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich, Germany<br>
‚Ä¢ Dataset: Pedestrian-focused benchmark from Argoverse 2, Samples: over 1,000,000, Modality: 2D pedestrian trajectories and semantic map polylines<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>3D-LEX v1.0: 3D Lexicons for American Sign Language and Sign Language of the Netherlands</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.01901"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OlineRanum/GLEX_Controller"><img src="https://img.shields.io/github/stars/OlineRanum/GLEX_Controller.svg?style=social&label=Star"></a><br><a href="https://osf.io/g7u9c/?view_only=8090319e12aa4fd991d81e369a1cbd88"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Logic, Language and Computation, University of Amsterdam<br>
‚Ä¢ Dataset: 3D-LEX v1.0, Samples: 2000, Modality: MoCap (optical markers, data gloves), Facial blendshapes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>AMG: Avatar Motion Guided Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.01502"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zshyang/amg"><img src="https://img.shields.io/github/stars/zshyang/amg.svg?style=social&label=Star"></a><br><a href="https://github.com/zshyang/amg"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Arizona State University<br>
‚Ä¢ Dataset: AMG Dataset, Samples: 5788, Modality: Paired real RGB videos, synthetic avatar videos (driven by extracted SMPL poses), and text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2024</td>
  <td style="width:70%;"><strong>Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2409.00904"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Information Engineering, Chang‚Äôan University, Xi‚Äôan, Shaanxi 710018, PR China<br>
‚Ä¢ Dataset: IArgoverse, Samples: 130565, Modality: Vehicle trajectories (2D coordinates)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and Body-Worn IMUs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.17168"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: PICO<br>
‚Ä¢ Dataset: EMHI, Samples: 885, Modality: stereo egocentric images, IMU, SMPL pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Efficient Camera Exposure Control for Visual Odometry via Deep Reinforcement Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.17005"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ShuyangUni/drl_exposure_ctrl"><img src="https://img.shields.io/github/stars/ShuyangUni/drl_exposure_ctrl.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: the Department of Electronic and Computer Engineering, the Hong Kong University of Science and Technology<br>
‚Ä¢ Dataset: Unnamed dataset for DRL-based exposure control, Samples: 3 sequences, Modality: Bracketed RGB images, LiDAR, IMU, pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>3D Pose-Based Temporal Action Segmentation for Figure Skating: A Fine-Grained and Jump Procedure-Aware Annotation Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.16638"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ryota-skating/FS-Jump3D"><img src="https://img.shields.io/github/stars/ryota-skating/FS-Jump3D.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nagoya University<br>
‚Ä¢ Dataset: FS-Jump3D, Samples: 253, Modality: Multi-view RGB videos + 3D poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>ESPARGOS: Phase-Coherent WiFi CSI Datasets for Wireless Sensing Research</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.16377"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Jeija/ESPARGOS-WiFi-ChannelCharting"><img src="https://img.shields.io/github/stars/Jeija/ESPARGOS-WiFi-ChannelCharting.svg?style=social&label=Star"></a><br><a href="https://espargos.net/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Telecommunications, Pfaffenwaldring 47, University of Stuttgart, 70569 Stuttgart, Germany<br>
‚Ä¢ Dataset: espargos-0002, Samples: 569190, Modality: WiFi CSI + robot trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.15511"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Aerospace Information Research Institute, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: SkyAgent-Act3k, Samples: 3000, Modality: Drone action sequences and pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>CMTA: Cross-Modal Temporal Alignment for Event-guided Video Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.14930"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/intelpro/CMTA"><img src="https://img.shields.io/github/stars/intelpro/CMTA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea Advanced Institute of Science and Technology<br>
‚Ä¢ Dataset: EVRB, Samples: 17 sequences (11 training, 6 test), Modality: blurred RGB videos, sharp RGB videos, event data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Towards Real-world Event-guided Low-light Video Enhancement and Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.14916"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/intelpro/ELEDNet"><img src="https://img.shields.io/github/stars/intelpro/ELEDNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea Advanced Institute of Science and Technology<br>
‚Ä¢ Dataset: RELED (Real-world Event-guided Low-light video Enhancement and Deblurring), Samples: 42, Modality: RGB videos + Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Temporally-consistent 3D Reconstruction of Birds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.13629"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/datasets/seabirds/common_murre_temporal"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Linkoping University, Sweden<br>
‚Ä¢ Dataset: common_murre_temporal, Samples: 10000 video frames; 100 test frames with 2D keypoint labels, Modality: RGB videos + 2D keypoint labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>N-DriverMotion: Driver motion learning and prediction using an event-based camera and directly trained spiking neural networks on Loihi 2</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.13379"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Engineering and Applied Sciences, Stony Brook University<br>
‚Ä¢ Dataset: N-DriverMotion, Samples: 1239, Modality: event-based camera streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>SynPlay: Importing Real-world Diversity for a Synthetic Human Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.11814"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://synplaydataset.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland College Park<br>
‚Ä¢ Dataset: SynPlay, Samples: 257, Modality: MoCap sequences rendered as RGB images with 2D/3D bounding boxes, instance-level segmentation masks, depth maps, and human keypoint locations.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>MCDubber: Multimodal Context-Aware Expressive Video Dubbing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.11593"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XiaoYuanJun-zy/MCDubber"><img src="https://img.shields.io/github/stars/XiaoYuanJun-zy/MCDubber.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Inner Mongolia University, Hohhot, China<br>
‚Ä¢ Dataset: Context Chem dataset, Samples: 3506, Modality: RGB videos + transcripts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>RaNDT SLAM: Radar SLAM Based on Intensity-Augmented Normal Distributions Transform</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.11576"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/IGMR-RWTH/RaNDT-SLAM"><img src="https://img.shields.io/github/stars/IGMR-RWTH/RaNDT-SLAM.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/record/8199947"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Mechanism Theory, Machine Dynamics and Robotics, RWTH Aachen University, Aachen, Germany<br>
‚Ä¢ Dataset: RaNDT SLAM Dataset / RaNDT Radar Benchmark, Samples: 2, Modality: LiDAR, Radar, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>EmoFace: Emotion-Content Disentangled Speech-Driven 3D Talking Face Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.11518"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: South China University of Technology<br>
‚Ä¢ Dataset: 3D-RAVDESS, Samples: 1440, Modality: 3D face mesh sequences + speech audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>RP1M: A Large-Scale Motion Dataset for Piano Playing with Bi-Manual Dexterous Robot Hands</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.11048"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://rp1m.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Aalto University, Finland<br>
‚Ä¢ Dataset: RP1M (Robot Piano 1 Million), Samples: 1M expert trajectories, Modality: Simulated robot kinematic trajectories (hand states, fingertip positions, joint positions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>DEGAS: Detailed Expressions on Full-Body Gaussian Avatars</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.10588"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://initialneil.github.io/DEGAS"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: DREAMS-Avatar, Samples: 12, Modality: multiview videos and registered SMPL-X<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Event Stream based Sign Language Translation: A High-Definition Benchmark Dataset and A New Algorithm</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.10488"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/OpenESL"><img src="https://img.shields.io/github/stars/Event-AHU/OpenESL.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: Event-CSL, Samples: 14827, Modality: Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>NeRF-US: Removing Ultrasound Imaging Artifacts from Neural Radiance Fields in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.10258"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="rishitdagli.com/nerf-us/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Departments of Computer Science; Medical Imaging, University of Toronto, Canada<br>
‚Ä¢ Dataset: Ultrasound in the Wild, Samples: 10, Modality: Ultrasound videos + probe poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Event Stream based Human Action Recognition: A High-Definition Benchmark Dataset and Algorithms</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.09764"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/CeleX-HAR"><img src="https://img.shields.io/github/stars/Event-AHU/CeleX-HAR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: CeleX-HAR, Samples: 124625, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>ALS-HAR: Harnessing Wearable Ambient Light Sensors to Enhance IMU-based Human Activity Recogntion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.09527"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: German Research Center for Artificial Intelligence, Kaiserslautern, Germany<br>
‚Ä¢ Dataset: Not explicitly named in the paper (referred to as a novel multi-modal HAR dataset), Samples: None, Modality: right wrist IMU signal, right wrist ALS signal, video footage, SMPL pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>OPPH: A Vision-Based Operator for Measuring Body Movements for Personal Healthcare</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.09409"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://groups.inf.ed.ac.uk/vision/DATASETS/HUMOLS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Edinburgh<br>
‚Ä¢ Dataset: HuMoLs, Samples: 67, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>EEPPR: Event-based Estimation of Periodic Phenomena Rate using Correlation in 3D</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.06899"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic<br>
‚Ä¢ Dataset: Unnamed in the paper, Samples: 12, Modality: Event stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>ViMo: Generating Motions from Casual Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.06614"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Chinese University of Hong Kong (Shenzhen)<br>
‚Ä¢ Dataset: Chinese classic dancing dataset, Samples: 750, Modality: 3D joint rotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>DEEPTalk: Dynamic Emotion Embedding for Probabilistic Speech-Driven 3D Face Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.06010"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://whwjdqls.github.io/deeptalk website/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Yonsei University<br>
‚Ä¢ Dataset: Emo-Vox, Samples: 1.3 hours of test data, Modality: Curated emotional audio clips from VoxCeleb2<br>
‚Ä¢ Dataset: 3D-MEAD, Samples: 36 hours, Modality: Pseudo-3D FLAME parameters from MEAD video dataset<br>
‚Ä¢ Dataset: 3D-CREMA-D, Samples: 5.23 hours, Modality: Pseudo-3D FLAME parameters from CREMA-D video dataset<br>
‚Ä¢ Dataset: 3D-RAVDESS, Samples: 1.5 hours, Modality: Pseudo-3D FLAME parameters from RAVDESS video dataset<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>FADE: A Dataset for Detecting Falling Objects around Buildings in Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.05750"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Zhengbo-Zhang/FADE"><img src="https://img.shields.io/github/stars/Zhengbo-Zhang/FADE.svg?style=social&label=Star"></a><br><a href="https://fadedataset.github.io/FADE.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan 430079, China<br>
‚Ä¢ Dataset: FADE, Samples: 1881, Modality: RGB and grayscale videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>CryoBench: Diverse and challenging datasets for the heterogeneity problem in cryo-EM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.05526"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ml-struct-bio/CryoBench"><img src="https://img.shields.io/github/stars/ml-struct-bio/CryoBench.svg?style=social&label=Star"></a><br><a href="https://cryobench.cs.princeton.edu"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Princeton University<br>
‚Ä¢ Dataset: IgG-1D, Samples: 100, Modality: Conformational states (atomic models) from a simulated 1D circular motion, and derived synthetic cryo-EM images.<br>
‚Ä¢ Dataset: IgG-RL, Samples: 100, Modality: Conformational states (atomic models) from sampling a flexible peptide linker, and derived synthetic cryo-EM images.<br>
‚Ä¢ Dataset: Spike-MD, Samples: 46789, Modality: Conformational states (atomic models) sampled from a molecular dynamics simulation, and derived synthetic cryo-EM images.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>GesturePrint: Enabling User Identification for mmWave-based Gesture Recognition Systems</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.05358"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: GesturePrint, Samples: 9332, Modality: mmWave radar point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.04631"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vgg-puppetmaster.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Geometry Group, University of Oxford<br>
‚Ä¢ Dataset: Objaverse-Animation, Samples: 16000, Modality: RGB videos + 2D motion trajectories from 3D animations<br>
‚Ä¢ Dataset: Objaverse-Animation-HQ, Samples: 10000, Modality: RGB videos + 2D motion trajectories from 3D animations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Incorporating Spatial Awareness in Data-Driven Gesture Generation for Virtual Agents</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.04127"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://huggingface.co/spaces/annadeichler/spatial-gesture"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: Synthetic Spatially-Aware Gesture Dataset, Samples: 1160, Modality: MoCap joints, speech audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>PLANRL: A Motion Planning and Imitation Learning Framework to Bootstrap Reinforcement Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.04054"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: PLANRL Real-World Demonstrations, Samples: 30, Modality: Teleoperated robot trajectories, RGB videos<br>
‚Ä¢ Dataset: PLANRL ModeNet/NavNet Training Data, Samples: 7700, Modality: RGB images with mode and waypoint labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Line-based 6-DoF Object Pose Estimation and Tracking With an Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.03225"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Zibin6/LOPET"><img src="https://img.shields.io/github/stars/Zibin6/LOPET.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Aerospace Science and Engineering, National University of Defense Technology, Changsha 410073, China<br>
‚Ä¢ Dataset: event-based moving object dataset, Samples: 9, Modality: Event streams + 3D pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Compositional Physical Reasoning of Objects and Events from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.02687"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zfchenZFC/ComPhy-PCR"><img src="https://img.shields.io/github/stars/zfchenZFC/ComPhy-PCR.svg?style=social&label=Star"></a><br><a href="https://physicalconceptreasoner.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MIT-IBM Watson AI lab<br>
‚Ä¢ Dataset: ComPhy (Synthetic), Samples: 12000, Modality: Synthetic RGB videos<br>
‚Ä¢ Dataset: ComPhy (Real-world), Samples: 492, Modality: Real-world RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.00762"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/X-niper/UniTalker"><img src="https://img.shields.io/github/stars/X-niper/UniTalker.svg?style=social&label=Star"></a><br><a href="https://github.com/X-niper/UniTalker"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SenseTime Research, China<br>
‚Ä¢ Dataset: A2F-Bench, Samples: 8654, Modality: Audio + 3D facial motion (Vertices, FLAME parameters, ARKit blendshape weights)<br>
‚Ä¢ Dataset: Ours(Faceforensics++), Samples: 1714, Modality: Audio + FLAME parameters<br>
‚Ä¢ Dataset: Ours(Speech), Samples: 789, Modality: Audio + ARKit blendshape weights<br>
‚Ä¢ Dataset: Ours(Song), Samples: 1349, Modality: Audio + ARKit blendshape weights<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>MotionFix: Text-Driven 3D Human Motion Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.00712"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motionfix.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, Germany<br>
‚Ä¢ Dataset: MotionFix, Samples: 6730, Modality: MoCap SMPL parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.00297"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nju-3dv.github.io/projects/EmoTalk3D"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory for Novel Software Technology, Nanjing University, China<br>
‚Ä¢ Dataset: EmoTalk3D, Samples: 600, Modality: Multi-view RGB videos, audio, emotion annotations, per-frame 3D facial shapes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2024</td>
  <td style="width:70%;"><strong>Head360: Learning a Parametric 3D Full-Head for Free-View Synthesis in 360¬∞</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2408.00296"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nju-3dv.github.io/projects/Head360"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory for Novel Software Technology, Nanjing University, China<br>
‚Ä¢ Dataset: SynHead100, Samples: 5200, Modality: 3D mesh models<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>PEAR: Phrase-Based Hand-Object Interaction Anticipation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.21510"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: EGO-HOIP, Samples: 5000, Modality: RGB images + text phrases + 3D hand trajectories + 3D hand poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Benchmarking Multi-dimensional AIGC Video Quality Assessment: A Dataset and Unified Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.21408"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zczhang-sjtu/UGVQ.git"><img src="https://img.shields.io/github/stars/zczhang-sjtu/UGVQ.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University, China<br>
‚Ä¢ Dataset: Large-scale Generated Video Quality assessment (LGVQ), Samples: 2808, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>MotionCraft: Crafting Whole-Body Motion with Plug-and-Play Multimodal Controls</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.21136"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cure-lab.github.io/MotionCraft"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: MC-Bench, Samples: None, Modality: SMPL-X motion data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>A Dataset for Multi-intensity Continuous Human Activity Recognition through Passive Sensing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.21125"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arghasen10/mmdoppler"><img src="https://img.shields.io/github/stars/arghasen10/mmdoppler.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IIT Kharagpur, India<br>
‚Ä¢ Dataset: mmDoppler, Samples: 75000, Modality: mmWave point cloud, mmWave range-doppler heatmaps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Restoring Real-World Degraded Events Improves Deblurring Quality</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.20502"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yeeesir/DVS_RDNet"><img src="https://img.shields.io/github/stars/Yeeesir/DVS_RDNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Megvii<br>
‚Ä¢ Dataset: DavisMCR, Samples: 100 sequences (over 16,000 pairs of images and events), Modality: blurry/sharp images + events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>XS-VID: An Extremely Small Video Object Detection Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.18137"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gjhhust.github.io/XS-VID/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huazhong University of Science and Technology<br>
‚Ä¢ Dataset: XS-VID, Samples: 38, Modality: RGB videos + bounding box trajectories + motion attributes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.17438"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zhenzhiwang/HumanVid"><img src="https://img.shields.io/github/stars/zhenzhiwang/HumanVid.svg?style=social&label=Star"></a><br><a href="https://humanvid.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: HumanVid (Real), Samples: 20000, Modality: RGB videos + estimated human pose + estimated camera pose<br>
‚Ä¢ Dataset: HumanVid (Synthetic SMPL-X), Samples: 50000, Modality: Rendered videos + ground truth human pose (SMPL-X) + ground truth camera pose<br>
‚Ä¢ Dataset: HumanVid (Synthetic Anime), Samples: 25000, Modality: Rendered videos + ground truth human pose + ground truth camera pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Deep Learning Assisted Inertial Dead Reckoning and Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.16387"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hatter Department of Marine Technologies, Charney School of Marine Sciences, University of Haifa, Israel<br>
‚Ä¢ Dataset: Mobile Robot and Quadrotor Inertial Dataset, Samples: 49, Modality: IMU (accelerometer, gyroscope), RTK-GNSS (ground truth)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Motion Capture from Inertial and Vision Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.16341"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of science and technology of China, Hefei, China<br>
‚Ä¢ Dataset: MINIONS, Samples: 5500000, Modality: IMU signals, RGB videos, 3D SMPL Mesh, 2D/3D joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>SAFNet: Selective Alignment Fusion Network for Efficient HDR Imaging</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.16308"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ltkong218/SAFNet"><img src="https://img.shields.io/github/stars/ltkong218/SAFNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: vivo Mobile Communication Co., Ltd, China<br>
‚Ä¢ Dataset: Challenge123, Samples: 123, Modality: multi-exposure LDR image sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Cluster Haptic Texture Database: Haptic Texture Database with Varied Velocity-Direction Sliding Contacts</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.16206"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cluster-lab/Cluster-Haptic-Texture-Database"><img src="https://img.shields.io/github/stars/cluster-lab/Cluster-Haptic-Texture-Database.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.6084/m9.figshare.29438288"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Cluster Metaverse Lab, 8-9-5 Nishigotanda, Shinagawa, Tokyo, Japan; Graduate School of Comprehensive Human Sciences, University of Tsukuba, 1-2 Kasuga, Tsukuba, Ibaraki, Japan<br>
‚Ä¢ Dataset: Cluster Haptic Texture Database, Samples: 18880, Modality: position (X, Y trajectories), audio, acceleration, force, images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.15708"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/bupt-ai-cz/SwinSF"><img src="https://img.shields.io/github/stars/bupt-ai-cz/SwinSF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beijing University of Posts and Telecommunications, China<br>
‚Ä¢ Dataset: spike-X4K, Samples: 1245, Modality: spike streams + ground truth images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Hierarchical Windowed Graph Attention Network and a Large Scale Dataset for Isolated Indian Sign Language Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.14224"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cs.rkmvu.ac.in/~isl"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: RKMVERI<br>
‚Ä¢ Dataset: FDMSE-ISL, Samples: 40033, Modality: RGB videos + depth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>PointRegGPT: Boosting 3D Point Cloud Registration using Generative Point-Cloud Pairs for Training</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.14054"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Chen-Suyi/PointRegGPT"><img src="https://img.shields.io/github/stars/Chen-Suyi/PointRegGPT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: PointRegGPT, Samples: 160000, Modality: Point cloud pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>The Effects of Selected Object Features on a Pick-and-Place Task: a Human Multimodal Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.13425"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lindalastrico/objectsManipulationDataset"><img src="https://img.shields.io/github/stars/lindalastrico/objectsManipulationDataset.svg?style=social&label=Star"></a><br><a href="https://www.kaggle.com/dataset/cec218d6597e7c2cac28c7d6a1e8cbd381e451a77192c16b648d2b4c5de70697"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Engine Room, Department of Informatics, Bioengineering, Robotics, and Systems Engineering (DIBRIS), University of Genoa, Italy; Cognitive Architecture for Collaborative Technologies Unit (CONTACT), Italian Institute of Technology, Italy<br>
‚Ä¢ Dataset: The Effects of Selected Object Features on a Pick-and-Place Task: a Human Multimodal Dataset, Samples: 1200, Modality: MoCap markers, IMU, RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>EvSign: Sign Language Recognition and Translation with Streaming Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.12593"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zhang-pengyu/EVSign"><img src="https://img.shields.io/github/stars/zhang-pengyu/EVSign.svg?style=social&label=Star"></a><br><a href="https://zhang-pengyu.github.io/EVSign"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dalian University of Technology<br>
‚Ä¢ Dataset: EvSign, Samples: 6773, Modality: Event streams + RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>SNAIL Radar: A large-scale diverse benchmark for evaluating 4D-radar-based SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.11705"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/snail-radar/dataset_tools"><img src="https://img.shields.io/github/stars/snail-radar/dataset_tools.svg?style=social&label=Star"></a><br><a href="https://snail-radar.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The state key Lab of Information Engineering in Surveying, Mapping And Remote Sensing (LIESMARS), Wuhan University, Hubei, China 430079<br>
‚Ä¢ Dataset: SNAIL Radar, Samples: 44, Modality: 4D Radar, 3D Lidar, Stereo Camera, IMU, GNSS/INS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Animate3D: Animating Any 3D Model with Multi-view Video Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.11398"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://animate3d.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CASIA<br>
‚Ä¢ Dataset: MV-Video, Samples: 115566, Modality: multi-view videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Towards High-Quality 3D Motion Transfer with Realistic Apparel Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.11266"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rongakowang/MMDMC"><img src="https://img.shields.io/github/stars/rongakowang/MMDMC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Australian National University<br>
‚Ä¢ Dataset: MMDMC, Samples: 120, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>High-Quality and Full Bandwidth Seismic Signal Synthesis using Operational GANs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.11040"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OzerCanDevecioglu/High-Quality-and-High-Bandwidth-Seismic-Signal-Synhesis-using-Operational-GANs"><img src="https://img.shields.io/github/stars/OzerCanDevecioglu/High-Quality-and-High-Bandwidth-Seismic-Signal-Synhesis-using-Operational-GANs.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computing Sciences, Tampere University, Tampere, Finland<br>
‚Ä¢ Dataset: SimGM Dataset, Samples: 201, Modality: 1D seismic acceleration signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Motion-prior Contrast Maximization for Dense Continuous-Time Motion Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.10802"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tub-rip/MotionPriorCMax"><img src="https://img.shields.io/github/stars/tub-rip/MotionPriorCMax.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Berlin and SCIoI Excellence Cluster, Berlin, Germany<br>
‚Ä¢ Dataset: EVIMO2 Continuous Flow Dataset, Samples: None, Modality: Event camera data + dense optical flow ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Kinetic Typography Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.10476"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://seonmip.github.io/kinety"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AI Graduate School, GIST, South Korea<br>
‚Ä¢ Dataset: KineTy dataset, Samples: 600000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>SpikeGS: 3D Gaussian Splatting from Spike Streams with High-Speed Camera Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.10062"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: SpikeGS Synthetic Dataset, Samples: 7, Modality: Spike stream + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>LiveHPS++: Robust and Coherent Motion Capture in Dynamic Free Environment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.09833"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dvlab.github.io/project_page/LiveHPS2.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: NoiseMotion, Samples: 1,021,802 human motions, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>IoT-LM: Large Multisensory Language Models for the Internet of Things</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.09801"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Multi-IoT/MultiIoT"><img src="https://img.shields.io/github/stars/Multi-IoT/MultiIoT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: MULTI IOT, Samples: 1150000, Modality: Inertial Measurement Units (IMU), thermal sensors, GPS, LiDAR, gaze, pose, capacitance sensors, images, audio, video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Apprenticeship-Inspired Elegance: Synergistic Knowledge Distillation Empowers Spiking Neural Networks for Efficient Single-Eye Emotion Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.09521"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Key Laboratory of Social Computing and Cognitive Intelligence, Dalian University of Technology<br>
‚Ä¢ Dataset: Diverse Single-eye Event-based Emotion (DSEE), Samples: 6235, Modality: intensity video frames + event data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>AirSketch: Generative Motion to Sketch</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.08906"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hxgr4ce/DoodleFusion"><img src="https://img.shields.io/github/stars/hxgr4ce/DoodleFusion.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Central Florida<br>
‚Ä¢ Dataset: Synthetic Air-Drawing Dataset, Samples: 5000, Modality: RGB videos<br>
‚Ä¢ Dataset: Real Air-Drawing Dataset, Samples: 500, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Robotic Control via Embodied Chain-of-Thought Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.08693"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/m-zawalski/embodied-cot"><img src="https://img.shields.io/github/stars/m-zawalski/embodied-cot.svg?style=social&label=Star"></a><br><a href="https://embodied-cot.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley, University of Warsaw<br>
‚Ä¢ Dataset: Embodied Chain-of-Thought (ECoT) dataset, Samples: more than 2.5M transitions, Modality: Synthetic textual reasoning chains (plans, sub-tasks, bounding boxes, gripper positions, etc.) annotating robot trajectories and image observations from the Bridge v2 dataset.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Infinite Motion: Extended Motion Generation via Long Text Instructions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.08443"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai University, Fudan University<br>
‚Ä¢ Dataset: HumanML3D-Extend, Samples: 35000, Modality: 3D human motion sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Long-range Turbulence Mitigation: A Large-scale Dataset and A Coarse-to-fine Framework</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.08377"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://shengqi77.github.io/RLR-AT.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Lab of Multispectral Information Intelligent Processing Technology, Huazhong University of Science and Technology, China<br>
‚Ä¢ Dataset: RLR-AT, Samples: 1500, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.06358"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mira-space/MiraData"><img src="https://img.shields.io/github/stars/mira-space/MiraData.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ARC Lab, Tencent PCG; The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: MiraData, Samples: 788000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>TAPVid-3D: A Benchmark for Tracking Any Point in 3D</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.05921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/google-deepmind/tapnet/tree/main/tapvid3d"><img src="https://img.shields.io/github/stars/main/tapvid3d.svg?style=social&label=Star"></a><br><a href="https://tapvid3d.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google DeepMind<br>
‚Ä¢ Dataset: TAPVid-3D, Samples: 4569, Modality: RGB videos + 3D point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>WOMD-Reasoning: A Large-Scale Dataset for Interaction Reasoning in Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.04281"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yhli123/WOMD-Reasoning"><img src="https://img.shields.io/github/stars/yhli123/WOMD-Reasoning.svg?style=social&label=Star"></a><br><a href="https://waymo.com/open/download/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: WOMD-Reasoning, Samples: 63000, Modality: Motion trajectories, Textual Q&A, rendered BEV and front-view videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>LiDAR-based Real-Time Object Detection and Tracking in Dynamic Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.04115"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MISTLab/lidar dynamic objects detection.git"><img src="https://img.shields.io/github/stars/MISTLab/lidar dynamic objects detection.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Polytechnique Montreal<br>
‚Ä¢ Dataset: lidar dynamic objects detection, Samples: 4, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>BVI-RLV: A Fully Registered Dataset and Benchmarks for Low-Light Video Enhancement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.03535"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.21227/mzny-8c77"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Information Laboratory, Bristol Vision Institute (BVI), University of Bristol, Bristol, UK BS1 5DD<br>
‚Ä¢ Dataset: BVI-RLV, Samples: 40, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Single Image Rolling Shutter Removal with Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.02906"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lhaippp/RS-Diffusion"><img src="https://img.shields.io/github/stars/lhaippp/RS-Diffusion.svg?style=social&label=Star"></a><br><a href="https://huggingface.co/Lhaippp/RS-Diffusion"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China, Megvii Technology<br>
‚Ä¢ Dataset: RS-Real, Samples: 41000, Modality: Rolling shutter images, Global shutter images, motion fields (from IMU data)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Aligning Human Motion Generation with Human Perceptions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.02272"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MotionCritic/MotionCritic"><img src="https://img.shields.io/github/stars/MotionCritic/MotionCritic.svg?style=social&label=Star"></a><br><a href="https://motioncritic.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center on Frontiers of Computing Studies, School of Compter Science, Peking University<br>
‚Ä¢ Dataset: MotionPercept, Samples: 73040, Modality: SMPL parameters with human preference annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event Stream</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.02174"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wu-cvgl/BeNeRF"><img src="https://img.shields.io/github/stars/wu-cvgl/BeNeRF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Westlake University<br>
‚Ä¢ Dataset: BeNeRF Synthetic Dataset (livingroom, whiteroom, pinkcastle), Samples: 60, Modality: blurry image + event stream<br>
‚Ä¢ Dataset: BeNeRF Synthetic Dataset (tanabata, outdoorpool), Samples: 40, Modality: blurry image + event stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>Small Aerial Target Detection for Airborne Infrared Detection Systems using LightGBM and Trajectory Constraints</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.01278"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://small-infrared-aerial-target-detection.grand-challenge.org/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Aerospace Science and Engineering, National University of Defense Technology<br>
‚Ä¢ Dataset: SIATD, Samples: 350, Modality: infrared image sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>CLHOP: Combined Audio-Video Learning for Horse 3D Pose and Shape Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.01244"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH, Sweden<br>
‚Ä¢ Dataset: Outdoor Dataset, Samples: 1604.54 seconds of video, Modality: 4K RGB videos with synchronized audio, 2D keypoints, segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>DaBiT: Depth and Blur informed Transformer for Video Focal Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.01230"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/crispianm/DaBiT"><img src="https://img.shields.io/github/stars/crispianm/DaBiT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, University of Bristol, Bristol, UK<br>
‚Ä¢ Dataset: DAVIS-Blur, Samples: 50, Modality: RGB videos + blur maps + depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>A Medical Low-Back Pain Physical Rehabilitation Dataset for Human Body Movement Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.00521"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nguyensmai/KeraalDataset"><img src="https://img.shields.io/github/stars/nguyensmai/KeraalDataset.svg?style=social&label=Star"></a><br><a href="http://nguyensmai.free.fr/KeraalDataset.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IMT Atlantique<br>
‚Ä¢ Dataset: Keraal, Samples: 2622, Modality: RGB videos, Kinect 3D skeleton (positions, orientations), 2D skeleton, Vicon MoCap joints, medical annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2024</td>
  <td style="width:70%;"><strong>SCOPE: Stochastic Cartographic Occupancy Prediction Engine for Uncertainty-Aware Dynamic Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2407.00144"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/TempleRAIL/scope"><img src="https://img.shields.io/github/stars/TempleRAIL/scope.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.7051560"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering, Temple University<br>
‚Ä¢ Dataset: OGM-Turtlebot2, Samples: 94891, Modality: robot states (pose, velocity), LiDAR measurements<br>
‚Ä¢ Dataset: OGM-Jackal, Samples: None, Modality: robot states (pose, velocity), LiDAR measurements<br>
‚Ä¢ Dataset: OGM-Spot, Samples: None, Modality: robot states (pose, velocity), LiDAR measurements<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.19353"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://core4d.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: CORE4D-Real, Samples: 1000, Modality: MoCap (human mesh, object pose), allocentric RGB-D videos, egocentric RGB videos, 2D segmentations<br>
‚Ä¢ Dataset: CORE4D-Synthetic, Samples: 10000, Modality: human mesh, object pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>AddBiomechanics Dataset: Capturing the Physics of Human Motion at Scale</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.18537"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="addbiomechanics.org/download_data.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: AddBiomechanics Dataset 1.0, Samples: 24 million+ frames, Modality: Optical marker locations, ground reaction forces/moments, joint kinematics, joint torques<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Human-Aware 3D Scene Generation with Spatially-constrained Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.18159"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Hong-xl/SHADE"><img src="https://img.shields.io/github/stars/Hong-xl/SHADE.svg?style=social&label=Star"></a><br><a href="https://hong-xl.github.io/SHADE"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Wuhan University<br>
‚Ä¢ Dataset: Calibrated 3D FRONT HUMAN, Samples: 11225, Modality: 3D human motion sequences within 3D scenes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Speech2UnifiedExpressions: Synchronous Synthesis of Co-Speech Affective Face and Body Expressions from Affordable Inputs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.18068"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UttaranB127/speech2unified_expressions"><img src="https://img.shields.io/github/stars/UttaranB127/speech2unified_expressions.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Adobe Inc.<br>
‚Ä¢ Dataset: TED Gesture+Face Dataset, Samples: 253186, Modality: 3D face landmarks, 3D body pose joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>PVUW 2024 Challenge on Complex Video Understanding: Methods and Results</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.17005"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://henghuiding.github.io/MOSE/ChallengeCVPR2024"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CVPR 2024 PVUW Workshop & Challenge organizers<br>
‚Ä¢ Dataset: MOSE, Samples: None, Modality: RGB videos + segmentation masks<br>
‚Ä¢ Dataset: MeViS, Samples: None, Modality: RGB videos + segmentation masks + text expressions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>From Perfect to Noisy World Simulation: Customizable Embodied Multi-modal Perturbations for SLAM Robustness Benchmarking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.16850"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Xiaohao-Xu/SLAM-under-Perturbation"><img src="https://img.shields.io/github/stars/Xiaohao-Xu/SLAM-under-Perturbation.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Michigan, Ann Arbor<br>
‚Ä¢ Dataset: Noisy-Replica, Samples: 1000, Modality: RGB-D video sequences + 6D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Rendering and Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.16038"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://livescenes.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: OmniSim, Samples: 20, Modality: RGBD images, camera poses, object masks, interaction variables, text prompts<br>
‚Ä¢ Dataset: InterReal, Samples: 8, Modality: RGB videos, camera poses, object masks, interaction variables, text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Multimodal Segmentation for Vocal Tract Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.15754"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rishiraij/multimodal-mri-avatar"><img src="https://img.shields.io/github/stars/rishiraij/multimodal-mri-avatar.svg?style=social&label=Star"></a><br><a href="https://rishiraij.github.io/multimodal-mri-avatar/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Berkeley<br>
‚Ä¢ Dataset: Speech MRI Open Dataset (labels), Samples: 75 speakers, Modality: RT-MRI video + 95 articulator point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and Smartphone Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.14498"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/BASHLab/LLaSA"><img src="https://img.shields.io/github/stars/BASHLab/LLaSA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Worcester Polytechnic Institute<br>
‚Ä¢ Dataset: SensorCaps, Samples: 35960, Modality: IMU sequences (accelerometer, gyroscope) + natural language captions<br>
‚Ä¢ Dataset: OpenSQA, Samples: 179727, Modality: IMU sequences (accelerometer, gyroscope) + question-answer pairs<br>
‚Ä¢ Dataset: Tune-OpenSQA, Samples: 19440, Modality: IMU sequences (accelerometer, gyroscope) + question-answer pairs<br>
‚Ä¢ Dataset: User-collected smartphone IMU dataset, Samples: None, Modality: IMU data (accelerometer, gyroscope)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Video Generation with Learned Action Prior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.14436"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://meenakshisarkar.github.io/Motion-Prediction-and-Planning/dataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Indian Institute of Science<br>
‚Ä¢ Dataset: RoAM, Samples: 307200, Modality: Stereo RGB videos + robot actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Benchmarking Monocular 3D Dog Pose Estimation Using In-The-Wild Motion Capture Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.14412"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford UK<br>
‚Ä¢ Dataset: 3DDogs-Lab, Samples: 143, Modality: MoCap joints + RGBD videos + IMUs + pressure mat<br>
‚Ä¢ Dataset: 3DDogs-Wild, Samples: 286, Modality: RGB videos + 3D poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>ViDSOD-100: A New Dataset and a Baseline Model for RGB-D Video Salient Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.12536"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: ViDSOD-100, Samples: 100, Modality: RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Holistic-Motion2D: Scalable Whole-body Human Motion Generation in 2D Space</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.11253"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://holistic-motion2d.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: Holistic-Motion2D, Samples: 1002463, Modality: 2D whole-body keypoints + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Nymeria: A Massive Collection of Multimodal Egocentric Daily Motion in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.09905"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.projectaria.com/datasets/nymeria"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta Reality Labs Research<br>
‚Ä¢ Dataset: Nymeria, Samples: 1200, Modality: Full-body motion (IMU suit), egocentric multimodal data (RGB/grayscale/eye-tracking videos, IMUs, magnetometer, barometer), third-person video, 3D scene point clouds, language descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>LAVIB: A Large-scale Video Interpolation Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.09754"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/alexandrosstergiou/LAVIB"><img src="https://img.shields.io/github/stars/alexandrosstergiou/LAVIB.svg?style=social&label=Star"></a><br><a href="https://alexandrosstergiou.github.io/datasets/LAVIB"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Twente, NL<br>
‚Ä¢ Dataset: LA VIB, Samples: 283484, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Introducing HOT3D: An Egocentric Dataset for 3D Hand and Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.09598"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://facebookresearch.github.io/hot3d"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta Reality Labs<br>
‚Ä¢ Dataset: HOT3D, Samples: 425, Modality: Multi-view RGB/monochrome videos, MoCap-based 3D hand/object/camera poses, 3D mesh models, scene point clouds, eye gaze<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.09326"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/agnJason/PianoMotion10M"><img src="https://img.shields.io/github/stars/agnJason/PianoMotion10M.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: PianoMotion10M, Samples: 10527167, Modality: RGB videos + 3D MANO hand poses + audio + MIDI<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.08858"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://omni.human2humanoid.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: OmniH2O-6, Samples: 6 tasks (40 minutes of demonstrations), Modality: RGBD images, motion goals (head and hands), joint targets<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Skim then Focus: Integrating Contextual and Fine-grained Views for Repetitive Action Counting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.08814"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huazhong University of Science and Technology, Wuhan, China.<br>
‚Ä¢ Dataset: Multi-RepCount, Samples: 984, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Vivid-ZOO: Multi-View Video Generation with Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.08659"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vividzoo.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: King Abdullah University of Science and Technology<br>
‚Ä¢ Dataset: MV-VideoNet, Samples: 14271, Modality: multi-view RGB videos + camera poses + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>From Sim-to-Real: Toward General Event-based Low-light Frame Interpolation with Per-scene Optimization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.08090"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://openimaginglab.github.io/Sim2Real/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University, China and Shanghai AI Laboratory, China<br>
‚Ä¢ Dataset: EVFI-LL, Samples: >20 sequences, Modality: RGB videos + Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Triple-domain Feature Learning with Frequency-aware Memory Enhancement for Moving Infrared Small Target Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.06949"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UESTC-nnLab/Tridos"><img src="https://img.shields.io/github/stars/UESTC-nnLab/Tridos.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China<br>
‚Ä¢ Dataset: ITSDT-15K, Samples: 60, Modality: infrared videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>MOSA: Music Motion with Semantic Annotation Dataset for Cross-Modal Music Processing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.06375"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset"><img src="https://img.shields.io/github/stars/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Science, Academia Sinica, Taiwan.<br>
‚Ä¢ Dataset: MOSA (Music mOtion with Semantic Annotation), Samples: 742, Modality: 3-D motion capture data, audio recordings, semantic annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>iMotion-LLM: Motion Prediction Instruction Tuning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.06211"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAUST<br>
‚Ä¢ Dataset: InstructWaymo, Samples: 327391, Modality: Vectorized motion/map data + Textual instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Diving Deep into the Motion Representation of Video-Text Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.05075"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chinmayad/motiondescriptions.git"><img src="https://img.shields.io/github/stars/chinmayad/motiondescriptions.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: GPT-4 Generated Motion Descriptions, Samples: 552, Modality: textual motion descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>SMART: Scene-motion-aware human action recognition framework for mental disorder group</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.04649"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Inowlzy/SMART.git"><img src="https://img.shields.io/github/stars/Inowlzy/SMART.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Key Laboratory of Navigation and Location-based Services, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai 200240, China<br>
‚Ä¢ Dataset: MentalHAD, Samples: 69, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>LipGER: Visually-Conditioned Generative Error Correction for Robust Automatic Speech Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.04432"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Sreyan88/LipGER"><img src="https://img.shields.io/github/stars/Sreyan88/LipGER.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park, USA<br>
‚Ä¢ Dataset: LipHyp, Samples: 601000, Modality: videos of lip motion + ASR hypothesis lists + text transcriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Pi-fusion: Physics-informed diffusion model for learning fluid dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.03711"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SIAT-SIH/fluid"><img src="https://img.shields.io/github/stars/SIAT-SIH/fluid.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: 2D flow past a circular cylinder, Samples: 1, Modality: Fluid dynamics simulation (velocity and pressure fields)<br>
‚Ä¢ Dataset: 3D blood flow in realistic hepatic portal vein, Samples: 1, Modality: Fluid dynamics simulation (velocity and pressure fields)<br>
‚Ä¢ Dataset: 3D blood flow in brain artery, Samples: 1, Modality: Fluid dynamics simulation (velocity and pressure fields)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Event3DGS: Event-Based 3D Gaussian Splatting for High-Speed Robot Egomotion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.02972"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/txiong23/Event3DGS"><img src="https://img.shields.io/github/stars/txiong23/Event3DGS.svg?style=social&label=Star"></a><br><a href="https://txiong23.github.io/Event3DGS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: Event3DGS real-world scenes, Samples: 6, Modality: RGB videos + camera poses + emulated event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>DeNVeR: Deformable Neural Vessel Representations for Unsupervised Video Vessel Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.01591"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="kirito878.github.io/DeNVeR"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Yang Ming Chiao Tung University<br>
‚Ä¢ Dataset: XACV, Samples: 111, Modality: X-ray angiography videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2024</td>
  <td style="width:70%;"><strong>Virtual avatar generation models as world navigators</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2406.01056"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://virtual-avatar-generation.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SABR<br>
‚Ä¢ Dataset: NAV-22M, Samples: 22000000, Modality: RGB videos + 3D pose tracks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>MotionLLM: Understanding Human Behaviors from Human Motions and Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.20340"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lhchen.top/MotionLLM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: MoVid, Samples: 496000, Modality: SMPL sequences, RGB videos, Text (captions and QA pairs)<br>
‚Ä¢ Dataset: MoVid-Bench, Samples: 1350, Modality: SMPL sequences, RGB videos, Text (QA pairs)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.20336"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vis-www.cs.umass.edu/RapVerse"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Massachusetts Amherst<br>
‚Ä¢ Dataset: RapVerse, Samples: 26.8 hours, Modality: 3D holistic body meshes (SMPL-X), vocals, lyrics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>SMPLX-Lite: A Realistic and Drivable Avatar Benchmark with Rich Geometry and Texture Annotations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.19609"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen International Graduate School, Tsinghua University<br>
‚Ä¢ Dataset: SMPLX-Lite, Samples: over 20k frames, Modality: multi-view RGB videos, 3D keypoints, 3D textured scanned meshes, SMPLX-Lite-D parametric models<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Towards Open Domain Text-Driven Synthesis of Multi-Person Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.18483"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Washington<br>
‚Ä¢ Dataset: LAION-Pose, Samples: 8000000, Modality: 3D Poses (SMPL) + Text<br>
‚Ä¢ Dataset: WebVid-Motion, Samples: 3500, Modality: 3D Motion Sequences (SMPL) + Text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Human4DiT: 360-degree Human Video Generation with 4D Diffusion Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.17405"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: Human4DiT-3D, Samples: 5000, Modality: 3D human scans + SMPL<br>
‚Ä¢ Dataset: Human4DiT-Video, Samples: 10000, Modality: RGB videos + SMPL<br>
‚Ä¢ Dataset: Human4DiT-4D, Samples: 100, Modality: Animatable 3D human models + SMPL<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Hawk: Learning to Understand Open-World Video Anomalies</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16886"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jqtangust/hawk"><img src="https://img.shields.io/github/stars/jqtangust/hawk.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: HAWK Video Anomaly Dataset, Samples: 8000, Modality: RGB videos + textual descriptions + QA pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16868"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ispc-lab/RCDN"><img src="https://img.shields.io/github/stars/ispc-lab/RCDN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tongji University<br>
‚Ä¢ Dataset: OPV2V-N, Samples: 6138, Modality: Multi-agent RGB videos + optical flow + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Diffusion4D: Fast Spatial-temporal Consistent 4D Generation via Video Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16645"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VITA-Group/Diffusion4D"><img src="https://img.shields.io/github/stars/VITA-Group/Diffusion4D.svg?style=social&label=Star"></a><br><a href="https://vita-group.github.io/Diffusion4D"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto<br>
‚Ä¢ Dataset: Curated dynamic 3D asset dataset from Objaverse, Samples: 54000, Modality: animated 3D models<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>MCGMapper: Light-Weight Incremental Structure from Motion and Visual Localization With Planar Markers and Camera Groups</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16599"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xieyuser/MCGMapper"><img src="https://img.shields.io/github/stars/xieyuser/MCGMapper.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology (Guangzhou)<br>
‚Ä¢ Dataset: MCGMapper Synthetic Dataset, Samples: 6, Modality: Synthetic RGB images + Ground-truth pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Flow Snapshot Neurons in Action: Deep Neural Networks Generalize to Biological Motion Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16493"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computing and Data Science, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: Biological Motion Perception (BMP) Dataset, Samples: 62656, Modality: RGB videos, point-light display videos (Joint videos, Sequential position actor videos)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Multi-Agent Inverse Reinforcement Learning in Real World Unstructured Pedestrian Crowds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16439"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mairl-uva/mairl_uva.github.io"><img src="https://img.shields.io/github/stars/mairl-uva/mairl_uva.github.io.svg?style=social&label=Star"></a><br><a href="https://mairl-uva.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Virginia<br>
‚Ä¢ Dataset: Speedway, Samples: 500, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Video Prediction Models as General Visual Encoders</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.16382"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/anonymous_VPT-5E85/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: Custom annotated BAIR Robot Pushing segmentation masks, Samples: 250, Modality: RGB frames + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.15758"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://wangyuchi369.github.io/InstructAvatar/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: InstructAvatar's Instruction-Video Paired Dataset, Samples: over 60000, Modality: RGB videos + text instructions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Motion Segmentation for Neuromorphic Aerial Surveillance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.15209"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://samiarja.github.io/evairborne/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: International Center for Neuromorphic Systems, Western Sydney University<br>
‚Ä¢ Dataset: Ev-Airborne, Samples: 9, Modality: event camera data + ground truth annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Drones Help Drones: A Collaborative Framework for Multi-Drone Object Trajectory Prediction and Beyond</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.14674"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WangzcBruce/DHD"><img src="https://img.shields.io/github/stars/WangzcBruce/DHD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Key Laboratory of Network Information System Technology, Aerospace Information Research Institute, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: Air-Co-Pred, Samples: 200, Modality: Simulated multi-view RGB videos + 3D bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Event-based dataset for the detection and classification of manufacturing assembly tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.14626"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Robotics-and-AI/DAVIS-data-capture-system"><img src="https://img.shields.io/github/stars/Robotics-and-AI/DAVIS-data-capture-system.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/10562563"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Centre for Mechanical Engineering, Materials and Processes (CEMMPRE), ARISE, University of Coimbra, 3030 -788, Coimbra, Portugal<br>
‚Ä¢ Dataset: Event-based Dataset of Assembly Tasks (EDAT24), Samples: 400, Modality: Event camera data (events, greyscale frames)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>S3O: A Dual-Phase Approach for Reconstructing Dynamic Shape and Skeleton of Articulated Objects from Single Monocular Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.12607"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/haoz19/LIMR"><img src="https://img.shields.io/github/stars/haoz19/LIMR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, USA<br>
‚Ä¢ Dataset: PlanetZoo, Samples: None, Modality: RGB videos + 2D keypoints + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>InterAct: Capture and Modelling of Realistic, Expressive and Interactive Activities between Two Persons in Daily Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.11690"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hku-cg.github.io/interact"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: InterAct, Samples: 241, Modality: MoCap body motions, 3D facial meshes, audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.11286"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://steve-zeyu-zhang.github.io/MotionAvatar"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: La Trobe University; The Australian National University<br>
‚Ä¢ Dataset: Zoo-300K, Samples: 300000, Modality: text-motion pairs<br>
‚Ä¢ Dataset: Avatar Q&A Dataset, Samples: 1756, Modality: text-pairs (instruction/output)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>SignAvatar: Sign Language 3D Motion Reconstruction and Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.07974"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dongludeeplearning.github.io/SignAvatar.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Engineering, University at Buffalo, NY , USA<br>
‚Ä¢ Dataset: ASL3DWord, Samples: 1547, Modality: SMPL-X parameters (3D joint rotation)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Event-based Structure-from-Orbit</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.06216"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/0thane/eSfO"><img src="https://img.shields.io/github/stars/0thane/eSfO.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.10884693"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Adelaide<br>
‚Ä¢ Dataset: TOPSPIN, Samples: 72, Modality: event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Rotation Initialization and Stepwise Refinement for Universal LiDAR Calibration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.05589"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yjsx/ULC"><img src="https://img.shields.io/github/stars/yjsx/ULC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, University of Science and Technology of China, Hefei, 230026, China<br>
‚Ä¢ Dataset: COLORFUL, Samples: 120 seconds at 20Hz, Modality: LiDAR<br>
‚Ä¢ Dataset: Campus-SS, Samples: None, Modality: LiDAR<br>
‚Ä¢ Dataset: Campus-BS, Samples: None, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Audio Matters Too! Enhancing Markerless Motion Capture with Audio Signals for String Performance Capture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.04963"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yitongishere/string_performance"><img src="https://img.shields.io/github/stars/Yitongishere/string_performance.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Central Conservatory of Music, China and Tsinghua University, China<br>
‚Ä¢ Dataset: String Performance Dataset (SPD), Samples: 120, Modality: 3D MoCap annotations, multi-view RGB videos, audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>BILTS: A Bi-Invariant Similarity Measure for Robust Object Trajectory Recognition under Reference Frame Variations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.04392"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.12806232"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering, KU Leuven, Leuven, Belgium<br>
‚Ä¢ Dataset: Synthetic (SYN) rigid-body motion dataset, Samples: None, Modality: Synthetically generated rigid-body pose trajectories<br>
‚Ä¢ Dataset: adapted DLA 1, Samples: None, Modality: MoCap-derived pose trajectories<br>
‚Ä¢ Dataset: adapted DLA 2, Samples: None, Modality: MoCap-derived pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment and Spatially-variant Deformation Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.04309"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://npucvr.github.io/TSM-NRSfM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronics and Information, Northwestern Polytechnical University<br>
‚Ä¢ Dataset: H3WB-NRSfM, Samples: 5, Modality: 3D keypoint trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Exposing AI-generated Videos: A Benchmark Dataset and a Local-and-Global Temporal Defect Based Detection Method</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.04133"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Cyber Science and Engineering, Sichuan University<br>
‚Ä¢ Dataset: AI-generated Video Benchmark Dataset, Samples: 5000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>MoDiPO: text-to-motion alignment via AI-feedback-driven Direct Preference Optimization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.03803"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sapienza University of Rome<br>
‚Ä¢ Dataset: Pick-a-Move, Samples: not specified, Modality: MoCap pose vectors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Elevator, Escalator, or Neither? Classifying Conveyor State Using Smartphone under Arbitrary Pedestrian Behavior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.03218"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong<br>
‚Ä¢ Dataset: ELESON Dataset (not explicitly named), Samples: 36420, Modality: Smartphone INS (accelerometer, gyroscope, magnetometer)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Panoptic-SLAM: Visual SLAM in Dynamic Environments using Panoptic Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.02177"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/iit-DLSLab/Panoptic-SLAM"><img src="https://img.shields.io/github/stars/iit-DLSLab/Panoptic-SLAM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dynamic Legged Systems Lab, Istituto Italiano di Tecnologia, Italy; Department of Mechanical Engineering at the Pontifical Catholic University of Rio de Janeiro, Brazil<br>
‚Ä¢ Dataset: Indoor Quadruped SLAM Dataset, Samples: 3, Modality: RGB-D video + Vicon ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>IFNet: Deep Imaging and Focusing for Handheld SAR with Millimeter-wave Signals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.02023"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Cyber Science and Technology, University of Science and Technology of China<br>
‚Ä¢ Dataset: Handheld SAR imaging dataset, Samples: 200, Modality: Pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Zero-Shot Monocular Motion Segmentation in the Wild by Combining Deep Learning with Geometric Motion Model Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.01723"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Vision and Image Processing Lab, University of Waterloo<br>
‚Ä¢ Dataset: KT3DInsMoSeg, Samples: None, Modality: RGB videos + point trajectories + pixel-level segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2024</td>
  <td style="width:70%;"><strong>Towards Real-World HDR Video Reconstruction: A Large-Scale Benchmark Dataset and A Two-Stage Alignment Network</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2405.00244"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yungsyu99/Real-HDRV"><img src="https://img.shields.io/github/stars/yungsyu99/Real-HDRV.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai University, China<br>
‚Ä¢ Dataset: Real-HDRV, Samples: 500, Modality: LDR/HDR video pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Ultra Inertial Poser: Scalable Motion Capture and Tracking from Sparse Inertial Sensors and Ultra-Wideband Ranging</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.19541"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/eth-siplab/UltraInertialPoser"><img src="https://img.shields.io/github/stars/eth-siplab/UltraInertialPoser.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, ETH Z√ºrich, Switzerland<br>
‚Ä¢ Dataset: UIP-DB, Samples: 25 motion types from 10 participants, totaling 200 minutes of data, Modality: 6-DoF IMU signals, UWB measurements and distances, SMPL parameters, Optical motion capture<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>MoST: Multi-modality Scene Tokenization for Motion Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.19531"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Waymo LLC<br>
‚Ä¢ Dataset: Waymo Open Motion Dataset (WOMD) [Augmented with Camera Embeddings], Samples: None, Modality: LiDAR, 3D bounding box tracks, road graph, traffic signals, camera embeddings<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Towards Real-world Video Face Restoration: A New Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.19500"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZiyannChen/VFR-FOS-Benchmark"><img src="https://img.shields.io/github/stars/ZiyannChen/VFR-FOS-Benchmark.svg?style=social&label=Star"></a><br><a href="https://ziyannchen.github.io/projects/VFRxBenchmark/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: FOS-V, Samples: 3316, Modality: RGB videos<br>
‚Ä¢ Dataset: FOS-real, Samples: 4253, Modality: RGB images<br>
‚Ä¢ Dataset: FOS-syn, Samples: 3150, Modality: RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.19110"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Imperial College London<br>
‚Ä¢ Dataset: FEED (Facial Extreme Emotions Dataset), Samples: 520, Modality: multi-view videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>4D-DRESS: A 4D Dataset of Real-world Human Clothing with Semantic Annotations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.18630"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ait.ethz.ch/4d-dress"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, ETH Z ¬®urich<br>
‚Ä¢ Dataset: 4D-DRESS, Samples: 520, Modality: 4D textured scans, garment meshes, SMPL(-X) body models, multi-view images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>LIKO: LiDAR, Inertial, and Kinematic Odometry for Bipedal Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.18047"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Mr-Zqr/LIKO"><img src="https://img.shields.io/github/stars/Mr-Zqr/LIKO.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Mechatronical Engineering, Beijing Institute of Technology (BIT), Beijing, China<br>
‚Ä¢ Dataset: LIKO biped robot dataset, Samples: 5, Modality: LiDAR, Inertial Measurement Unit (IMU), joint encoders, Force/Torque (F/T) sensors, motion capture ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Retrieval Robust to Object Motion Blur</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.18025"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Rong-Zou/Retrieval-Robust-to-Object-Motion-Blur"><img src="https://img.shields.io/github/stars/Rong-Zou/Retrieval-Robust-to-Object-Motion-Blur.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich<br>
‚Ä¢ Dataset: Synthetic Blurred Object Retrieval Dataset, Samples: 245760, Modality: 3D object models + motion trajectories -> RGB images<br>
‚Ä¢ Dataset: Real-world Blurred Object Retrieval Dataset, Samples: 139, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.17063"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hilab-open-source/wheelpose"><img src="https://img.shields.io/github/stars/hilab-open-source/wheelpose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Los Angeles<br>
‚Ä¢ Dataset: WheelPose Motion Sequences, Samples: 200, Modality: 3D joint trajectories<br>
‚Ä¢ Dataset: Wheelchair User Testing Dataset, Samples: 2464, Modality: RGB images + 2D keypoint annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Motor Focus: Fast Ego-Motion Prediction for Assistive Visual Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.17031"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arazi2/VisionGPT"><img src="https://img.shields.io/github/stars/arazi2/VisionGPT.svg?style=social&label=Star"></a><br><a href="https://arazi2.github.io/aisends.github.io/project/VisionGPT"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computing, Clemson University<br>
‚Ä¢ Dataset: Visual Navigation Dataset, Samples: 50, Modality: RGB videos + moving direction annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Learning Visuotactile Skills with Two Multifingered Hands</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.16823"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Toru-Lin/HATO"><img src="https://img.shields.io/github/stars/Toru-Lin/HATO.svg?style=social&label=Star"></a><br><a href="https://toru-lin.github.io/hato/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Berkeley<br>
‚Ä¢ Dataset: HATO teleoperation dataset, Samples: 800, Modality: robot kinematics, RGB-D videos, tactile sensor readings, control actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>You Think, You ACT: The New Task of Arbitrary Text to Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.14745"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Engineering Research Center for Multimedia Software, Wuhan University; School of Computer Science, Wuhan University<br>
‚Ä¢ Dataset: HUMAN ML3D++, Samples: 15000, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Non-Uniform Exposure Imaging via Neuromorphic Shutter Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.13972"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University<br>
‚Ä¢ Dataset: Neuromorphic Exposure Dataset (NED), Samples: 51, Modality: RGB frames + events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>HOIST-Former: Hand-held Objects Identification, Segmentation, and Tracking in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.13819"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://supreethn.github.io/research/hoistformer/index.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stony Brook University, USA<br>
‚Ä¢ Dataset: HOIST, Samples: 4228, Modality: RGB videos + bounding boxes + segmentation masks + tracking IDs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>A Dataset and Model for Realistic License Plate Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.13677"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/haoyGONG/LPDGAN"><img src="https://img.shields.io/github/stars/haoyGONG/LPDGAN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of AI and Advanced Computing, Xi‚Äôan Jiaotong-Liverpool University<br>
‚Ä¢ Dataset: LPBlur, Samples: 10288, Modality: Paired sharp and motion-blurred RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>MLP: Motion Label Prior for Temporal Sentence Localization in Untrimmed 3D Human Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.13657"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/eanson023/mlp"><img src="https://img.shields.io/github/stars/eanson023/mlp.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence, Chongqing University of Technology, Chongqing 401120, China<br>
‚Ä¢ Dataset: HumanML3D (Restore), Samples: 5784, Modality: 3D human poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Turb-Seg-Res: A Segment-then-Restore Pipeline for Dynamic Videos with Atmospheric Turbulence</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.13605"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/riponcs/TurbSegRes"><img src="https://img.shields.io/github/stars/riponcs/TurbSegRes.svg?style=social&label=Star"></a><br><a href="https://riponcs.github.io/TurbSegRes"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Arizona State University<br>
‚Ä¢ Dataset: Augmented URG-T Dataset, Samples: None, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>ConCLVD: Controllable Chinese Landscape Video Generation via Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.12903"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Artificial Intelligence, Xiamen University<br>
‚Ä¢ Dataset: CLV-HD, Samples: 1300, Modality: RGB videos + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Seeing Motion at Nighttime with an Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.11884"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Liu-haoyue/NER-Net"><img src="https://img.shields.io/github/stars/Liu-haoyue/NER-Net.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Lab of Multispectral Information Intelligent Processing Technology School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, China<br>
‚Ä¢ Dataset: RLED, Samples: 64200, Modality: event streams + RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Text-controlled Motion Mamba: Text-Instructed Temporal Grounding of Human Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.11375"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: BABEL-Grounding, Samples: 5339, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>VBR: A Vision Benchmark in Rome</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.11322"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.rvp-group.net/datasets/slam"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer, Control, and Management Engineering ‚ÄúAntonio Ruberti‚Äù, Sapienza University of Rome, Italy<br>
‚Ä¢ Dataset: VBR (Vision Benchmark in Rome), Samples: 6, Modality: RGB, LiDAR, IMU, GPS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Neuromorphic Vision-based Motion Segmentation with Graph Transformer Neural Network</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.10940"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yusra-alkendi/EMS-GTNN"><img src="https://img.shields.io/github/stars/Yusra-alkendi/EMS-GTNN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Propulsion and Space Research Center (PSRC) at the Technology Innovation Institute (TII), Abu Dhabi, UAE<br>
‚Ä¢ Dataset: EMS-DOMEL (Event dataset for Motion Segmentation), Samples: 9, Modality: Event streams + motion segmentation labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Generating Human Interaction Motions in Scenes with Text Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.10685"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://research.nvidia.com/labs/toronto-ai/tesmo"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA<br>
‚Ä¢ Dataset: Loco-3D-FRONT, Samples: 9500 walking motion sequences (resulting in 95k locomotion-scene pairs), Modality: MoCap joints + text descriptions + 3D scenes<br>
‚Ä¢ Dataset: SAMP (extended), Samples: approx. 16000 sub-sequences (derived from 80 original sequences), Modality: MoCap joints + text descriptions + 3D objects<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.09748"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://garageworld.letsgoproject.com/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University, China and Stereye Intelligent Technology Co.,Ltd., China<br>
‚Ä¢ Dataset: GarageWorld, Samples: 8, Modality: LiDAR point clouds, IMU data, RGB fisheye images, pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Exploring Text-to-Motion Generation with Human Preference</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.09445"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: Text-to-Motion Preference Pairs, Samples: 3528, Modality: Text prompts + tokenized motion pairs + human preference labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.08640"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/EventEgo3D/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Informatics, SIC; Saarland University, SIC<br>
‚Ä¢ Dataset: EE3D-S, Samples: 946 motion sequences, Modality: Event streams + 3D human poses + segmentation masks<br>
‚Ä¢ Dataset: EE3D-R, Samples: 464000 poses, Modality: Event streams + 3D human poses + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>FusionPortableV2: A Unified Multi-Sensor Dataset for Generalized SLAM Across Diverse Platforms and Scalable Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.08563"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fusionportable.github.io/dataset/fusionportable_v2"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China.<br>
‚Ä¢ Dataset: FusionPortableV2, Samples: 27, Modality: IMU, stereo frame cameras, stereo event cameras, 3D LiDAR, INS, wheel encoders, legged robot sensors, ground-truth trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Context-aware Video Anomaly Detection in Long-Term Datasets</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.07887"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Rensselaer Polytechnic Institute<br>
‚Ä¢ Dataset: WF dataset, Samples: 2000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.07569"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mh0797/interPlan"><img src="https://img.shields.io/github/stars/mh0797/interPlan.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of T√ºbingen<br>
‚Ä¢ Dataset: interPlan, Samples: 80, Modality: Simulator scenarios based on augmented nuPlan data (agent bounding boxes, maps, navigation goals)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.06710"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University<br>
‚Ä¢ Dataset: RGB & Spike 3D (RS-3D), Samples: 240 sequences (6 scenes x 40 viewpoints), Modality: RGB videos + Spike streams + Camera Poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Multi-agent Long-term 3D Human Pose Forecasting via Interaction-aware Trajectory Conditioning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.05218"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Jaewoo97/T2P"><img src="https://img.shields.io/github/stars/Jaewoo97/T2P.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST<br>
‚Ä¢ Dataset: JRDB-GlobMultiPose (JRDB-GMP), Samples: 5746, Modality: 3D human pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.05014"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pku-yuangroup.github.io/MagicTime"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Graduate School, Peking University<br>
‚Ä¢ Dataset: ChronoMagic, Samples: 2265, Modality: RGB videos + text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>NeRF2Points: Large-Scale Point Cloud Generation From Street Views' Radiance Field Optimization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.04875"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ruqi Mobility Inc., Guangzhou, China<br>
‚Ä¢ Dataset: None, Samples: 180000, Modality: RGB images, camera poses, LiDAR point clouds, depth maps, normal vectors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>CEAR: Comprehensive Event Camera Dataset for Rapid Perception of Agile Quadruped Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.04698"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://daroslab.github.io/cear/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Manning College of Information & Computer Sciences, University of Massachusetts Amherst, MA, USA<br>
‚Ä¢ Dataset: CEAR, Samples: 106, Modality: Event camera, RGB-D, LiDAR, IMU, Joint encoders, MoCap ground-truth pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>HDR Imaging for Dynamic Scenes with Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.03210"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lxp-whu/Self-EHDRI"><img src="https://img.shields.io/github/stars/lxp-whu/Self-EHDRI.svg?style=social&label=Star"></a><br><a href="https://lxp-whu.github.io/Self-EHDRI"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University<br>
‚Ä¢ Dataset: BL2SHD, Samples: 120, Modality: blurry LDR images, event streams, sharp LDR images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Event-assisted Low-Light Video Object Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.01945"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HebeiFast/EventLowLightVOS"><img src="https://img.shields.io/github/stars/HebeiFast/EventLowLightVOS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Science and Technology of China<br>
‚Ä¢ Dataset: LLE-DAVIS, Samples: 90, Modality: low-light videos + event streams<br>
‚Ä¢ Dataset: LLE-VOS, Samples: 70, Modality: normal-light videos + low-light videos + event streams + segmentation annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Large Motion Model for Unified Multi-Modal Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.01284"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mingyuan-zhang.github.io/projects/LMM.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: MotionVerse, Samples: 320000, Modality: Unified motion representation (positions, velocities, rotations) consolidated from 16 existing datasets (SMPL, SMPL-X, 3D keypoints).<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Gyro-based Neural Single Image Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.00916"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: POSTECH CSE<br>
‚Ä¢ Dataset: GyroBlur-Synth, Samples: 15240, Modality: RGB images + gyroscope data<br>
‚Ä¢ Dataset: GyroBlur-Real, Samples: 117, Modality: RGB images + gyroscope data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.00562"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JunukCha/Text2HOI"><img src="https://img.shields.io/github/stars/JunukCha/Text2HOI.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UNIST<br>
‚Ä¢ Dataset: H2O (extended with text prompts), Samples: 660, Modality: hand-object mesh sequences + text prompts<br>
‚Ä¢ Dataset: GRAB (extended with text prompts), Samples: 1335, Modality: hand-object mesh sequences + text prompts<br>
‚Ä¢ Dataset: ARCTIC (extended with text prompts), Samples: 4597, Modality: hand-object mesh sequences + text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2024</td>
  <td style="width:70%;"><strong>Choreographing the Digital Canvas: A Machine Learning Approach to Artistic Performance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2404.00054"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Maryland, College Park<br>
‚Ä¢ Dataset: falling pose dataset, Samples: 150, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>A Unified Framework for Human-centric Point Cloud Video Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.20031"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Human Body Segmentation Synthetic Dataset (LiDARPart-Human), Samples: 1000000, Modality: synthetic LiDAR point clouds + body part labels<br>
‚Ä¢ Dataset: Human Motion Flow Synthetic Dataset (LiDARFlow-Human), Samples: 2378871, Modality: synthetic LiDAR point clouds + motion flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>SceneTracker: Long-term Scene Flow Estimation Network</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.19924"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wwsource/SceneTracker"><img src="https://img.shields.io/github/stars/wwsource/SceneTracker.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China<br>
‚Ä¢ Dataset: LSFOdyssey, Samples: 127527, Modality: RGB-D videos + 3D trajectories<br>
‚Ä¢ Dataset: LSFDriving, Samples: 180, Modality: RGB-D videos + 3D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.19501"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.lidarhumanmotion.net/reli11d/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University<br>
‚Ä¢ Dataset: RELI11D, Samples: 48, Modality: LiDAR point clouds, IMU, RGB videos, Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Beyond Talking -- Generating Holistic 3D Human Dyadic Motion for Communication</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.19467"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua Shenzhen International Graduate School, China<br>
‚Ä¢ Dataset: HoCo, Samples: 22913, Modality: RGB videos + audio + text transcripts + SMPL-X parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.19417"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://oakink.net/v2"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: OAKINK2, Samples: 627, Modality: Multi-view RGB videos + MoCap-derived 3D poses (human body, hands, objects)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Within the Dynamic Context: Inertia-aware 3D Human Modeling with Pose Sequence</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.19160"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Ytong-Chen/Dyco-release"><img src="https://img.shields.io/github/stars/Ytong-Chen/Dyco-release.svg?style=social&label=Star"></a><br><a href="https://ai4sports.opengvlab.com/Dyco"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Artificial Intelligence Laboratory<br>
‚Ä¢ Dataset: I3D-Human, Samples: 6, Modality: Multi-view RGB videos + SMPL poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>EgoNav: Egocentric Scene-aware Human Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.19026"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: Egocentric Navigation Dataset, Samples: 220000, Modality: 6-DoF torso pose, leg joint angles, torso velocities, gait frequency, RGBD images, semantic segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.18820"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vcai.mpi-inf.mpg.de/projects/MetaCap/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: WildDynaCap, Samples: 2, Modality: Multi-view RGB videos + skeletal motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.18811"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lisiyao21/Duolando"><img src="https://img.shields.io/github/stars/lisiyao21/Duolando.svg?style=social&label=Star"></a><br><a href="https://lisiyao21.github.io/projects/Duolando"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: DD100, Samples: 100, Modality: MoCap (SMPL-X format with inertial glove data)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Tracking-Assisted Object Detection with Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.18330"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tkyen1110/TEDNet"><img src="https://img.shields.io/github/stars/tkyen1110/TEDNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Taiwan University<br>
‚Ä¢ Dataset: 1 Megapixel Automotive Detection Dataset (Cleaned with Visibility Labels), Samples: 15.65 hours of recordings, Modality: Event camera data + bounding boxes with visibility labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.17936"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, SIC<br>
‚Ä¢ Dataset: DND GROUP GESTURE, Samples: 2.7M poses, Modality: markerless MoCap (3D body and hand poses)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.17827"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://diffh2o.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH, Switzerland and Meta, Switzerland<br>
‚Ä¢ Dataset: GRAB dataset (extended with detailed textual descriptions), Samples: 1335, Modality: Textual descriptions for MoCap sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.17610"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://metaverse-ai-lab-thu.github.io/MMVP-Dataset/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Beihang University<br>
‚Ä¢ Dataset: MMVP, Samples: 44000, Modality: RGBD video + pressure sensor data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Benchmarking Video Frame Interpolation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.17128"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Darmstadt, hessian.AI<br>
‚Ä¢ Dataset: Ours, Samples: 666, Modality: Synthetic RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>TAIL: A Terrain-Aware Multi-Modal SLAM Dataset for Robot Locomotion in Deformable Granular Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.16875"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tailrobot.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Key Laboratory of Biomimetic Robotics and Intelligent Systems, Department of Mechanical and Energy Engineering, Southern University of Science and Technology (SUSTech), Shenzhen, 518055, China<br>
‚Ä¢ Dataset: TAIL, Samples: 14, Modality: Stereo cameras, RGB-D cameras, 3D LiDAR, IMU, RTK, Robot odometry/kinematics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.16425"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/gokulbnr/fast-slow-biased-event-vpr"><img src="https://img.shields.io/github/stars/gokulbnr/fast-slow-biased-event-vpr.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Centre for Robotics, Faculty of Engineering, Queensland University of Technology, Brisbane, QLD Australia 4000<br>
‚Ä¢ Dataset: QCR-Fast-and-Slow-Dataset, Samples: 366, Modality: event streams, conventional image data, ground truth robot poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.16169"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Takiee/Gaze-HOI"><img src="https://img.shields.io/github/stars/Takiee/Gaze-HOI.svg?style=social&label=Star"></a><br><a href="https://takiee.github.io/gaze-hoi/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Not provided in the paper<br>
‚Ä¢ Dataset: GazeHOI, Samples: 1378, Modality: 3D hand poses (MANO), 6D object poses, 3D gaze points, multi-view RGB videos, egocentric RGBD videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.16080"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pku-dymvhumans.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University Shenzhen Graduate School, Peng Cheng Laboratory<br>
‚Ä¢ Dataset: PKU-DyMVHumans, Samples: 2668, Modality: Multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Contact-aware Human Motion Generation from Textual Descriptions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.15709"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://xymsh.github.io/RICH-CAT"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Sydney, Australia<br>
‚Ä¢ Dataset: RICH-CAT, Samples: 8566, Modality: SMPL-X MoCap joints + contact labels + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>DragAPart: Learning a Part-Level Motion Prior for Articulated Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.15382"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="dragapart.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Geometry Group, University of Oxford<br>
‚Ä¢ Dataset: Drag-a-Move, Samples: 40000000, Modality: Synthetic RGB image pairs + drag annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.14781"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fudan-generative-vision.github.io/champ"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing University<br>
‚Ä¢ Dataset: in-the-wild human video dataset, Samples: 5000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Explorative Inbetweening of Time and Space</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.14611"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://time-reversal.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems<br>
‚Ä¢ Dataset: Bounded Generation Dataset, Samples: 395, Modality: RGB image pairs and static images (some with ground-truth videos)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.13667"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Carmenw1203/DanceCamera3D-Official"><img src="https://img.shields.io/github/stars/Carmenw1203/DanceCamera3D-Official.svg?style=social&label=Star"></a><br><a href="https://github.com/Carmenw1203/DanceCamera3D-Official"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China<br>
‚Ä¢ Dataset: DCM, Samples: 108, Modality: Paired 3D dance motion (60 joints), 3D camera movement, and music audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Motion Generation from Fine-grained Textual Descriptions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.13518"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/KunhangL/finemotiondiffuse"><img src="https://img.shields.io/github/stars/KunhangL/finemotiondiffuse.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University, The University of Tokyo<br>
‚Ä¢ Dataset: FineHumanML3D, Samples: 29228, Modality: 3D human motions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>LaserHuman: Language-guided Scene-aware Human Motion Generation in Free Environment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.13307"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/4DVLab/LaserHuman"><img src="https://img.shields.io/github/stars/4DVLab/LaserHuman.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: LaserHuman, Samples: 3374, Modality: SMPL, RGB-D, text, Point Cloud, Scene-Map<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Map-Aware Human Pose Prediction for Robot Follow-Ahead</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.13294"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://qingyuan-jiang.github.io/iros2024_poseForecasting/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Minnesota<br>
‚Ä¢ Dataset: Real Indoor Motion (Real-IM), Samples: 12, Modality: RGB-D video + 3D skeleton poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>WHAC: World-grounded Humans and Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.12959"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SenseTime Research, The University of Tokyo<br>
‚Ä¢ Dataset: WHAC-A-Mole, Samples: 2434 sequences, Modality: Synthetic rendered RGB videos with SMPL-X and camera pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>EmoVOCA: Speech-Driven Emotional 3D Talking Heads</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.12886"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/miccunifi/EmoVOCA"><img src="https://img.shields.io/github/stars/miccunifi/EmoVOCA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Florence, Italy<br>
‚Ä¢ Dataset: EmoVOCAv1, Samples: 7200, Modality: 3D face mesh sequences<br>
‚Ä¢ Dataset: EmoVOCAv2, Samples: 15840, Modality: 3D face mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>ReGenNet: Towards Human Action-Reaction Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.11882"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/liangxuy/ReGenNet"><img src="https://img.shields.io/github/stars/liangxuy/ReGenNet.svg?style=social&label=Star"></a><br><a href="https://liangxuy.github.io/ReGenNet/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China; Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China<br>
‚Ä¢ Dataset: NTU120-AS, Samples: 8118, Modality: SMPL-X<br>
‚Ä¢ Dataset: InterHuman-AS, Samples: 6022, Modality: SMPL<br>
‚Ä¢ Dataset: Chi3D-AS, Samples: 373, Modality: SMPL-X<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>FE-DeTr: Keypoint Detection and Tracking in Low-quality Image Frames with Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.11662"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yuyangpoi/FE-DeTr"><img src="https://img.shields.io/github/stars/yuyangpoi/FE-DeTr.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: EIS, Wuhan University, Wuhan, China<br>
‚Ä¢ Dataset: Extreme Corner, Samples: 32, Modality: RGB frames + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures for Human Avatar Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.11589"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://alex-jyj.github.io/UV-Gaussians/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua Shenzhen International Graduate School, Tsinghua University<br>
‚Ä¢ Dataset: unnamed human motion dataset, Samples: 5, Modality: multi-view images, scanned models, SMPL-X model parameters, SMPLX-D model parameters, texture maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>FORCE: Physics-aware Human-object Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.11237"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://virtualhumans.mpi-inf.mpg.de/force/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: T¬®ubingen AI Center, University of T ¬®ubingen; Max Planck Institute for Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: FORCE, Samples: 450, Modality: MoCap (4x RGB-D cameras + 17x IMUs) yielding human (SMPL) and object poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>THOR: Text to Human-Object Interaction Diffusion via Relation Intervention</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.11208"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Text-BEHAVE, Samples: 2377, Modality: SMPL-H poses + object 6D poses + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>NetTrack: Tracking Highly Dynamic Objects with a Net</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.11186"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/george-zhuang/NetTrack"><img src="https://img.shields.io/github/stars/george-zhuang/NetTrack.svg?style=social&label=Star"></a><br><a href="https://george-zhuang.github.io/nettrack"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: Bird Flock Tracking (BFT), Samples: 106, Modality: RGB videos + tracking annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Reconfigurable Robot Identification from Motion Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.10496"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/H-Y-H-Y-H/meta_selfmodeling_id"><img src="https://img.shields.io/github/stars/H-Y-H-Y-H/meta_selfmodeling_id.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Columbia University<br>
‚Ä¢ Dataset: 12-DoF Reconfigurable Legged Robot Motion Dataset, Samples: 200000, Modality: proprioceptive robot states (body position, orientation, joint angles) and actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>CPGA: Coding Priors-Guided Aggregation Network for Compressed Video Quality Enhancement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.10362"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VQE-CPGA/CPGA.git"><img src="https://img.shields.io/github/stars/VQE-CPGA/CPGA.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: VideoCoding Priors (VCP), Samples: 300, Modality: RGB videos + motion vectors + predictive frames + residual frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>RID-TWIN: An end-to-end pipeline for automatic face de-identification in videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.10058"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AnirbanMukherjeeXD/RID-Twin"><img src="https://img.shields.io/github/stars/AnirbanMukherjeeXD/RID-Twin.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: International Institute of Information Technology, Bangalore<br>
‚Ä¢ Dataset: Proposed Custom Dataset, Samples: None, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Interactive Distance Field Mapping and Planning to Enable Human-Robot Collaboration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.09988"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UTS-RI/IDMP"><img src="https://img.shields.io/github/stars/UTS-RI/IDMP.svg?style=social&label=Star"></a><br><a href="https://uts-ri.github.io/IDMP"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Robotics (CERI) at the Technical University of Applied Sciences W√ºrzburg-Schweinfurt (THWS), Germany<br>
‚Ä¢ Dataset: Custom synthetic dataset featuring a dynamically moving ball on a table, Samples: None, Modality: simulated RGB-D camera<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.09850"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jiayi-wu-umd/MARVIS"><img src="https://img.shields.io/github/stars/jiayi-wu-umd/MARVIS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Maryland Robotics Center (MRC), University of Maryland, College Park, MD 20742, USA<br>
‚Ä¢ Dataset: MARVIS Synthetic Dataset, Samples: 3012, Modality: RGB video frame pairs + real/virtual masks<br>
‚Ä¢ Dataset: MARVIS Real Dataset, Samples: 450, Modality: Stereo RGB video frame pairs + real/virtual masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>GenAD: Generalized Predictive Model for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.09630"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OpenDriveLab/DriveAGI"><img src="https://img.shields.io/github/stars/OpenDriveLab/DriveAGI.svg?style=social&label=Star"></a><br><a href="https://github.com/OpenDriveLab/DriveAGI"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: OpenDriveLab and Shanghai AI Lab<br>
‚Ä¢ Dataset: OpenDV-2K, Samples: 65000000, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.09486"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chenkang455/S-SDM"><img src="https://img.shields.io/github/stars/chenkang455/S-SDM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University<br>
‚Ä¢ Dataset: RSB (Real-world Spike-guided Blur), Samples: 10, Modality: blurry RGB images + spike streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>LM2D: Lyrics- and Music-Driven Dance Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.09407"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://youtu.be/4XCgvYookvA"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: unnamed, Samples: 1867, Modality: 3D human motion (SMPL parameters) + music + lyrics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>TH√ñR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.09285"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tmralmeida/thor-magni-tools"><img src="https://img.shields.io/github/stars/tmralmeida/thor-magni-tools.svg?style=social&label=Star"></a><br><a href="http://thor.oru.se/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ¬®Orebro University, Sweden<br>
‚Ä¢ Dataset: TH¬®OR-MAGNI, Samples: 52, Modality: Motion capture (position, velocity, head orientation), eye tracking (gaze), 3D LiDAR, RGB-D camera, RGB camera<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Caltech Aerial RGB-Thermal Dataset in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.08997"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/aerorobotics/caltech-aerial-rgbt-dataset"><img src="https://img.shields.io/github/stars/aerorobotics/caltech-aerial-rgbt-dataset.svg?style=social&label=Star"></a><br><a href="https://github.com/aerorobotics/caltech-aerial-rgbt-dataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: California Institute of Technology<br>
‚Ä¢ Dataset: Caltech Aerial RGB-Thermal Dataset, Samples: 37, Modality: RGB video, Thermal video, GPS, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.08764"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://enriccorona.github.io/vlogger/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google Research<br>
‚Ä¢ Dataset: MENTOR, Samples: 800000, Modality: RGB videos + audio + 3D pose and expression annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Scaling Up Dynamic Human-Scene Interaction Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.08629"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jnnan.github.io/trumans/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for AI, Peking University; National Key Lab of General AI, BIGAI<br>
‚Ä¢ Dataset: TRUMANS, Samples: 1.6 million frames, Modality: MoCap, SMPL-X, RGBD videos, segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.08268"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://follow-your-click.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: HKUST<br>
‚Ä¢ Dataset: WebVid-Motion, Samples: Not specified; derived by filtering and re-annotating the WebVid-10M dataset, Modality: RGB videos + short motion prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>NeRF-Supervised Feature Point Detection and Description</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.08156"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/aliyoussef1/SiLK-PrP"><img src="https://img.shields.io/github/stars/aliyoussef1/SiLK-PrP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science University College London<br>
‚Ä¢ Dataset: NeRF-synthesised multi-view dataset, Samples: 10000, Modality: Synthetic RGB images + depth maps + camera intrinsics/extrinsics<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>CMax-SLAM: Event-based Rotational-Motion Bundle Adjustment and SLAM System using Contrast Maximization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.08119"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tub-rip/cmax_slam"><img src="https://img.shields.io/github/stars/tub-rip/cmax_slam.svg?style=social&label=Star"></a><br><a href="https://github.com/tub-rip/cmax_slam"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Electrical Engineering and Computer Science of TU Berlin, Berlin, Germany<br>
‚Ä¢ Dataset: Self-recorded rotational motion dataset with VGA-resolution event camera, Samples: 10, Modality: Events + IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.07436"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Lab of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China<br>
‚Ä¢ Dataset: various-illumination event (VIE) dataset, Samples: , Modality: paired RGB-event data with manually handcrafted moving object bounding box labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic Architecture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.07347"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Jiafei127/FD4MM"><img src="https://img.shields.io/github/stars/Jiafei127/FD4MM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Information Engineering, Hefei University of Technology, China<br>
‚Ä¢ Dataset: Synthetic Motion Magnification Dataset, Samples: 10, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Physics Sensor Based Deep Learning Fall Detection System</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.06994"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WuShaoa/SensorDataClassification-TCN/tree/main/SeqClassifyCNN"><img src="https://img.shields.io/github/stars/main/SeqClassifyCNN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer, Northwestern Polytechnical University, Xi‚Äôan, Shaanxi 710072, China.<br>
‚Ä¢ Dataset: Our data, Samples: 26, Modality: Foot-worn IMU (accelerometer, gyroscope, orientation) and pressure sensor data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Real-Time Simulated Avatar from Head-Mounted Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.06862"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zhengyiluo.github.io/SimXR/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Reality Labs Research, Meta; Carnegie Mellon University<br>
‚Ä¢ Dataset: Synthetic Quest 2 Dataset, Samples: 5169 sequences (2,216,000 frames), Modality: Synthetic monochrome images (4 views), 6DoF headset poses, ground-truth MoCap poses<br>
‚Ä¢ Dataset: Real-world Quest 2 Dataset, Samples: 10 sequences (40,000 frames), Modality: Real-world monochrome SLAM images, 6DoF headset poses, third-person images for pseudo-ground truth pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Eliminating Warping Shakes for Unsupervised Online Video Stitching</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.06378"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nie-lang/StabStitch"><img src="https://img.shields.io/github/stars/nie-lang/StabStitch.svg?style=social&label=Star"></a><br><a href="https://www.youtube.com/watch?v=03kGEZJHxzI"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Science, Beijing Jiaotong University; Beijing Key Laboratory of Advanced Information Science and Network<br>
‚Ä¢ Dataset: StabStitch-D, Samples: 100+ video pairs, Modality: RGB video pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Audio-Synchronized Visual Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.05659"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lzhangbj.github.io/projects/asva/asva.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Wisconsin-Madison<br>
‚Ä¢ Dataset: AVSync15, Samples: 1500, Modality: RGB videos + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Pix2Gif: Motion-Guided Diffusion for GIF Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.04634"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hiteshk03.github.io/Pix2Gif/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Microsoft Research, India<br>
‚Ä¢ Dataset: Curated TGIF for Pix2Gif, Samples: 78692, Modality: RGB image pairs + text captions + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Out of the Room: Generalizing Event-Based Dynamic Motion Segmentation for Complex Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.04562"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rwn17/DSEC-MOTS"><img src="https://img.shields.io/github/stars/rwn17/DSEC-MOTS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huawei Technologies, Zurich Research Center<br>
‚Ä¢ Dataset: DSEC-MOTS, Samples: None, Modality: Event camera data + motion segmentation masks + tracking annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>HMD-Poser: On-Device Real-time Human Motion Tracking from Scalable Sparse Observations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.03561"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: PICO, ByteDance<br>
‚Ä¢ Dataset: free-dancing motion dataset, Samples: 74, Modality: HMD sensor data, IMU sensor data, ground-truth SMPL parameters from OptiTrack<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Biomechanical Comparison of Human Walking Locomotion on Solid Ground and Sand</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.03105"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://dx.doi.org/10.17632/jgdpjrf584.2"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Rutgers, The State University of New Jersey<br>
‚Ä¢ Dataset: Biomechanical Comparison of Human Walking Locomotion on Solid Ground and Sand, Samples: , Modality: MoCap markers, Ground Reaction Forces (GRF)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>RT-H: Action Hierarchies Using Language</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.01823"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="rt-hierarchy.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google DeepMind, Stanford University<br>
‚Ä¢ Dataset: Diverse, Samples: 30000, Modality: Robot demonstrations (visual observations, proprioception, actions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>6DoF SELD: Sound Event Localization and Detection Using Microphones and Motion Tracking Sensors on self-motioning human</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.01670"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nttrd-mdlab/6dof-seld"><img src="https://img.shields.io/github/stars/nttrd-mdlab/6dof-seld.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NTT Corporation, Japan<br>
‚Ä¢ Dataset: 6DoF SELD Dataset, Samples: None, Modality: Audio + Motion tracker (head position, posture)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Kick Back & Relax++: Scaling Beyond Ground-Truth Depth with SlowTV & CribsTV</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.01569"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jspenmar/slowtv_monodepth"><img src="https://img.shields.io/github/stars/jspenmar/slowtv_monodepth.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CVSSP, University of Surrey<br>
‚Ä¢ Dataset: SlowTV, Samples: 1700000, Modality: RGB videos<br>
‚Ä¢ Dataset: CribsTV, Samples: 330000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>Tri-Modal Motion Retrieval by Learning a Joint Embedding Space</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.00691"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lavimo2023.github.io/User-Study-LAVIMO/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: HumanML3D (augmented), Samples: 14616, Modality: MoCap + Text + Rendered RGB videos<br>
‚Ä¢ Dataset: KIT-ML (augmented), Samples: 3911, Modality: MoCap + Text + Rendered RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2024</td>
  <td style="width:70%;"><strong>CustomListener: Text-guided Responsive Interaction for User-friendly Listening Head Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2403.00274"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://customlistener.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Vision AI Department, Meituan<br>
‚Ä¢ Dataset: Text-annotated ViCo, Samples: None, Modality: RGB videos, 3DMM coefficients, text annotations, speech transcripts<br>
‚Ä¢ Dataset: Text-annotated RealTalk, Samples: None, Modality: RGB videos, 3DMM coefficients, text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>Detection of Micromobility Vehicles in Urban Traffic Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.18503"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sabrikhalil/Micro Mobility Detection"><img src="https://img.shields.io/github/stars/sabrikhalil/Micro Mobility Detection.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Polytechnique Montr√©al<br>
‚Ä¢ Dataset: PolyMMV, Samples: 105, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.17171"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: FreeMotion, Samples: 578,775 frames, Modality: LiDAR point clouds, RGB images, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>XAI-based gait analysis of patients walking with Knee-Ankle-Foot orthosis using video cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.16175"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://tinyurl.com/5ds5f33c"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of EEE, BITS Pilani, K K Birla, Goa Campus, 403726 Goa, India<br>
‚Ä¢ Dataset: KAFO user Dataset, Samples: None, Modality: RGB videos + MoCap ground truth + metadata<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>RealDex: Towards Human-like Grasping for Robotic Dexterous Hand</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.13853"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dvlab.github.io/RealDex"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University, The University of Hong Kong<br>
‚Ä¢ Dataset: RealDex, Samples: 2630, Modality: Robotic hand kinematics, RGB-D images, point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>3D Kinematics Estimation from Video with a Biomechanical Model and Synthetic Training Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.13172"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Vision Lab, Department of Intelligent Systems, Delft University of Technology<br>
‚Ä¢ Dataset: ODAH (OpenSim Driven Animated Human), Samples: 1132, Modality: Synthetic RGB videos + OpenSim kinematics (joint angles, body segment scales)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>PIP-Net: Pedestrian Intention Prediction in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.12810"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Transport Studies, Computer Vision and Machine Learning Group, University of Leeds<br>
‚Ä¢ Dataset: Urban-PIP, Samples: 1481, Modality: multi-camera videos, LiDAR, Radar, IMU<br>
‚Ä¢ Dataset: Frontal Urban-PIP, Samples: 184, Modality: single front-camera videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>Event-Based Motion Magnification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.11957"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://openimaginglab.github.io/emm/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory, The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: SM-ERGB (Sub-pixel Motion Event-RGB dataset), Samples: 10007, Modality: RGB videos + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>DivaTrack: Diverse Bodies and Motions from Acceleration-Enhanced Three-Point Trackers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.09211"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea Advanced Institue of Science and Technology<br>
‚Ä¢ Dataset: DivaTrack, Samples: 772 motion clips, Modality: IMU signals + MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>Sophia-in-Audition: Virtual Production with a Robot Performer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.06978"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University, LumiAni Technology<br>
‚Ä¢ Dataset: Sophia-in-Audition (SiA) dataset, Samples: 50, Modality: multi-view RGB videos, facial motion capture (MoCap) data, HDR environment maps, robot motor parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>Advancing Video Anomaly Detection: A Concise Review and a New Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.04857"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Australian National University<br>
‚Ä¢ Dataset: Multi-Scenario Anomaly Detection (MSAD), Samples: 720, Modality: RGB<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.04519"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://biodrone.aitestunion.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China.<br>
‚Ä¢ Dataset: BioDrone, Samples: 600, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>Capturing the Unseen: Vision-Free Facial Motion Capture Using Inertial Measurement Units</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.03944"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: IMU-ARKit dataset, Samples: 20 participants, Modality: IMU signals, RGB videos, audio, ARKit parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>Advancing Location-Invariant and Device-Agnostic Motion Activity Recognition on Wearable Devices</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.03714"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="www.to-be-added.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Apple, USA<br>
‚Ä¢ Dataset: MotionPrint, Samples: 408 million accelerometer samples, Modality: 3D IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>MSPM: A Multi-Site Physiological Monitoring Dataset for Remote Pulse, Respiration, and Blood Pressure Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.02224"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Notre Dame<br>
‚Ä¢ Dataset: MSPM, Samples: 103, Modality: multi-view RGB videos + NIR video + physiological signals (PPG, BP, SpO2)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>BVI-Lowlight: Fully Registered Benchmark Dataset for Low-Light Video Enhancement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.01970"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.21227/mzny-8c77"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visual Information Laboratory
University of Bristol<br>
‚Ä¢ Dataset: BVI-Lowlight, Samples: 40, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>MUSTAN: Multi-scale Temporal Context as Attention for Robust Video Foreground Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.00918"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AI-CoE, Jio Platforms Limited (JPL), India<br>
‚Ä¢ Dataset: Indoor Surveillance Dataset (ISD), Samples: 150538 frames, Modality: RGB videos + foreground masks + depth maps + normal maps + instance semantic maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2024</td>
  <td style="width:70%;"><strong>REACT: Two Datasets for Analyzing Both Human Reactions and Evaluative Feedback to Robots Over Time</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2402.00190"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yale-img/react"><img src="https://img.shields.io/github/stars/yale-img/react.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Yale University<br>
‚Ä¢ Dataset: REACT-Nao, Samples: 432, Modality: RGB videos + Head pose + Gaze trajectories + Facial landmarks + Facial Action Units (AUs) + Robot game actions<br>
‚Ä¢ Dataset: REACT-Shutter, Samples: 240, Modality: RGB videos + Head pose + Gaze trajectories + Facial landmarks + Facial Action Units (AUs) + Robot pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>Physical Priors Augmented Event-Based 3D Reconstruction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.17121"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Mercerai/PAEv3d"><img src="https://img.shields.io/github/stars/Mercerai/PAEv3d.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MICS Thrust, HKUST(GZ)<br>
‚Ä¢ Dataset: PAEv3d Dataset, Samples: 101 objects, Modality: event streams, RGB images, depth maps, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.15348"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Lab of CAD&CG, Zhejiang University<br>
‚Ä¢ Dataset: AniDress Dataset, Samples: 12, Modality: Multi-view RGB videos + garment masks + 3D body poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>Dataset and Benchmark: Novel Sensors for Autonomous Vehicle Perception</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.13853"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/umautobots/nsavp_tools"><img src="https://img.shields.io/github/stars/umautobots/nsavp_tools.svg?style=social&label=Star"></a><br><a href="https://umautobots.github.io/nsavp"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics Department, University of Michigan, Ann Arbor, MI 48109 USA<br>
‚Ä¢ Dataset: Novel Sensors for Autonomous Vehicle Perception (NSAVP), Samples: 10, Modality: 6-DoF ground truth poses, stereo event cameras, stereo thermal cameras, stereo monochrome cameras, stereo RGB cameras<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>Learning Dynamics from Multicellular Graphs with Deep Neural Networks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.12196"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/GuoLab-CellMechanics/GNN-collective-cell-dynamics"><img src="https://img.shields.io/github/stars/GuoLab-CellMechanics/GNN-collective-cell-dynamics.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.13988939"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering, Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA<br>
‚Ä¢ Dataset: MCF-10A cell monolayer dataset, Samples: 240, Modality: Time-lapsed microscopy videos of cell monolayers<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.10232"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jlogkim.github.io/parahome"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Seoul National University<br>
‚Ä¢ Dataset: ParaHome, Samples: 208 captures, Modality: Multi-view RGB, IMU-based motion capture (body suit, gloves), 3D parametric models (human SMPL-X, objects), text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>TUMTraf Event: Calibration and Fusion Resulting in a Dataset for Roadside Event-Based and RGB Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.08474"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://innovation-mobility.com/tumtraf-dataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Chair of Robotics, Artificial Intelligence and Real-time Systems, TUM School of Computation, Information and Technology, Technical University of Munich, Munich, Germany<br>
‚Ä¢ Dataset: TUMTraf Event Dataset, Samples: 4111, Modality: Event-based camera images, RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.08399"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Tsinghua-MARS-Lab/TACO"><img src="https://img.shields.io/github/stars/Tsinghua-MARS-Lab/TACO.svg?style=social&label=Star"></a><br><a href="https://taco2024.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: TACO, Samples: 2500, Modality: Multi-view RGB videos, Egocentric RGBD videos, 3D hand-object meshes, Motion capture data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.06578"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://akaneqwq.github.io/360DVD/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic and Computer Engineering, Peking University; Peking University Shenzhen Graduate School-Rabbitpre AIGC Joint Research Laboratory<br>
‚Ä¢ Dataset: WEB360, Samples: 2114, Modality: ERP videos + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>Big Data and Deep Learning in Smart Cities: A Comprehensive Dataset for AI-Driven Traffic Accident Detection and Computer Vision Systems</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.03587"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of IT, University of Cincinnati<br>
‚Ä¢ Dataset: Traffic Accident Detection Video Dataset, Samples: 5700, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>An Event-Oriented Diffusion-Refinement Method for Sparse Events Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.03153"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Automation, Tsinghua University<br>
‚Ä¢ Dataset: Self-Captured Dataset, Samples: 21355, Modality: event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.01885"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Codec Avatars Lab, Meta, Pittsburgh; University of California, Berkeley<br>
‚Ä¢ Dataset: Photorealistic conversational dataset, Samples: 8 hours, Modality: multi-view video, audio, precomputed joint angles, face expression codes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.01505"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HopLee6/Sports-QA"><img src="https://img.shields.io/github/stars/HopLee6/Sports-QA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computing and Information Systems, University of Melbourne.<br>
‚Ä¢ Dataset: Sports-QA, Samples: 5967, Modality: RGB videos + textual QA pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>Indoor Obstacle Discovery on Reflective Ground via Monocular Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.01445"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XuefengBUPT/IndoorObstacleDiscovery-RG"><img src="https://img.shields.io/github/stars/XuefengBUPT/IndoorObstacleDiscovery-RG.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Beijing University of Posts and Telecommunications, Beijing 100876, China<br>
‚Ä¢ Dataset: Obstacle on Reflective Ground (ORG), Samples: 223, Modality: RGB videos + Robot odometry/poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>3D Human Pose Perception from Egocentric Stereo Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.00889"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/UnrealEgo2/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, SIC<br>
‚Ä¢ Dataset: UnrealEgo2, Samples: 15207, Modality: Egocentric stereo fisheye videos + 3D joint poses + depth maps<br>
‚Ä¢ Dataset: UnrealEgo-RW, Samples: 591, Modality: Egocentric stereo fisheye videos + 3D joint poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2024</td>
  <td style="width:70%;"><strong>EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2401.00374"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pantomatrix.github.io/EMAGE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo<br>
‚Ä¢ Dataset: BEAT2 (BEAT-SMPLX-FLAME), Samples: 1762, Modality: MoCap (SMPL-X body and FLAME head parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>iKUN: Speak to Trackers without Retraining</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.16245"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dyhBUPT/iKUN"><img src="https://img.shields.io/github/stars/dyhBUPT/iKUN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The school of Artificial Intelligence, Beijing University of Posts and Telecommunications<br>
‚Ä¢ Dataset: Refer-Dance, Samples: 65, Modality: RGB videos + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Inter-X: Towards Versatile Human-Human Interaction Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.16051"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://liangxuy.github.io/inter-x/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University, Eastern Institute of Technology, Ningbo<br>
‚Ä¢ Dataset: Inter-X, Samples: 11388, Modality: MoCap, SMPL-X parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Get a Grip: Reconstructing Hand-Object Stable Grasps in Egocentric Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.15719"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zhifanzhu/Get-a-Grip"><img src="https://img.shields.io/github/stars/zhifanzhu/Get-a-Grip.svg?style=social&label=Star"></a><br><a href="https://zhifanzhu.github.io/getagrip"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, University of Bristol, UK<br>
‚Ä¢ Dataset: EPIC-Grasps, Samples: 2431, Modality: Egocentric RGB videos + 2D segmentation masks + stable grasp temporal annotations<br>
‚Ä¢ Dataset: ARCTIC-Grasps, Samples: 1303, Modality: RGB videos + 3D hand/object meshes + stable grasp temporal annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.15004"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mingyuan-zhang/FineMoGen"><img src="https://img.shields.io/github/stars/mingyuan-zhang/FineMoGen.svg?style=social&label=Star"></a><br><a href="https://mingyuan-zhang.github.io/projects/FineMoGen.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: HuMMan-MoGen, Samples: 2968, Modality: SMPL parameters (6D rotation representation + root translation) with fine-grained spatio-temporal text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>MACS: Mass Conditioned 3D Hand and Object Motion Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.14929"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vcai.mpi-inf.mpg.de/projects/MACS/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Informatics, SIC, VIA Research Center<br>
‚Ä¢ Dataset: Mass-conditioned 3D hand and object motion dataset (unnamed), Samples: 110000 frames, Modality: 3D reconstructed hand (GHUM model) and object motions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>3D Pose Estimation of Two Interacting Hands from a Monocular Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.14157"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/Ev2Hands/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Informatics, SIC; Saarland University, SIC<br>
‚Ä¢ Dataset: Ev2Hands-S, Samples: None, Modality: Synthetic event streams + 3D hand pose annotations + segmentation labels<br>
‚Ä¢ Dataset: Ev2Hands-R, Samples: 8, Modality: Event streams + RGB videos + 3D hand pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Deep Hybrid Camera Deblurring for Smartphone Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.13317"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cg.postech.ac.kr/research/HCDeblur"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: POSTECH<br>
‚Ä¢ Dataset: HCBlur, Samples: 9039, Modality: RGB images + burst RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Relightable and Animatable Neural Avatars from Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.12877"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wenbin-lin/RelightableAvatar"><img src="https://img.shields.io/github/stars/wenbin-lin/RelightableAvatar.svg?style=social&label=Star"></a><br><a href="https://wenbin-lin.github.io/RelightableAvatar-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Software and BNRist, Tsinghua University<br>
‚Ä¢ Dataset: Synthetic Relightable Avatar Dataset, Samples: 4, Modality: multi-view RGB videos + 3D geometry + material + lighting<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>MotionScript: Natural Language Descriptions for Expressive 3D Human Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.12634"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/pjyazdian/MotionScript"><img src="https://img.shields.io/github/stars/pjyazdian/MotionScript.svg?style=social&label=Star"></a><br><a href="https://pjyazdian.github.io/MotionScript"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computing Science, Simon Fraser University, Burnaby, BC, Canada<br>
‚Ä¢ Dataset: MotionScript Captions for HumanML3D, Samples: 1461, Modality: MoCap joints + structured text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>EVI-SAM: Robust, Real-time, Tightly-coupled Event-Visual-Inertial State Estimation and 3D Dense Mapping</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.11911"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arclab-hku/Event based VO-VIO-SLAM"><img src="https://img.shields.io/github/stars/arclab-hku/Event based VO-VIO-SLAM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: EVI-SAM Handheld Dataset, Samples: 6, Modality: monocular event camera (DAVIS346: events, grayscale frames, IMU), RGB-D camera (Intel D455)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Towards Effective Multi-Moving-Camera Tracking: A New Dataset and Lightweight Link Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.11035"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dhu-mmct.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Donghua University<br>
‚Ä¢ Dataset: MMCT, Samples: 32, Modality: RGB videos + pedestrian trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Global-Local MAV Detection under Challenging Conditions based on Appearance and Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.11008"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WestlakeIntelligentRobotics/GLAD"><img src="https://img.shields.io/github/stars/WestlakeIntelligentRobotics/GLAD.svg?style=social&label=Star"></a><br><a href="https://youtu.be/Tv473mAzHbU"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology at Zhejiang University and the School of Engineering at Westlake University, Hangzhou, China.<br>
‚Ä¢ Dataset: ARD-MAV, Samples: 60, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.10877"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zeqing-wang.github.io/Mimic/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: South China University of Technology, X-ERA.ai<br>
‚Ä¢ Dataset: 3D-HDTF, Samples: 220, Modality: audio-mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Multi-level Reasoning for Robotic Assembly: From Sequence Inference to Contact Selection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.10571"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: D4PAS (Dataset for Part Assembly Sequences), Samples: 84326, Modality: assembly trajectories, enumerated assembly sequences, viable contact points, part point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Learning-based Axial Video Motion Magnification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.09551"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://axial-momag.github.io/axial-momag/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Graduate School of AI, POSTECH<br>
‚Ä¢ Dataset: Axial Motion Magnification Synthetic Dataset, Samples: 100000, Modality: Synthetic RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.09245"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/OpenGVLab/DriveMLM"><img src="https://img.shields.io/github/stars/OpenGVLab/DriveMLM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong; OpenGVLab, Shanghai AI Laboratory<br>
‚Ä¢ Dataset: DriveMLM Dataset, Samples: 50000, Modality: Multi-view images, LiDAR point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Interactive Humanoid: Online Full-Body Motion Reaction Synthesis with Social Affordance Canonicalization and Forecasting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.08983"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yunzeliu.github.io/iHuman/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: HHI, Samples: 5000, Modality: MoCap<br>
‚Ä¢ Dataset: CoChair, Samples: 3000, Modality: MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.08869"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: Inertial andMulti-view Highly Dynamic human-object interactions Dataset (IMHD2), Samples: 295, Modality: Multi-view RGB videos, object-mounted IMU, ground-truth human and object meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>PhyOT: Physics-informed object tracking in surveillance cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.08650"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="smartwarehouse2023.cmkl.ac.th"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: Warehouse Dataset, Samples: 680, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Leveraging Driver Field-of-View for Multimodal Ego-Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.08558"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://meakbiyik.github.io/routeformer"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z ¬®urich<br>
‚Ä¢ Dataset: GEM, Samples: None, Modality: GPS trajectories, RGB videos (scene and first-person), gaze data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.08459"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://shivangi-aneja.github.io/projects/facetalk"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich<br>
‚Ä¢ Dataset: FaceTalk dataset, Samples: 1000, Modality: NPHM expression codes + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.07937"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: BOTH57M, Samples: 1384, Modality: SMPLH skeletal motion + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Improving the Robustness of 3D Human Pose Estimation: A Benchmark and Learning from Noisy Input</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.06797"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Illinois at Urbana-Champaign, USA<br>
‚Ä¢ Dataset: Human3.6M-C, Samples: None, Modality: Corrupted RGB videos + 3D poses<br>
‚Ä¢ Dataset: HumanEva-I-C, Samples: None, Modality: Corrupted RGB videos + 3D poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.06553"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/neu-vi/HOI-Diff"><img src="https://img.shields.io/github/stars/neu-vi/HOI-Diff.svg?style=social&label=Star"></a><br><a href="https://neu-vi.github.io/HOI-Diff/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northeastern University<br>
‚Ä¢ Dataset: BEHAVE (annotated), Samples: 1451, Modality: 3D HOI sequences (SMPL-H, object mesh, 6DoF poses) with text descriptions and affordance annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.06398"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vLAR-group/NVFi"><img src="https://img.shields.io/github/stars/vLAR-group/NVFi.svg?style=social&label=Star"></a><br><a href="https://vlar-group.github.io/NVFi.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: vLAR Group, The Hong Kong Polytechnic University<br>
‚Ä¢ Dataset: Dynamic Object Dataset, Samples: 6, Modality: Multi-view RGB videos<br>
‚Ä¢ Dataset: Dynamic Indoor Scene Dataset, Samples: 4, Modality: Multi-view RGB videos with segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Wild Motion Unleashed: Markerless 3D Kinematics and Force Estimation in Cheetahs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.05879"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/African-Robotics-Unit/torque-estimation"><img src="https://img.shields.io/github/stars/African-Robotics-Unit/torque-estimation.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Cape Town, Department of Electrical Engineering, Cape Town, 7700, South Africa<br>
‚Ä¢ Dataset: Kinetic Dataset, Samples: 5, Modality: grayscale videos + synchronised force plate data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.04867"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://handdiffuse.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University, School of Information Science and Technology, Shanghai, China<br>
‚Ä¢ Dataset: HandDiffuse12.5M, Samples: 250K temporal frames, Modality: RGB videos, 2D/3D keypoints, MANO poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Sim-to-Real Causal Transfer: A Metric Learning Approach to Causally-Aware Interaction Representations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.04540"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vita-epfl/CausalSim2Real"><img src="https://img.shields.io/github/stars/vita-epfl/CausalSim2Real.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: √âcole Polytechnique F√©d√©rale de Lausanne (EPFL)<br>
‚Ä¢ Dataset: Sim-to-Real Causal Transfer Diagnostic Dataset, Samples: 26000, Modality: Simulated 2D agent trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.04524"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rehg-lab/RAVE"><img src="https://img.shields.io/github/stars/rehg-lab/RAVE.svg?style=social&label=Star"></a><br><a href="https://rave-video.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Georgia Tech<br>
‚Ä¢ Dataset: RAVE evaluation dataset, Samples: 186, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>SingingHead: A Large-scale 4D Dataset for Singing Head Animation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.04369"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wsj-sjtu/SingingHead"><img src="https://img.shields.io/github/stars/wsj-sjtu/SingingHead.svg?style=social&label=Star"></a><br><a href="https://wsj-sjtu.github.io/SingingHead/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: SingingHead, Samples: 1952, Modality: RGB videos + 3D facial motion (FLAME parameters) + audio + background music<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Natural-language-driven Simulation Benchmark and Copilot for Efficient Production of Object Interactions in Virtual Road Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.04008"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tianjin University<br>
‚Ä¢ Dataset: Language-to-Interaction (L2I), Samples: 120000, Modality: Natural language descriptions paired with Python code for generating vehicle and pedestrian trajectories in virtual road scenes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Low-power, Continuous Remote Behavioral Localization with Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.03799"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tub-rip.github.io/eventpenguins/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit√§t Berlin<br>
‚Ä¢ Dataset: EventPenguins, Samples: 24, Modality: Events + Grayscale frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.02963"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: FNii, CUHKSZ; SSE, CUHKSZ<br>
‚Ä¢ Dataset: MVHumanNet, Samples: 60000, Modality: Multi-view RGB videos, human masks, camera parameters, 2D/3D keypoints, SMPL/SMPLX parameters, textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Neural Sign Actors: A diffusion model for 3D sign language production from text</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.02702"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/baltatzisv/neural-sign-actors"><img src="https://img.shields.io/github/stars/baltatzisv/neural-sign-actors.svg?style=social&label=Star"></a><br><a href="https://baltatzisv.github.io/neural-sign-actors/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Imperial College London<br>
‚Ä¢ Dataset: How2Sign (SMPL-X annotations), Samples: 35000, Modality: SMPL-X parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Revisit Human-Scene Interaction via Space Occupancy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.02700"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Foruck/Occupancy-Motion-Controller"><img src="https://img.shields.io/github/stars/Foruck/Occupancy-Motion-Controller.svg?style=social&label=Star"></a><br><a href="https://foruck.github.io/occu-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: Motion Occupancy Base (MOB), Samples: 98000, Modality: SMPL(-X) motion parameters + Voxelized Occupancy Volume<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.02134"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/aipixel/GaussianAvatar"><img src="https://img.shields.io/github/stars/aipixel/GaussianAvatar.svg?style=social&label=Star"></a><br><a href="https://github.com/aipixel/GaussianAvatar"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Harbin Institute of Technology<br>
‚Ä¢ Dataset: DynVideo, Samples: 2, Modality: RGB videos + SMPL parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>DPHMs: Diffusion Parametric Head Models for Depth-based Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.01068"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tangjiapeng.github.io/projects/DPHMs"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich<br>
‚Ä¢ Dataset: DPHM-Kinect Dataset, Samples: 130, Modality: monocular depth sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>UAVs and Birds: Enhancing Short-Range Navigation through Budgerigar Flight Studies</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.00597"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: American International University -Bangladesh<br>
‚Ä¢ Dataset: Budges355, Samples: 355, Modality: Stereo RGB videos + 2D annotations + 3D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Event Recognition in Laparoscopic Gynecology Videos with Hybrid Transformers</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.00593"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ftp.itec.aau.at/datasets/LapGyn6-Events/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Information Technology (ITEC), Klagenfurt University, Austria<br>
‚Ä¢ Dataset: LapGyn6-Events, Samples: 174, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2023</td>
  <td style="width:70%;"><strong>Event-based Continuous Color Video Decompression from Single Frames</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2312.00113"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.cis.upenn.edu/~ziyunw/continuitycam/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Pennsylvania, USA.<br>
‚Ä¢ Dataset: Event Extreme Decompression Dataset (E2D2), Samples: , Modality: RGB images + events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Motion-Conditioned Image Animation for Video Editing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.18827"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="facebookresearch.github.io/MoCA"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: GenAI, Meta, UC Berkeley<br>
‚Ä¢ Dataset: VideoEdit benchmark (custom subset), Samples: 117, Modality: RGB videos + text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>E2PNet: Event to Point Cloud Registration with Spatio-Temporal Representation Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.18433"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xmu-qcj/E2PNet"><img src="https://img.shields.io/github/stars/xmu-qcj/E2PNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fujian Key Lab of Sensing and Computing for Smart Cities, School of Informatics, Xiamen University (XMU), China.<br>
‚Ä¢ Dataset: MVSEC-E2P, Samples: None, Modality: LiDAR, event camera, camera poses<br>
‚Ä¢ Dataset: VECtor-E2P, Samples: None, Modality: LiDAR, event camera, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>OmniMotionGPT: Animal Motion Generation with Limited Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.18303"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Arizona State University<br>
‚Ä¢ Dataset: AnimalML3D, Samples: 1240, Modality: 3D skeletal motion + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks, Methods, and Applications</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.18168"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Apple<br>
‚Ä¢ Dataset: VoxCeleb2-DECA, Samples: >1000000, Modality: 3D facial motion sequences + audio<br>
‚Ä¢ Dataset: VoxCeleb2-SPECTRE, Samples: >1000000, Modality: 3D facial motion sequences + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.17948"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hcis-lab/Action-slot"><img src="https://img.shields.io/github/stars/hcis-lab/Action-slot.svg?style=social&label=Star"></a><br><a href="https://hcis-lab.github.io/Action-slot/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Yang Ming Chiao Tung University<br>
‚Ä¢ Dataset: TACO, Samples: 5178, Modality: RGB videos + instance segmentation<br>
‚Ä¢ Dataset: nuScenes (annotated), Samples: 426, Modality: RGB videos + new atomic activity annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>AgentAvatar: Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.17465"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dorniwang.github.io/AgentAvatar_project/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xiaobing.AI<br>
‚Ä¢ Dataset: Fine-grained CelebvMotion Description Dataset (FilmData), Samples: 8978, Modality: RGB videos + fine-grained text descriptions of facial motion<br>
‚Ä¢ Dataset: EnvPersona, Samples: 1200, Modality: text descriptions (environment, persona, and planned facial motion)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.17057"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vcai.mpi-inf.mpg.de/projects/remos"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: German Research Center for Artificial Intelligence (DFKI)<br>
‚Ä¢ Dataset: ReMoCap, Samples: 275700 frames, Modality: multiview RGB videos + 3D full-body motion capture (including finger articulations)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Egocentric Whole-Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.16495"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://people.mpi-inf.mpg.de/jianwang/projects/egowholemocap/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: EgoWholeBody, Samples: 2629, Modality: Synthetic fisheye RGB images, depth maps, and 3D whole-body pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Dual-Stream Attention Transformers for Sewer Defect Classification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.16145"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/RedwanNewaz/ds mshvit"><img src="https://img.shields.io/github/stars/RedwanNewaz/ds mshvit.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of New Orleans<br>
‚Ä¢ Dataset: Carencro dataset, Samples: 50000, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>GAIA: Zero-shot Talking Avatar Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.15230"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://microsoft.github.io/GAIA"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Microsoft<br>
‚Ä¢ Dataset: GAIA Talking Avatar Dataset, Samples: 15,969 unique speakers / 1,169 hours, Modality: RGB videos with speech, facial landmarks, and head poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Dynamic Compositional Graph Convolutional Network for Efficient Composite Human Motion Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.13781"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WanyingZhang/DCGCN"><img src="https://img.shields.io/github/stars/WanyingZhang/DCGCN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sun Yat-Sen University<br>
‚Ä¢ Dataset: CHAMP, Samples: None, Modality: 3D joint coordinates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Intelligent Knee Sleeves: A Real-time Multimodal Dataset for 3D Lower Body Motion Estimation Using Smart Textile</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.12829"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Zhang-Wenwen/IntelligentKneeBrace"><img src="https://img.shields.io/github/stars/Zhang-Wenwen/IntelligentKneeBrace.svg?style=social&label=Star"></a><br><a href="https://feel.ece.ubc.ca/smartkneesleeve/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, University of British Columbia<br>
‚Ä¢ Dataset: Intelligent Knee Sleeves Dataset, Samples: 140000, Modality: Wearable sensors (IMU, pressure sensors), MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>MaskFlow: Object-Aware Motion Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.12476"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Imagination Technologies<br>
‚Ä¢ Dataset: MaskFlow dataset, Samples: 31550, Modality: RGB image pairs + optical flow ground truth + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Event Camera Data Dense Pre-training</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.11533"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: BDSI, Australian National University, Canberra, Australia<br>
‚Ä¢ Dataset: E-TartanAir, Samples: 1037 sequences, Modality: Event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Implicit Event-RGBD Neural SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.11013"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://delinqu.github.io/EN-SLAM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: DEV-Indoors, Samples: 9, Modality: RGB images, depth maps, event streams, ground truth meshes, ground truth trajectories<br>
‚Ä¢ Dataset: DEV-Reals, Samples: 8, Modality: RGBD, event streams, LiDAR-based ground truth trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>FOCAL: A Cost-Aware Video Dataset for Active Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.10591"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/olivesgatech/FOCAL"><img src="https://img.shields.io/github/stars/olivesgatech/FOCAL.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/records/10145325"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Georgia Institute of Technology<br>
‚Ä¢ Dataset: FOCAL, Samples: 126, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Video-based Sequential Bayesian Homography Estimation for Soccer Field Registration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.10361"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Paulkie99/KeypointAnnotator"><img src="https://img.shields.io/github/stars/Paulkie99/KeypointAnnotator.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical, Electronic and Computer Engineering, University of Pretoria, Lynnwood Road, Hatfield, Pretoria, 0028, South Africa<br>
‚Ä¢ Dataset: consolidated and refined WorldCup (CARWC), Samples: 4207, Modality: RGB images + homography annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Amodal Optical Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.07761"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://amodal-flow.cs.uni-freiburg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of Freiburg, Germany<br>
‚Ä¢ Dataset: AmodalSynthDrive, Samples: 150, Modality: RGB videos + amodal optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Sounding Bodies: Modeling 3D Spatial Sound of Humans Using Body Pose and Audio</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.06285"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/SoundingBodies"><img src="https://img.shields.io/github/stars/facebookresearch/SoundingBodies.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory<br>
‚Ä¢ Dataset: Sounding Bodies, Samples: 15822, Modality: 3D body pose + spatial audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Social Motion Prediction with Cognitive Hierarchies</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.04726"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/walter0807/Social-CH"><img src="https://img.shields.io/github/stars/walter0807/Social-CH.svg?style=social&label=Star"></a><br><a href="https://walter0807.github.io/Social-CH/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center on Frontiers of Computing Studies, School of Computer Science, Peking University<br>
‚Ä¢ Dataset: Wusi, Samples: 60K frames, Modality: 3D multi-person motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Exploring Event-based Human Pose Estimation with 3D Event Representations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.04591"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MasterHow/EventPointPose"><img src="https://img.shields.io/github/stars/MasterHow/EventPointPose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, Zhejiang University, Hangzhou 310027, China<br>
‚Ä¢ Dataset: EV-3DPW, Samples: 23,475 train samples and 40,145 test samples, Modality: synthetic events, RGB videos, 2D pose annotations<br>
‚Ä¢ Dataset: EV-JAAD, Samples: derived from 346 video clips, Modality: synthetic events, human bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Scalable and Efficient Continual Learning from Demonstration via a Hypernetwork-generated Stable Dynamics Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.03600"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sayantanauddy/clfd-snode"><img src="https://img.shields.io/github/stars/sayantanauddy/clfd-snode.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of Innsbruck, Austria<br>
‚Ä¢ Dataset: High-dimensional LASA, Samples: 210, Modality: High-dimensional trajectories<br>
‚Ä¢ Dataset: RoboTasks9, Samples: 81, Modality: Robot end-effector position and orientation trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Unsupervised Region-Growing Network for Object Segmentation in Atmospheric Turbulence</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.03572"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/turb-research/DOST"><img src="https://img.shields.io/github/stars/turb-research/DOST.svg?style=social&label=Star"></a><br><a href="https://turb-research.github.io/DOST/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Clemson University<br>
‚Ä¢ Dataset: Dynamic Object Segmentation in Turbulence (DOST), Samples: 38, Modality: RGB videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Race Against the Machine: a Fully-annotated, Open-design Dataset of Autonomous and Piloted High-speed Flight</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.02667"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tii-racing/drone-racing-dataset"><img src="https://img.shields.io/github/stars/tii-racing/drone-racing-dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Autonomous Robotics Research Center of the Technology Innovation Institute, Abu Dhabi, United Arab Emirates; University of Bologna, Bologna, Italy<br>
‚Ä¢ Dataset: TII-RATM (Race Against the Machine), Samples: 30, Modality: MoCap poses, IMU data, RGB videos, control inputs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>MAAIP: Multi-Agent Adversarial Interaction Priors for imitation from fighting demonstrations for physics-based characters</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.02502"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Inria, IRISA, University of Rennes, France<br>
‚Ä¢ Dataset: Boxing fighting interactions dataset, Samples: None, Modality: MoCap landmarks<br>
‚Ä¢ Dataset: QwanKiDo fighting interactions dataset, Samples: None, Modality: MoCap landmarks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>LocoMuJoCo: A Comprehensive Imitation Learning Benchmark for Locomotion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.02496"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/robfiras/loco-mujoco"><img src="https://img.shields.io/github/stars/robfiras/loco-mujoco.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Intelligent Autonomous Systems Group, TU Darmstadt<br>
‚Ä¢ Dataset: LocoMuJoCo, Samples: 27, Modality: Motion capture (kinematic trajectories), robot joint states and actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>ECMD: An Event-Centric Multisensory Driving Dataset for SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.02327"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://arclab-hku.github.io/ecmd/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: ECMD, Samples: 81, Modality: Stereo event cameras, Stereo RGB cameras, Infrared camera, LiDAR, IMU, GNSS-RTK/INS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.02191"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://upc-virvig.github.io/SparsePoser"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universitat Polit√®cnica de Catalunya, Spain<br>
‚Ä¢ Dataset: VR-specific motion capture database, Samples: 2000000 poses, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.02077"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/NVlabs/EmerNeRF"><img src="https://img.shields.io/github/stars/NVlabs/EmerNeRF.svg?style=social&label=Star"></a><br><a href="https://emernerf.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Southern California<br>
‚Ä¢ Dataset: NeRF On-The-Road (NOTR), Samples: 120, Modality: RGB videos, LiDAR, 3D scene flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Joint 3D Shape and Motion Estimation from Rolling Shutter Light-Field Images</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.01292"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ICB-Vision-AI/RSLF"><img src="https://img.shields.io/github/stars/ICB-Vision-AI/RSLF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universit ¬¥e de Bourgogne, CNRS UMR 6303 ICB; Universit ¬¥e de Franche-Comt ¬¥e, CNRS UMR 6174 FEMTO-ST<br>
‚Ä¢ Dataset: Rolling Shutter Light Fields (RSLF), Samples: 77, Modality: Rolling Shutter Light-Field images with ground truth depth maps and camera motion parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Enhancing Traffic Object Detection in Variable Illumination with RGB-Event Fusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.00436"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/YN-Yang/SFNet"><img src="https://img.shields.io/github/stars/YN-Yang/SFNet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information Engineering, Chang‚Äôan University, Shaanxi, Xi‚Äôan 710000, China<br>
‚Ä¢ Dataset: DSEC-Det, Samples: 53, Modality: RGB, Event camera stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Event-based Background-Oriented Schlieren</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.00434"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/tub-rip/event-based-bos"><img src="https://img.shields.io/github/stars/tub-rip/event-based-bos.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technische Universit√§t Berlin, Berlin, Germany; Department of Electronics and Electrical Engineering, Faculty of Science and Technology, Keio University, Kanagawa, Japan<br>
‚Ä¢ Dataset: Schlieren Event-Frames Dataset, Samples: 9, Modality: Event camera data, grayscale frames, derived optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2023</td>
  <td style="width:70%;"><strong>Learning Cooperative Trajectory Representations for Motion Forecasting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2311.00371"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AIR-THU/V2X-Graph"><img src="https://img.shields.io/github/stars/AIR-THU/V2X-Graph.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for AI Industry Research (AIR), Tsinghua University<br>
‚Ä¢ Dataset: V2X-Traj, Samples: 10102, Modality: Trajectories derived from LiDAR and cameras<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.20436"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://signavatars.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Imperial College London<br>
‚Ä¢ Dataset: SignAvatars, Samples: 70000, Modality: RGB videos + SMPL-X/MANO parameters + 2D/3D keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Do we need scan-matching in radar odometry?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.18117"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kubelvla/mine-and-forest-radar-dataset"><img src="https://img.shields.io/github/stars/kubelvla/mine-and-forest-radar-dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AASS research centre at √ñrebro University, Sweden<br>
‚Ä¢ Dataset: mine-and-forest-radar-dataset, Samples: 2, Modality: 4D Radar, IMU, LiDAR, Camera<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.17462"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kiedani/Towards-Learning-Monocular-3D-Object-Localization-From-2D-Labels-Using-the-Physical-Laws-of-Motion"><img src="https://img.shields.io/github/stars/kiedani/Towards-Learning-Monocular-3D-Object-Localization-From-2D-Labels-Using-the-Physical-Laws-of-Motion.svg?style=social&label=Star"></a><br><a href="https://kiedani.github.io/3DV2024/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Augsburg<br>
‚Ä¢ Dataset: Synthetic Dataset (SD), Samples: 7200, Modality: Synthetic RGB videos + 3D trajectories<br>
‚Ä¢ Dataset: Real Dataset (RD), Samples: 366, Modality: RGB videos + 3D trajectories<br>
‚Ä¢ Dataset: Spring Dataset, Samples: 300, Modality: Synthetic RGB videos + 3D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Automatic Edge Error Judgment in Figure Skating Using 3D Pose Estimation from a Monocular Camera and IMUs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.17193"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ryota-takedalab/JudgeAI-LutzEdge"><img src="https://img.shields.io/github/stars/ryota-takedalab/JudgeAI-LutzEdge.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nagoya University<br>
‚Ä¢ Dataset: Figure Skating Single Lutz Jump Dataset, Samples: 232, Modality: RGB videos, IMU, 3D pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Language-driven Scene Synthesis using Multi-conditional Diffusion Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.15948"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VinAIResearch/LSDM"><img src="https://img.shields.io/github/stars/VinAIResearch/LSDM.svg?style=social&label=Star"></a><br><a href="https://lang-scene-synth.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: FSOFT AI Center<br>
‚Ä¢ Dataset: PRO-teXt, Samples: 200, Modality: Human motions (poses) + 3D scenes (point clouds) + Text prompts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Object Pose Estimation Annotation Pipeline for Multi-view Monocular Camera Systems in Industrial Settings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.14914"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://anonymous.4open.science/r/bop_toolkit-6F86"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Dortmund University, Dortmund, Germany<br>
‚Ä¢ Dataset: Multi-log, Samples: 26500, Modality: RGB images + 6D object poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>PACE: Human and Camera Motion Estimation from in-the-wild Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.13768"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nvlabs.github.io/PACE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA, Max Planck Institute for Intelligent Systems, Tubingen, Germany, ETH Zurich, Switzerland<br>
‚Ä¢ Dataset: HCM (Human and Camera Motion) dataset, Samples: 25, Modality: RGB videos + ground-truth 3D human motion + ground-truth camera motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.13258"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://portal-cornell.github.io/manicast/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Cornell University<br>
‚Ä¢ Dataset: Collaborative Manipulation Dataset (CoMaD), Samples: 61, Modality: Motion Capture + RGB video + Audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Simultaneous Learning of Contact and Continuous Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.12054"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mhalm/continuous-contact-nets"><img src="https://img.shields.io/github/stars/mhalm/continuous-contact-nets.svg?style=social&label=Star"></a><br><a href="https://sites.google.com/view/continuous-contact-nets/home"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: GRASP Laboratory, University of Pennsylvania<br>
‚Ä¢ Dataset: Articulated Object Toss Trajectories, Samples: over 500, Modality: Pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>MOCHA: Real-Time Motion Characterization via Context Matching</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.10079"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST and MOVIN Inc.
South Korea<br>
‚Ä¢ Dataset: MOCHA character motion dataset, Samples: 573k frames, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>MoEmo Vision Transformer: Integrating Cross-Attention and Movement Vectors in 3D Pose Estimation for HRI Emotion Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.09757"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Santa Clara University<br>
‚Ä¢ Dataset: Naturalistic Motion Database, Samples: 1512, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Dynamic Appearance Particle Neural Radiance Field</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.07916"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Cenbylin/DAP-NeRF"><img src="https://img.shields.io/github/stars/Cenbylin/DAP-NeRF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, Australian Artificial Intelligence Institute (AAII), University of Technology Sydney, Sydney, NSW 2007, Australia<br>
‚Ä¢ Dataset: Motion Modeling Evaluation Dataset, Samples: 3, Modality: Synthetic RGB videos + Ground-truth 3D velocity fields<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Saturation-Aware Angular Velocity Estimation: Extending the Robustness of SLAM to Aggressive Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.07844"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/norlab-ulaval/Norlab_wiki/wiki/TIGS-Dataset"><img src="https://img.shields.io/github/stars/wiki/TIGS-Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northern Robotics Laboratory, Universit√© Laval, Quebec City, Quebec, Canada<br>
‚Ä¢ Dataset: TIGS, Samples: 32, Modality: Lidar, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Towards More Efficient Depression Risk Recognition via Gait</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.06283"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Artificial Intelligence, Beijing Normal University, Beijing 100875, China<br>
‚Ä¢ Dataset: Gait-based Depression Risk Recognition Dataset, Samples: 40281, Modality: gait silhouettes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.05934"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Image and Video Systems Lab, KAIST<br>
‚Ä¢ Dataset: 3D-HDTF, Samples: 10000, Modality: 3D face mesh sequences + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Universal Humanoid Motion Representations for Physics-Based Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.04582"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zhengyiluo/pulse"><img src="https://img.shields.io/github/stars/zhengyiluo/pulse.svg?style=social&label=Star"></a><br><a href="https://zhengyiluo.github.io/PULSE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Reality Labs Research, Meta; Carnegie Mellon University<br>
‚Ä¢ Dataset: Cleaned AMASS dataset, Samples: 11313 training sequences, 138 testing sequences, Modality: MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>SwimXYZ: A large-scale dataset of synthetic swimming motions and videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.04360"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://g-fiche.github.io/research-pages/swimxyz/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CentraleSup√©lec, IETR UMR CNRS 6164, France<br>
‚Ä¢ Dataset: SwimXYZ, Samples: 240, Modality: SMPL parameters, synthetic RGB videos, 2D/3D joint annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>A Dataset of Anatomical Environments for Medical Robots: Modeling Respiratory Deformation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.04289"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UNC-Robotics/Med-RAD"><img src="https://img.shields.io/github/stars/UNC-Robotics/Med-RAD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of North Carolina at Chapel Hill<br>
‚Ä¢ Dataset: Medical Robotics Anatomical Dataset (Med-RAD), Samples: 3, Modality: 3D anatomical models (from CT) + 3D respiratory deformation fields<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>GroundLink: A Dataset Unifying Human Body Movement and Ground Reaction Dynamics</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.03930"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://csr.bu.edu/groundlink/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Science, Boston University<br>
‚Ä¢ Dataset: GroundLink, Samples: 368, Modality: MoCap markers, SMPL-X parameters, Ground Reaction Force (GRF), Center of Pressure (CoP)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.03205"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://neuface-dataset.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Electrical Engineering, POSTECH<br>
‚Ä¢ Dataset: NeuFace-dataset, Samples: 1245000, Modality: 3D face mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>A Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.02523"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://share.weiyun.com/9qOxwpqz"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Jinan University<br>
‚Ä¢ Dataset: STSCB, Samples: 44670, Modality: RGB videos + behavior annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>EvDNeRF: Reconstructing Event Data with Dynamic Neural Radiance Fields</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.02437"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/anish-bhattacharya/EvDNeRF"><img src="https://img.shields.io/github/stars/anish-bhattacharya/EvDNeRF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Pennsylvania<br>
‚Ä¢ Dataset: Simulated Dynamic Event Scenes (Jet-Down, Jet-Spiral, Jet-Land, Multi, Lego), Samples: 5, Modality: multi-view eventstreams, RGB frames<br>
‚Ä¢ Dataset: Real-World Dynamic Event Scenes (Real-Fork, Real-Knife), Samples: 2, Modality: multi-view eventstreams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>RSRD: A Road Surface Reconstruction Dataset and Benchmark for Safe and Comfortable Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.02262"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://thu-rsxd.com/rsrd/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Vehicle and Mobility, Tsinghua University<br>
‚Ä¢ Dataset: RSRD (Road Surface Reconstruction Dataset), Samples: 191, Modality: Stereo images, LiDAR point clouds, IMU, RTK (pose, location, velocity)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Visual Temporal Fusion Based Free Space Segmentation for Autonomous Surface Vessels</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.00879"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ORCA-Uboat, Shaanxi, 710075 China<br>
‚Ä¢ Dataset: video sequence dataset for ASVs free space segmentation, Samples: 10, Modality: video sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Propagating Semantic Labels in Video Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.00783"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dataverse.tdl.org/dataverse/robotics"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Departent of Computer Science, The University of Texas at Austin<br>
‚Ä¢ Dataset: not explicitly named, Samples: 5, Modality: RGB videos + semantic masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Music- and Lyrics-driven Dance Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.00455"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yy1lab/LMD"><img src="https://img.shields.io/github/stars/yy1lab/LMD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: JustLMD, Samples: 1867, Modality: 3D dance motion (24-joint SMPL format)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.00434"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://diffposetalk.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: BNRist, Tsinghua University, China<br>
‚Ä¢ Dataset: Talking Face with Head Poses (TFHP), Samples: 1052, Modality: Audio-visual videos + reconstructed 3DMM FLAME parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2023</td>
  <td style="width:70%;"><strong>Berkeley Open Extended Reality Recordings 2023 (BOXRR-23): 4.7 Million Motion Capture Recordings from 105,852 Extended Reality Device Users</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2310.00430"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/metaguard/xror"><img src="https://img.shields.io/github/stars/metaguard/xror.svg?style=social&label=Star"></a><br><a href="https://rdi.berkeley.edu/metaverse/boxrr-23/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: BOXRR-23, Samples: 4717215, Modality: 6DoF head and hand motion capture<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>EGVD: Event-Guided Video Deraining</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.17239"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/booker-max/EGVD"><img src="https://img.shields.io/github/stars/booker-max/EGVD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electronic Engineering and Information Science, University of Science and Technology of China<br>
‚Ä¢ Dataset: Rain-DAVIS, Samples: 10, Modality: RGB videos + event streams<br>
‚Ä¢ Dataset: N-NTURain, Samples: 33, Modality: RGB videos + event streams<br>
‚Ä¢ Dataset: N-GoproRain, Samples: 77, Modality: RGB videos + event streams<br>
‚Ä¢ Dataset: N-AdobeRainH, Samples: 128, Modality: RGB videos + event streams<br>
‚Ä¢ Dataset: N-AdobeRainL, Samples: 128, Modality: RGB videos + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>UniQuadric: A SLAM Backend for Unknown Rigid Object 3D Tracking and Light-Weight Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.17036"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Linghao-Yang/Synthesized-Multimotion-and-Modeling-Dataset"><img src="https://img.shields.io/github/stars/Linghao-Yang/Synthesized-Multimotion-and-Modeling-Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xi‚Äôan Precision Machinery Research Institute Kunming Branch, China<br>
‚Ä¢ Dataset: SMMD (Synthesized Multimotion and Modeling dataset), Samples: 6 sequences mentioned (Subway, car, Cuboid, Cube, SkateBoard, Ball), Modality: RGB images, depth images, simulated LiDAR data, camera pose, object pose, object scale<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>SpikeMOT: Event-based Multi-Object Tracking with Sparse Motion Features</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16987"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Electronic Engineering at The University of Hong Kong, Hong Kong, China<br>
‚Ä¢ Dataset: DSEC-MOT, Samples: 12, Modality: Event streams + RGB images + Bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>CrossZoom: Simultaneously Motion Deblurring and Event Super-Resolving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16949"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Bestrivenzc/CZ-Net"><img src="https://img.shields.io/github/stars/Bestrivenzc/CZ-Net.svg?style=social&label=Star"></a><br><a href="https://bestrivenzc.github.io/CZ-Net/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University<br>
‚Ä¢ Dataset: Cross-Resolution Deblurring and Resolving (CRDR), Samples: 76, Modality: RGB images (sharp, blurry) and event streams (HR, LR)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16940"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MediaBrain-SJTU/CoBEVFlow"><img src="https://img.shields.io/github/stars/MediaBrain-SJTU/CoBEVFlow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Cooperative Medianet Innovation Center, Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: IRregular V2V (IRV2V), Samples: 8449, Modality: LiDAR + RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Decaf: Monocular Deformation Capture for Face and Hand Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16670"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/soshishimada/Decaf_release"><img src="https://img.shields.io/github/stars/soshishimada/Decaf_release.svg?style=social&label=Star"></a><br><a href="https://vcai.mpi-inf.mpg.de/projects/Decaf"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Informatics, SIC, VIA Research Center, Germany<br>
‚Ä¢ Dataset: Decaf Dataset, Samples: 100K frames, Modality: RGB videos, 2D hand keypoints, 2D face landmarks, foreground segmentation masks, hand-face bounding boxes, 3D mesh for hand and face, 3D surface deformations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Deep Geometrized Cartoon Line Inbetweening</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16643"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lisiyao21/AnimeInbet"><img src="https://img.shields.io/github/stars/lisiyao21/AnimeInbet.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: MixamoLine240, Samples: 240, Modality: 2D line drawing sequences + vertex coordinates/topology + vertex correspondences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Radar Instance Transformer: Reliable Moving Instance Segmentation in Sparse Radar Point Clouds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16435"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CARIAD SE and University of Bonn<br>
‚Ä¢ Dataset: RadarScenes Moving Instance Segmentation Benchmark, Samples: 158, Modality: Radar point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Object Motion Guided Human Motion Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.16237"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University, USA<br>
‚Ä¢ Dataset: Unnamed (OMOMO dataset), Samples: approx. 10 hours duration, Modality: 3D object geometry, object motion, and full-body human motion (MoCap as SMPL-X parameters)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Interaction-Aware Sampling-Based MPC with Learned Local Goal Predictions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.14931"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://autonomousrobots.nl/pubpage/IA_MPPI_LBM.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Cognitive Robotics Department, TU Delft<br>
‚Ä¢ Dataset: Artificial Vessel Trajectory Dataset, Samples: 4696, Modality: position and velocity trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>AgriSORT: A Simple Online Real-time Tracking-by-Detection framework for robotics in precision agriculture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.13393"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sapienza University of Rome<br>
‚Ä¢ Dataset: Agricultural MOT benchmark for table grape vineyards, Samples: 4, Modality: RGB-D video sequences with MOT annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>CloudGripper: An Open Source Cloud Robotics Testbed for Robotic Manipulation Research, Benchmarking and Data Collection at Scale</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.12786"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cloudgripper.org"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology<br>
‚Ä¢ Dataset: CloudGripper-Rope-100, Samples: None, Modality: robot motion commands, RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>DIOR: Dataset for Indoor-Outdoor Reidentification -- Long Range 3D/2D Skeleton Gait Collection Pipeline, Semi-Automated Gait Keypoint Labeling and Baseline Evaluation Methods</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.12429"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Engineering, University at Buffalo, SUNY<br>
‚Ä¢ Dataset: DIOR, Samples: 112, Modality: RGB videos + 3D/2D skeleton labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>PanoVOS: Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.12303"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/shilinyan99/PanoVOS"><img src="https://img.shields.io/github/stars/shilinyan99/PanoVOS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University<br>
‚Ä¢ Dataset: PanoVOS, Samples: 150, Modality: Panoramic RGB videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>A Novel Deep Neural Network for Trajectory Prediction in Automated Vehicles Using Velocity Vector Field</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.10948"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Amir-Samadi/VVF-TP"><img src="https://img.shields.io/github/stars/Amir-Samadi/VVF-TP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Warwick Manufacturing Group (WMG), The University of Warwick<br>
‚Ä¢ Dataset: VVF dataset for highD, Samples: 110000, Modality: Velocity Vector Field (VVF) images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>DRIVE: Data-driven Robot Input Vector Exploration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.10718"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/norlab-ulaval/DRIVE"><img src="https://img.shields.io/github/stars/norlab-ulaval/DRIVE.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Northern Robotics Laboratory, Universit√© Laval, Quebec City, Quebec, Canada<br>
‚Ä¢ Dataset: DRIVE, Samples: 1.8 hours / 7 km of driving data, Modality: Robot kinematics (commands, velocities, pose trajectories), Point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Asteroids co-orbital motion classification based on Machine Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.10603"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IFAC-CNR, Istituto di Fisica Applicata ‚ÄúNello Carrara‚Äù, Consiglio Nazionale delle Ricerche, via Madonna del Piano 10, 50019 Sesto Fiorentino (FI), Italy<br>
‚Ä¢ Dataset: Co-orbital Asteroid Motion Dataset (Real), Samples: 50, Modality: Time series of asteroid orbital elements (resonant angle Œ∏)<br>
‚Ä¢ Dataset: Co-orbital Asteroid Motion Dataset (Ideal Simulated), Samples: 1999, Modality: Time series of asteroid orbital elements (resonant angle Œ∏)<br>
‚Ä¢ Dataset: Co-orbital Asteroid Motion Dataset (Perturbed Simulated), Samples: 347, Modality: Time series of asteroid orbital elements (resonant angle Œ∏)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Deep Visual Odometry with Events and Frames</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.09947"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/uzh-rpg/rampvo"><img src="https://img.shields.io/github/stars/uzh-rpg/rampvo.svg?style=social&label=Star"></a><br><a href="https://github.com/uzh-rpg/rampvo"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Robotics and Perception Group, University of Zurich, Switzerland<br>
‚Ä¢ Dataset: Apollo landing, Samples: 6, Modality: real RGB images, event camera data, MoCap pose trajectories<br>
‚Ä¢ Dataset: Malapert landing, Samples: 2, Modality: simulated RGB images, synthetic events, pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Sparse and Privacy-enhanced Representation for Human Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.09515"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://lyhsieh.github.io/sphp/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Vision Science Lab, National Tsing Hua University<br>
‚Ä¢ Dataset: SPHP (Sparse and Privacy-enhanced Dataset for Human Pose Estimation), Samples: 640, Modality: edge images, two-directional motion vector images, grayscale images, 2D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Learning Parallax for Stereo Event-based Motion Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.09513"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Mingyuan-Lin/St-EDNet"><img src="https://img.shields.io/github/stars/Mingyuan-Lin/St-EDNet.svg?style=social&label=Star"></a><br><a href="https://mingyuan-lin.github.io/St-EDweb/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University, Wuhan 430072, China<br>
‚Ä¢ Dataset: StEIC, Samples: 65, Modality: RGB-D images, Event streams, Disparity maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>MOVIN: Real-time Motion Capture using a Single LiDAR</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.09314"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://movin3d.github.io/movin_pg2023/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MOVIN Inc., Korea Advanced Institute of Science and Technology (KAIST)<br>
‚Ä¢ Dataset: MOVIN dataset, Samples: 161179, Modality: LiDAR point cloud and optical motion capture data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>RMP: A Random Mask Pretrain Framework for Motion Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.08989"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/KTH-RPL/RMP"><img src="https://img.shields.io/github/stars/KTH-RPL/RMP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KTH Royal Institute of Technology, Scania CV AB<br>
‚Ä¢ Dataset: Post-processed INTERACTION dataset, Samples: None, Modality: Agent trajectories with occlusion labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.08596"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wengflow/robust-e-nerf"><img src="https://img.shields.io/github/stars/wengflow/robust-e-nerf.svg?style=social&label=Star"></a><br><a href="https://wengflow.github.io/robust-e-nerf"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The NUS Graduate School‚Äôs Integrative Sciences and Engineering Programme (ISEP)<br>
‚Ä¢ Dataset: Robust e-NeRF Synthetic Event Dataset, Samples: 7 scenes with multiple sequences generated under varying conditions, Modality: Event stream + camera pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.08588"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fabiendelattre.com/robust-rotation-estimation"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Massachusetts Amherst<br>
‚Ä¢ Dataset: BUsy Street Scenes (BUSS), Samples: 17, Modality: RGB videos + IMU + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Learning Quasi-Static 3D Models of Markerless Deformable Linear Objects for Bimanual Robotic Manipulation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.07609"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/PPI-PUT/neural_dlo_model"><img src="https://img.shields.io/github/stars/PPI-PUT/neural_dlo_model.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Robotics and Machine Intelligence, Poznan University of Technology, Poznan, Poland<br>
‚Ä¢ Dataset: two-wire cable DLO dataset (50 cm), Samples: 54080, Modality: Robot end-effector poses and DLO 3D point sequences from RGBD<br>
‚Ä¢ Dataset: two-wire cable DLO dataset (45 cm), Samples: 14606, Modality: Robot end-effector poses and DLO 3D point sequences from RGBD<br>
‚Ä¢ Dataset: two-wire cable DLO dataset (40 cm), Samples: 13264, Modality: Robot end-effector poses and DLO 3D point sequences from RGBD<br>
‚Ä¢ Dataset: solar cable DLO dataset (50 cm), Samples: 5344, Modality: Robot end-effector poses and DLO 3D point sequences from RGBD<br>
‚Ä¢ Dataset: braided cable DLO dataset (50 cm), Samples: 6136, Modality: Robot end-effector poses and DLO 3D point sequences from RGBD<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>STUPD: A Synthetic Dataset for Spatial and Temporal Relation Reasoning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.06680"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Frontier AI Research, Agency for Science, Technology, and Research, Singapore<br>
‚Ä¢ Dataset: STUPD (Spatial and Temporal Understanding of Prepositions Dataset), Samples: 200000, Modality: RGB videos + 3D coordinates + bounding boxes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>STAR-loc: Dataset for STereo And Range-based localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.05518"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/utiasASRL/starloc"><img src="https://img.shields.io/github/stars/utiasASRL/starloc.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto<br>
‚Ä¢ Dataset: STAR-loc, Samples: 22, Modality: Stereo images, IMU, UWB range measurements, Vicon MoCap<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>HiLM-D: Enhancing MLLMs with Multi-Scale High-Resolution Details for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.05186"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://usa.honda-ri.com/drama"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China.<br>
‚Ä¢ Dataset: DRAMA-ROLISP, Samples: 17785, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>FreeMan: Towards Benchmarking 3D Human Pose Estimation under Real-World Conditions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.05073"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/IDEA-Research/deepdataspace"><img src="https://img.shields.io/github/stars/IDEA-Research/deepdataspace.svg?style=social&label=Star"></a><br><a href="https://wangjiongw.github.io/freeman"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong, Shenzhen, Tencent<br>
‚Ä¢ Dataset: FreeMan, Samples: 8000, Modality: RGB videos + 2D/3D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Stereo Matching in Time: 100+ FPS Video Stereo Matching for Extended Reality</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.04183"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tencent XR Vision Labs<br>
‚Ä¢ Dataset: XR-Stereo, Samples: 17, Modality: RGB videos + 6-DoF camera trajectories + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Leveraging Geometrical Acoustic Simulations of Spatial Room Impulse Responses for Improved Sound Event Detection and Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.03337"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ChrisIck/DCASE_Synth_Data"><img src="https://img.shields.io/github/stars/ChrisIck/DCASE_Synth_Data.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Music and Audio Research Laboratory, New York University<br>
‚Ä¢ Dataset: SIM-SRIR, Samples: 38530, Modality: Simulated Spatial Room Impulse Responses (SRIRs) along motion trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.01372"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/axdfhj/MDD"><img src="https://img.shields.io/github/stars/axdfhj/MDD.svg?style=social&label=Star"></a><br><a href="https://github.com/axdfhj/MDD"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: WildMotion-Caption (WMC), Samples: 8888, Modality: SMPL-based skeleton keypoints and motion features<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>BodySLAM++: Fast and Tightly-Coupled Visual-Inertial Camera and Human Motion Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.01236"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Smart Robotics Lab, Dept. of Computing, Imperial College London, UK<br>
‚Ä¢ Dataset: BodySLAM dataset, Samples: 30, Modality: Stereo visual-inertial data, ground truth 6D camera poses, ground truth 3D human joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>An Asynchronous Linear Filter Architecture for Hybrid Event-Frame Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.01159"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ziweiwwang/Event-Asynchronous-Filter"><img src="https://img.shields.io/github/stars/ziweiwwang/Event-Asynchronous-Filter.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Systems Theory and Robotics (STR) Group, College of Engineering and Computer Science, Australian National University, Canberra, ACT 2601, Australia<br>
‚Ä¢ Dataset: HDR Hybrid Event-Frame Dataset, Samples: 6, Modality: Stereo hybrid event-frame camera (event camera data, LDR RGB videos, HDR reference images)<br>
‚Ä¢ Dataset: AHDR (Artificial HDR) Dataset, Samples: 4, Modality: Stereo hybrid event-frame camera (event camera data, simulated LDR videos, HDR reference images)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2023</td>
  <td style="width:70%;"><strong>Dense Voxel 3D Reconstruction Using a Monocular Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2309.00385"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science, The University of Sydney<br>
‚Ä¢ Dataset: None, Samples: 39739, Modality: simulated event data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.16894"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ait.ethz.ch/emdb"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Z√ºrich, Department of Computer Science<br>
‚Ä¢ Dataset: EMDB, Samples: 81, Modality: RGB-D videos, EM sensor measurements, 3D SMPL poses, shapes, and global trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.16876"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://neu-vi.github.io/SportsSlomo/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC San Diego<br>
‚Ä¢ Dataset: SportsSloMo, Samples: 131464, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Learning to Predict 3D Rotational Dynamics from Images of a Rigid Body with Unknown Mass Distribution</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.14666"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical and Aerospace Engineering, Princeton University, Princeton, NJ 08544, USA; The Aerospace Corporation, El Segundo, CA 90245, USA<br>
‚Ä¢ Dataset: Uniform mass density cube, Samples: 1000, Modality: synthetic images<br>
‚Ä¢ Dataset: Uniform mass density prism, Samples: 1000, Modality: synthetic images<br>
‚Ä¢ Dataset: Non-uniform mass density cube, Samples: 1000, Modality: synthetic images<br>
‚Ä¢ Dataset: Non-uniform mass density prism, Samples: 1000, Modality: synthetic images<br>
‚Ä¢ Dataset: Uniform density synthetic-satellites (CALIPSO), Samples: 1000, Modality: synthetic images<br>
‚Ä¢ Dataset: Uniform density synthetic-satellites (CloudSat), Samples: 1000, Modality: synthetic images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>LAC: Latent Action Composition for Skeleton-based Action Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.14500"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/walker1126/LAC"><img src="https://img.shields.io/github/stars/walker1126/LAC.svg?style=social&label=Star"></a><br><a href="https://walker1126.github.io/LAC/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Inria, Universit√© C√¥te d‚ÄôAzur<br>
‚Ä¢ Dataset: Charades, Samples: None, Modality: 2D skeleton data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>4D Myocardium Reconstruction with Decoupled Motion and Shape Model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.14083"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yuan-xiaohan/4D-Myocardium-Reconstruction-with-Decoupled-Motion-and-Shape-Model"><img src="https://img.shields.io/github/stars/yuan-xiaohan/4D-Myocardium-Reconstruction-with-Decoupled-Motion-and-Shape-Model.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southeast University, China<br>
‚Ä¢ Dataset: 4D myocardial dataset, Samples: 55, Modality: cine magnetic resonance (CMR) derived 3D shape sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>A Conflict Resolution Dataset Derived from Argoverse-2: Analysis of the Safety and Efficiency Impacts of Autonomous Vehicles at Intersections</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.13839"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/RomainLITUD/conflict_resolution_dataset"><img src="https://img.shields.io/github/stars/RomainLITUD/conflict_resolution_dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Transport and Planning, Civil Engineering and Geosciences, Delft University of Technology<br>
‚Ä¢ Dataset: Conflict Resolution Dataset, Samples: 21431, Modality: Processed trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Dance with You: The Diversity Controllable Dancer Generation via Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.13551"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JJessicaYao/AIST-M-Dataset"><img src="https://img.shields.io/github/stars/JJessicaYao/AIST-M-Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Chinese University of Hong Kong, Shenzhen<br>
‚Ä¢ Dataset: AIST-M, Samples: 340 lead-partner dancer pairs, Modality: 2D keypoints, 3D keypoints, SMPL skeletons, music<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>AccFlow: Backward Accumulation for Long-Range Optical Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.13133"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/mulns/AccFlow"><img src="https://img.shields.io/github/stars/mulns/AccFlow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: CVO, Samples: 12000, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>ROAM: Robust and Object-Aware Motion Generation Using Neural Pose Descriptors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.12969"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vcai.mpi-inf.mpg.de/projects/ROAM/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, Saarland Informatics Campus<br>
‚Ä¢ Dataset: ROAM Dataset, Samples: around 90 minutes of motion, Modality: Skeletal motion from markerless motion capture using multi-view RGB cameras, with foot contact and phase labels.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>The GENEA Challenge 2023: A large scale evaluation of gesture generation models in monadic and dyadic settings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.12646"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="svito-zar.github.io/GENEAchallenge2023/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SEED ‚Äì Electronic Arts (EA), Sweden<br>
‚Ä¢ Dataset: GENEA Challenge 2023 Dataset, Samples: 70 test chunks, Modality: MoCap joints + speech audio + text transcriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>NPF-200: A Multi-Modal Eye Fixation Dataset and Method for Non-Photorealistic Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.12163"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yangziyu/NPF200"><img src="https://img.shields.io/github/stars/Yangziyu/NPF200.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: South China University of Technology<br>
‚Ä¢ Dataset: NPF-200, Samples: 200, Modality: RGB videos + audio + eye fixation points<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>The TYC Dataset for Understanding Instance-Level Semantics and Motions of Cells in Microstructures</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.12116"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://christophreich1996.github.io/tyc_dataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering and Information Technology, Centre for Synthetic Biology, Technische Universit ¬®at Darmstadt<br>
‚Ä¢ Dataset: TYC dataset, Samples: 261, Modality: brightfield microscopy videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Machine Learning-based Positioning using Multivariate Time Series Classification for Factory Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.11670"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Telematics, Hamburg University of Technology<br>
‚Ä¢ Dataset: Motion-Ambient, Samples: 4635217, Modality: IMU measurements (accelerometer, gyroscope, magnetometer), pressure, temperature, humidity, spectrum<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>TOPIC: A Parallel Association Paradigm for Multi-Object Tracking under Complex Motions and Diverse Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.11157"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/holmescao/TOPICTrack"><img src="https://img.shields.io/github/stars/holmescao/TOPICTrack.svg?style=social&label=Star"></a><br><a href="https://drive.google.com/file/d/1KvqQTZWWhyDsQuEIqQY3XLM8QlYjDqi/view?usp=sharing"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Key Laboratory for Urban Habitat Environmental Science and Technology, School of Environment and Energy, Peking University Shenzhen Graduate School<br>
‚Ä¢ Dataset: BEE24, Samples: 36, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Recursive Video Lane Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.11106"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dongkwonjin/RVLD"><img src="https://img.shields.io/github/stars/dongkwonjin/RVLD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Korea University<br>
‚Ä¢ Dataset: OpenLane-V, Samples: 590, Modality: RGB videos + temporally consistent lane annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>PsyMo: A Dataset for Estimating Self-Reported Psychological Traits from Gait</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.10631"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bit.ly/3Q91ypD"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University Politehnica of Bucharest<br>
‚Ä¢ Dataset: PsyMo, Samples: 14976, Modality: silhouettes, 2D / 3D human skeletons, 3D SMPL human meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.08544"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://henghuiding.github.io/MeViS"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanyang Technological University<br>
‚Ä¢ Dataset: MeViS, Samples: 2006, Modality: RGB videos + text expressions + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Leveraging Next-Active Objects for Context-Aware Anticipation in Egocentric Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.08303"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Pattern Analysis and Computer Vision (PA VIS), Istituto Italiano di Tecnologia (IIT)<br>
‚Ä¢ Dataset: Epic-Kitchens-100 (Next-Active-Object annotations), Samples: None, Modality: RGB videos + next-active-object bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Neural-Network-Driven Method for Optimal Path Planning via High-Accuracy Region Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.07974"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Graduate School of Information, Production and Systems, Waseda University, Kitakyushu, Japan<br>
‚Ä¢ Dataset: Complex Environment Motion Planning (CEMP), Samples: 16000, Modality: 2D RGB maps with optimal path regions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Real-time Automatic M-mode Echocardiography Measurement with Panel Attention from Local-to-Global Pixels</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.07717"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hanktseng131415go/RAMEM"><img src="https://img.shields.io/github/stars/hanktseng131415go/RAMEM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science
The University of Manchester<br>
‚Ä¢ Dataset: MEIS, Samples: 2639, Modality: M-mode echocardiography images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Understanding User Behavior in Volumetric Video Watching: Dataset, Analysis and Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.07578"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cuhksz-inml.github.io/user-behavior-in-vv-watching/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Future Network of Intelligence Institute, The Chinese University of Hong Kong, Shenzhen; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen<br>
‚Ä¢ Dataset: Volumetric Video Viewing Behavior Dataset, Samples: 300, Modality: 6DoF headset trajectories, 6DoF controller trajectories, Gaze data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Generalizing Event-Based Motion Deblurring in Real-World Scenarios</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.05932"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XiangZ-0/GEM"><img src="https://img.shields.io/github/stars/XiangZ-0/GEM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Wuhan University<br>
‚Ä¢ Dataset: Multi-Scale Real-world Blurry Dataset (MS-RBD), Samples: 32, Modality: RGB frames + events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Joint-Relation Transformer for Multi-Person Motion Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.04808"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MediaBrain-SJTU/JRTransformer"><img src="https://img.shields.io/github/stars/MediaBrain-SJTU/JRTransformer.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: 3DPW-SoMoF/RC, Samples: None, Modality: 3D skeleton joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>SODFormer: Streaming Object Detection with Transformer Using Events and Frames</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.04047"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dianzl/SODFormer"><img src="https://img.shields.io/github/stars/dianzl/SODFormer.svg?style=social&label=Star"></a><br><a href="https://www.pkuml.org/research/pku-davis-sod-dataset.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Engineering Research Center for Visual Technology, School of Computer Science, Peking University, Beijing 100871, China<br>
‚Ä¢ Dataset: PKU-DAVIS-SOD, Samples: 220, Modality: asynchronous events, frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Who Is Alyx? A new Behavioral Biometric Dataset for User Identification in XR</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.03788"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cschell/who-is-alyx"><img src="https://img.shields.io/github/stars/cschell/who-is-alyx.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Human-Computer Interaction (HCI) Group, Informatik, University of W¬®urzburg, W¬®urzburg, Germany<br>
‚Ä¢ Dataset: Who Is Alyx?, Samples: 71 users, 2 sessions/user, 45 mins/session, Modality: VR motion tracking (HMD, controllers), eye-tracking, physiological signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Reconstructing Three-Dimensional Models of Interacting Humans</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.01854"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://ci3d.imar.ro"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Mathematics of the Romanian Academy<br>
‚Ä¢ Dataset: CHI3D, Samples: 631, Modality: MoCap joints, GHUM/SMPLX parameters, multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>MVFlow: Deep Optical Flow Estimation of Compressed Videos with Motion Vector Prior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.01568"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fudan University<br>
‚Ä¢ Dataset: Compressed FlyingThings3D, Samples: None, Modality: compressed RGB videos + motion vectors + optical flow<br>
‚Ä¢ Dataset: Compressed MPI Sintel, Samples: None, Modality: compressed RGB videos + motion vectors + optical flow<br>
‚Ä¢ Dataset: Compressed KITTI 2012, Samples: None, Modality: compressed RGB videos + motion vectors + optical flow<br>
‚Ä¢ Dataset: Compressed KITTI 2015, Samples: None, Modality: compressed RGB videos + motion vectors + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>Efficient neural supersampling on a novel gaming dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.01483"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Qualcomm AI Research<br>
‚Ä¢ Dataset: QRISP (Qualcomm Rasterized Images for Super-resolution Processing), Samples: 8760, Modality: RGB videos + depth + motion vectors<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2023</td>
  <td style="width:70%;"><strong>On the Generation of a Synthetic Event-Based Vision Dataset for Navigation and Landing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2308.00394"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.com/EuropeanSpaceAgency/trajectory-to-events"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Advanced Concepts Team, European Space Agency, European Space Research and Technology Centre (ESTEC), Keplerlaan 1, 2201 AZ Noordwijk, The Netherlands<br>
‚Ä¢ Dataset: None, Samples: 500, Modality: ['optimal landing trajectories', 'photorealistic video', 'event streams', 'motion field ground truth']<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.16897"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="ivl.cs.brown.edu/research/diva"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Brown University<br>
‚Ä¢ Dataset: DiVa-360, Samples: 54, Modality: Multi-view RGB videos, foreground-background segmentation masks, synchronized audio, text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Towards Imbalanced Motion: Part-Decoupling Network for Video Portrait Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.16565"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing 100191 , China<br>
‚Ä¢ Dataset: MVPS, Samples: 101, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.16062"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.11237258"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Eindhoven University of Technology<br>
‚Ä¢ Dataset: None, Samples: 544, Modality: Motion tracker trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.15942"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XiaRho/CMDA"><img src="https://img.shields.io/github/stars/XiaRho/CMDA.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: East China University of Science and Technology<br>
‚Ä¢ Dataset: DSEC Night-Semantic, Samples: 1692 training samples, 150 testing samples, Modality: Images + Events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Supervised Homography Learning with Realistic Dataset Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.15353"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JianghaiSCU/RealSH"><img src="https://img.shields.io/github/stars/JianghaiSCU/RealSH.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sichuan University, Megvii Technology<br>
‚Ä¢ Dataset: CA-sup, Samples: 800000, Modality: RGB image pairs + homography labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.15055"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pointodyssey.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: PointOdyssey, Samples: 104, Modality: Synthetic RGB videos + 2D/3D point trajectories, depth, normals, segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Event-based Vision for Early Prediction of Manipulation Actions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.14332"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DaniDeniz/DavisHandDataset-Events"><img src="https://img.shields.io/github/stars/DaniDeniz/DavisHandDataset-Events.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Architecture and Technology, CITIC, University of Granada<br>
‚Ä¢ Dataset: Event-based Manipulation Action Dataset (E-MAD), Samples: 750, Modality: event stream<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.10713"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jspenmar/slowtv_monodepth"><img src="https://img.shields.io/github/stars/jspenmar/slowtv_monodepth.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Surrey<br>
‚Ä¢ Dataset: SlowTV, Samples: 40, Modality: monocular video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.10173"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dna-rendering.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory<br>
‚Ä¢ Dataset: DNA-Rendering, Samples: 5000, Modality: multi-view RGB videos, depth data, 2D/3D keypoints, SMPLX models, foreground masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>AGAR: Attention Graph-RNN for Adaptative Motion Prediction of Point Clouds of Deformable Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.09936"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/pedro-dm-gomes/AGAR"><img src="https://img.shields.io/github/stars/pedro-dm-gomes/AGAR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University College London<br>
‚Ä¢ Dataset: Mixamo Human Bodies Activities, Samples: 9527, Modality: Point Cloud Sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Online Self-Supervised Thermal Water Segmentation for Aerial Vehicles</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.09027"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/connorlee77/uav-thermal-water-segmentation"><img src="https://img.shields.io/github/stars/connorlee77/uav-thermal-water-segmentation.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: California Institute of Technology<br>
‚Ä¢ Dataset: Aerial and Ground Thermal Near-shore Dataset, Samples: 20, Modality: Thermal video sequences + IMU data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>A Study in Zucker: Insights on Interactions Between Humans and Small Service Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.08668"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motion-lab.github.io/ZuckerDataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computing at Clemson University, SC, USA<br>
‚Ä¢ Dataset: Zucker Dataset, Samples: 309 human trajectories, 97 robot trajectories, Modality: Pose trajectories from HTC Vive Trackers<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Video Frame Interpolation with Stereo Event and Intensity Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.08228"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dingchao1214.github.io/web sevfi/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University, Wuhan 430072, China<br>
‚Ä¢ Dataset: Stereo Event-Intensity Dataset (SEID), Samples: 34, Modality: Stereo events, RGB frames, and depth maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>TVPR: Text-to-Video Person Retrieval and a New Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.07184"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Nanjing Tech University<br>
‚Ä¢ Dataset: Text-to-Video Person Re-identification (TVPReid), Samples: 6559, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Towards Anytime Optical Flow Estimation with Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.05033"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Yaozhuwa/EVA-Flow"><img src="https://img.shields.io/github/stars/Yaozhuwa/EVA-Flow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory of Extreme Photonics and Instrumentation, College of Optical Science and Engineering, Zhejiang University, Hangzhou 310027, China<br>
‚Ä¢ Dataset: EVA-FlowSet, Samples: 4, Modality: Event camera data for optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>FTFDNet: Learning to Detect Talking Face Video Manipulation with Tri-Modality Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.03990"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ASGO, School of Computer Science, Northwestern Polytechnical University, Xi‚Äôan, China<br>
‚Ä¢ Dataset: Fake Talking Face Detection Dataset (FTFDD), Samples: 64679, Modality: RGB videos + audio + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Ground-Challenge: A Multi-sensor SLAM Dataset Focusing on Corner Cases for Ground Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.03890"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sjtuyinjie/Ground-Challenge"><img src="https://img.shields.io/github/stars/sjtuyinjie/Ground-Challenge.svg?style=social&label=Star"></a><br><a href="https://github.com/sjtuyinjie/Ground-Challenge"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: independent researchers<br>
‚Ä¢ Dataset: Ground-Challenge, Samples: 36, Modality: RGB-D camera, IMU, wheel odometer, 3D LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Safe & Accurate at Speed with Tendons: A Robot Arm for Exploring Dynamic Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.02654"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="webdav.tuebingen.mpg.de/pamy2"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, 72076 T ¬®ubingen, Germany.<br>
‚Ä¢ Dataset: Pamy2 Proprioceptive Dataset, Samples: 25 days, Modality: Proprioceptive data: joint positions, joint velocities, observed and desired muscle pressures.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Unveiling the Potential of Spike Streams for Foreground Occlusion Removal from Densely Continuous Views</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.00821"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: S-OCC, Samples: 128, Modality: Spike streams and ground truth background images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.00818"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/IDEA-Research/Motion-X"><img src="https://img.shields.io/github/stars/IDEA-Research/Motion-X.svg?style=social&label=Star"></a><br><a href="https://motion-x-dataset.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: International Digital Economy Academy (IDEA)<br>
‚Ä¢ Dataset: Motion-X, Samples: 81100, Modality: SMPL-X parameters + Text descriptions + RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2023</td>
  <td style="width:70%;"><strong>RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2307.00595"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="rh20t.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai Jiao Tong University<br>
‚Ä¢ Dataset: RH20T, Samples: 110000, Modality: RGB images, depth images, binocular IR images, 6DoF force-torque, audio, proprioception (joint angles, joint torques, end-effector Cartesian pose, gripper states), fingertip tactile<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.17010"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Toytiny/milliFlow"><img src="https://img.shields.io/github/stars/Toytiny/milliFlow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Edinburgh<br>
‚Ä¢ Dataset: milliFlow Dataset, Samples: 216, Modality: mmWave radar point clouds, RGB-D images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.16940"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://bedlam.is.tue.mpg.de/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T√ºbingen, Germany<br>
‚Ä¢ Dataset: BEDLAM, Samples: 2311, Modality: RGB videos + SMPL-X ground truth + depth maps + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.16917"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://davidrecasens.github.io/TheDrunkard‚ÄôsOdometry/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Zaragoza<br>
‚Ä¢ Dataset: The Drunkard's Dataset, Samples: 19, Modality: RGB images, depth maps, optical flow, normal maps, camera pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Action-conditioned Deep Visual Prediction with RoAM, a new Indoor Human Motion Dataset for Autonomous Robots</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.15852"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tinyurl.com/RoAMData"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Aerospace Engineering Dept, Indian Institute of Science, Bangalore<br>
‚Ä¢ Dataset: RoAM, Samples: 25, Modality: Stereo color images, 2D LiDAR scan, Odometry, IMU, depth maps, robot control actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Detector-Free Structure from Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.15669"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zju3dv/DetectorFreeSfM"><img src="https://img.shields.io/github/stars/zju3dv/DetectorFreeSfM.svg?style=social&label=Star"></a><br><a href="https://zju3dv.github.io/DetectorFreeSfM/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Zhejiang University<br>
‚Ä¢ Dataset: Texture-Poor SfM Dataset, Samples: 1020, Modality: RGB videos + ground-truth camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Self-supervised Learning of Event-guided Video Frame Interpolation for Rolling Shutter Frames</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.15507"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yunfanLu/Self-EvRSVFI"><img src="https://img.shields.io/github/stars/yunfanLu/Self-EvRSVFI.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AI Thrust, HKUST(GZ)<br>
‚Ä¢ Dataset: ERS, Samples: 29, Modality: RGB videos + events<br>
‚Ä¢ Dataset: UAV-RS, Samples: 9, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>AutoGraph: Predicting Lane Graphs from Traffic Observations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.15410"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://autograph.cs.uni-freiburg.de/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Engineering, University of Freiburg, 79115 Freiburg, Germany<br>
‚Ä¢ Dataset: UrbanTracklet, Samples: 882592, Modality: Vehicle tracklets from LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>BotanicGarden: A High-Quality Dataset for Robot Navigation in Unstructured Natural Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.14137"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/robot-pesg/BotanicGarden"><img src="https://img.shields.io/github/stars/robot-pesg/BotanicGarden.svg?style=social&label=Star"></a><br><a href="https://github.com/robot-pesg/BotanicGarden"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Sensing Science and Engineering, Shanghai Jiao Tong University, Shanghai 200240, China<br>
‚Ä¢ Dataset: BotanicGarden, Samples: 33, Modality: Stereo cameras (Gray, RGB), 3D LiDAR, IMU, wheel odometry, pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>The MI-Motion Dataset and Benchmark for 3D Multi-Person Motion Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.13566"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mi-motion.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Hangzhou Dianzi University<br>
‚Ä¢ Dataset: MI-Motion, Samples: 210, Modality: Game engine synthesized 3D skeleton poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Audio-Driven 3D Facial Animation from In-the-Wild Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.11541"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://faw3d.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Internation Digital Economy Academy<br>
‚Ä¢ Dataset: HDTF-3D, Samples: 133, Modality: 3D facial motion parameters (FLAME)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Robot Learning with Sensorimotor Pre-training</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.10007"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/irados/rpt"><img src="https://img.shields.io/github/stars/irados/rpt.svg?style=social&label=Star"></a><br><a href="https://bair.berkeley.edu/blog/2023/06/20/rpt/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Berkeley<br>
‚Ä¢ Dataset: N/A, Samples: 20000, Modality: multi-view RGB images + proprioceptive robot states + actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.09126"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sony/audio-visual-seld-dcase2023"><img src="https://img.shields.io/github/stars/sony/audio-visual-seld-dcase2023.svg?style=social&label=Star"></a><br><a href="https://zenodo.org/record/7880637"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Sony AI<br>
‚Ä¢ Dataset: STARSS23, Samples: 168, Modality: Multichannel audio, 360¬∞ video, MoCap-derived pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Emotional Speech-Driven Animation with Content-Emotion Disentanglement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.08990"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/radekdanecek/EMOTE"><img src="https://img.shields.io/github/stars/radekdanecek/EMOTE.svg?style=social&label=Star"></a><br><a href="https://emote.is.tue.mpg.de/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T ¬®ubingen, Germany<br>
‚Ä¢ Dataset: Pseudo ground-truth 3D (FLAME parameters) for the MEAD dataset, Samples: None, Modality: 3D facial motion parameters (FLAME)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Motion Capture Dataset for Practical Use of AI-based Motion Editing and Stylization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.08861"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://acesinc.co.jp/projects/project-1760"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ACES Inc., Japan<br>
‚Ä¢ Dataset: Bandai-Namco-Research-Motiondataset-1, Samples: None, Modality: MoCap joints<br>
‚Ä¢ Dataset: Bandai-Namco-Research-Motiondataset-2, Samples: None, Modality: MoCap joints<br>
‚Ä¢ Dataset: Emote motion dataset, Samples: 42, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Instant Multi-View Head Capture through Learnable Registration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.07437"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tempeh.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: MPI for Intelligent Systems, T√ºbingen<br>
‚Ä¢ Dataset: FaMoS, Samples: 2660, Modality: multi-view gray-scale images + 3D head scans<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>4DHumanOutfit: a multi-subject 4D dataset of human motion sequences in varying outfits exhibiting large displacements</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.07399"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://kinovis.inria.fr/4dhumanoutfit/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NAVER LABS Europe<br>
‚Ä¢ Dataset: 4DHumanOutfit, Samples: 1540, Modality: RGB videos + 3D mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Anomaly Detection in Satellite Videos using Diffusion Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.05376"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, Houston, TX 77004<br>
‚Ä¢ Dataset: GOES-16/17 Wildfire Anomaly Dataset, Samples: 520, Modality: Satellite videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>Dance Generation by Sound Symbolic Words</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.03646"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/onomatopoeia-dance/home/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Tsukuba, Japan<br>
‚Ä¢ Dataset: Onomatopoeia-dance motion pairs, Samples: 44, Modality: 3D motion data (AIST++) and time-aligned onomatopoeia annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>VR.net: A Real-world Dataset for Virtual Reality Motion Sickness Research</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.03381"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vrhook.ahlab.org"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Auckland<br>
‚Ä¢ Dataset: VR.net, Samples: 12 hours of gameplay videos, Modality: RGB videos, depth maps, motion vectors, camera/object/headset pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2023</td>
  <td style="width:70%;"><strong>TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2306.02850"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Arthur151/DynaCam"><img src="https://img.shields.io/github/stars/Arthur151/DynaCam.svg?style=social&label=Star"></a><br><a href="https://www.yusun.work/TRACE/TRACE.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Harbin Institute of Technology<br>
‚Ä¢ Dataset: DynaCam, Samples: more than 500 annotated DC-videos, Modality: RGB videos with camera poses and pseudo-ground-truth 3D human annotations (pose, shape, global trajectories)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>A Multi-Modal Transformer Network for Action Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.19624"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://aiaiproject.weebly.com/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA 22904<br>
‚Ä¢ Dataset: instructional activity dataset, Samples: None, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Large Car-following Data Based on Lyft level-5 Open Dataset: Following Autonomous Vehicles vs. Human-driven Vehicles</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.18921"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/RomainLITUD/Car-Following-Dataset-HV-vs-AV"><img src="https://img.shields.io/github/stars/RomainLITUD/Car-Following-Dataset-HV-vs-AV.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Transport & Planning, Delft University of Technology, the Netherlands<br>
‚Ä¢ Dataset: Car-Following-Dataset-HV-vs-AV, Samples: 72341, Modality: Vehicle trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.18891"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XingqunQi-lab/EmotionGestures"><img src="https://img.shields.io/github/stars/XingqunQi-lab/EmotionGestures.svg?style=social&label=Star"></a><br><a href="https://xingqunqi-lab.github.io/Emotion-Gesture-Web/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: the Academy of Interdisciplinary Studies, The Hong Kong University of Science and Technology, Hong Kong, China<br>
‚Ä¢ Dataset: TED Emotion dataset, Samples: 78734, Modality: 3D upper body joints (pseudo ground truth), speech audio, text transcripts<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Motion-Scenario Decoupling for Rat-Aware Video Position Prediction: Strategy and Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.18310"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dalian University of Technology, Liaoning, China<br>
‚Ä¢ Dataset: RatPose, Samples: 1023, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Alignment-free HDR Deghosting with Semantics Consistent Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.18135"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://steven-tel.github.io/sctnet/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Burgundy, ImViA<br>
‚Ä¢ Dataset: SCTNet HDR Deghosting Dataset, Samples: 144, Modality: LDR images of dynamic scenes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Z-GMOT: Zero-shot Generic Multiple Object Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.17648"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://fsoft-aic.github.io/Z-GMOT"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: FPT Software AI Center, Vietnam<br>
‚Ä¢ Dataset: Referring GMOT dataset, Samples: 98, Modality: RGB videos + textual descriptions<br>
‚Ä¢ Dataset: Refer-GMOT40, Samples: 40, Modality: RGB videos + textual descriptions<br>
‚Ä¢ Dataset: Refer-Animal, Samples: 58, Modality: RGB videos + textual descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>BASED: Benchmarking, Analysis, and Structural Estimation of Deblurring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.17477"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/illaitar/based"><img src="https://img.shields.io/github/stars/illaitar/based.svg?style=social&label=Star"></a><br><a href="https://videoprocessing.ai/benchmarks/deblurring.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Lomonosov Moscow State University<br>
‚Ä¢ Dataset: BASED, Samples: 23, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Comparison of Pedestrian Prediction Models from Trajectory and Appearance Data for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.15942"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Applied Research Team, Five AI (Bosch UK), Edinburgh, United Kingdom<br>
‚Ä¢ Dataset: NuScenes-Appearance, Samples: None, Modality: RGB images + 3D trajectories<br>
‚Ä¢ Dataset: motion-changes dataset, Samples: None, Modality: RGB images + 3D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Malicious or Benign? Towards Effective Content Moderation for Children's Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.15551"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/syedhammadahmed/mob"><img src="https://img.shields.io/github/stars/syedhammadahmed/mob.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of Central Florida, Orlando, FL USA<br>
‚Ä¢ Dataset: Malicious or Benign (MOB), Samples: 1565, Modality: RGB videos + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>EgoVSR: Towards High-Quality Egocentric Video Super-Resolution</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.14708"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/chiyich/EGOVSR/"><img src="https://img.shields.io/github/stars/EGOVSR/.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen International Graduate School, Tsinghua University<br>
‚Ä¢ Dataset: EgoVSR, Samples: 46724, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>FEDORA: Flying Event Dataset fOr Reactive behAvior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.14392"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Purdue University, West Lafayette, IN 47907, USA<br>
‚Ä¢ Dataset: FEDORA, Samples: 5, Modality: RGB images, Event streams, IMU data, Depth, Pose, Optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Flare-Aware Cross-modal Enhancement Network for Multi-spectral Vehicle Re-identification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.13659"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Anonymous<br>
‚Ä¢ Dataset: WMVeID863, Samples: 4709, Modality: RGB, NI, and TI image triplets<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.13353"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://RenderMe-360.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory, SenseTime<br>
‚Ä¢ Dataset: RenderMe-360, Samples: 800000, Modality: Synchronized multi-view HD videos (60 cameras, 30FPS), audio, 3D scans, 2D/3D facial landmarks, FLAME models, text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>SIDAR: Synthetic Image Dataset for Alignment & Restoration</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.12036"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Vision & Remote Sensing, Technische Universit√§t Berlin<br>
‚Ä¢ Dataset: SIDAR, Samples: None, Modality: Rendered RGB images + occlusion masks + homographies<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>ZeroFlow: Scalable Scene Flow via Distillation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.10424"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/kylevedder/zeroflow"><img src="https://img.shields.io/github/stars/kylevedder/zeroflow.svg?style=social&label=Star"></a><br><a href="https://vedder.io/zeroflow"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Pennsylvania<br>
‚Ä¢ Dataset: Argoverse 2 Scene Flow Pseudo-Labels, Samples: 700 training sequences, Modality: LiDAR + scene flow pseudo-labels<br>
‚Ä¢ Dataset: Waymo Open Scene Flow Pseudo-Labels, Samples: 798 training sequences, Modality: LiDAR + scene flow pseudo-labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.09662"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://azadis.github.io/make-an-animation"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta AI<br>
‚Ä¢ Dataset: Text Pseudo-Pose (TPP) dataset, Samples: 35000000, Modality: text descriptions + 3D pseudo-pose SMPL annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>AMD: Autoregressive Motion Diffusion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.09381"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology, Zhejiang Univerisity<br>
‚Ä¢ Dataset: HumanLong3D, Samples: 43696, Modality: 3D human motions + text<br>
‚Ä¢ Dataset: HumanMusic, Samples: 137136, Modality: 3D human motions + audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Motion Question Answering via Modular Motion Programs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.08953"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/markendo/HumanMotionQA/"><img src="https://img.shields.io/github/stars/HumanMotionQA/.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Stanford University<br>
‚Ä¢ Dataset: BABEL-QA, Samples: 1109, Modality: MoCap joints, rotations, body and hand meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Benchmarking UWB-Based Infrastructure-Free Positioning and Multi-Robot Relative Localization: Dataset and Characterization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.08532"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/TIERS/uwb-relative-localization-dataset"><img src="https://img.shields.io/github/stars/TIERS/uwb-relative-localization-dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Turku Intelligent Embedded and Robotic Systems (TIERS) Lab, University of Turku, Finland.<br>
‚Ä¢ Dataset: UWB-Based Infrastructure-Free Positioning and Multi-Robot Relative Localization Dataset, Samples: 24, Modality: ['UWB ranging', 'MOCAP trajectories', 'IMU', 'Odometry', 'VIO']<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Lightweight Delivery Detection on Doorbell Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.07812"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Qualcomm Technologies<br>
‚Ä¢ Dataset: Doorbell delivery detection dataset, Samples: 10873, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with Bird's Eye View based Appearance and Motion Features</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.07336"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xiekkki/motionbev"><img src="https://img.shields.io/github/stars/xiekkki/motionbev.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Automation, Southeast University<br>
‚Ä¢ Dataset: SipailouCampus, Samples: 26279 frames, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.07214"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/MMGEgo4D"><img src="https://img.shields.io/github/stars/facebookresearch/MMGEgo4D.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Texas at Austin<br>
‚Ä¢ Dataset: MMG-Ego4D, Samples: None, Modality: video, audio, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.06477"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC V6T 1Z4, Canada<br>
‚Ä¢ Dataset: IR-labelled tissue dataset, Samples: 1321, Modality: Stereo surgical videos with IR ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.06356"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="www.actors-hq.com"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Synthesia, Germany<br>
‚Ä¢ Dataset: ActorsHQ, Samples: 16, Modality: multi-view 12MP RGB videos + per-frame 3D meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Eiffel Tower: A Deep-Sea Underwater Dataset for Long-Term Visual Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.05301"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/clementinboittiaux/sfm-pipeline"><img src="https://img.shields.io/github/stars/clementinboittiaux/sfm-pipeline.svg?style=social&label=Star"></a><br><a href="https://seanoe.org/data/00810/92226/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ifremer, Zone Portuaire de Br¬¥egaillon, La Seyne-sur-Mer, France<br>
‚Ä¢ Dataset: Eiffel Tower, Samples: 18082, Modality: RGB images + 6DOF camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.03713"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://research.nvidia.com/labs/nxp/avatar-fingerprinting/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA<br>
‚Ä¢ Dataset: NVIDIA FacialReenactment (NVFAIR), Samples: 654726, Modality: RGB videos of talking heads<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.03187"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Georgia Institute of Technology<br>
‚Ä¢ Dataset: Virtual IMU data generated from virtual textual descriptions, Samples: 1600, Modality: Virtual accelerometer data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.03027"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tobias-kirschstein.github.io/nersemble"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich, Germany<br>
‚Ä¢ Dataset: NeRSemble, Samples: 4734, Modality: Multi-view RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.01618"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zehaozhu.github.io/ContactArt/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Texas at Austin<br>
‚Ä¢ Dataset: ContactArt, Samples: 552000, Modality: Hand poses, articulated object poses, contact regions, rendered RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.01241"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vimeo.com/823756031"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Social Cognitive Systems Group, Bielefeld University, Germany<br>
‚Ä¢ Dataset: BiGe dataset, Samples: 54360, Modality: 3D full-body joints, audio, text<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.00767"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cao-cong/RViDeformer"><img src="https://img.shields.io/github/stars/cao-cong/RViDeformer.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical and Information Engineering, Tianjin University<br>
‚Ä¢ Dataset: ReCRVD, Samples: 120, Modality: paired noisy-clean raw videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">May 2023</td>
  <td style="width:70%;"><strong>Event-Free Moving Object Segmentation from Moving Ego Vehicle</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2305.00126"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZZY-Zhou/DSEC-MOS"><img src="https://img.shields.io/github/stars/ZZY-Zhou/DSEC-MOS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Burgundy, Dijon, France; University of Wurzburg, Wurzburg, Germany<br>
‚Ä¢ Dataset: DSEC-MOS, Samples: 13314, Modality: RGB videos, Event camera data, pixel-level moving object segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Density Invariant Contrast Maximization for Neuromorphic Earth Observations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.14125"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/neuromorphicsystems/event_warping"><img src="https://img.shields.io/github/stars/neuromorphicsystems/event_warping.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Western Sydney University<br>
‚Ä¢ Dataset: ISS event dataset, Samples: 10, Modality: Event camera data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>What Happened 3 Seconds Ago? Inferring the Past with Thermal Imaging</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.13651"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZitianTang/Thermal-IM"><img src="https://img.shields.io/github/stars/ZitianTang/Thermal-IM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IIIS, Tsinghua University<br>
‚Ä¢ Dataset: Thermal-IM, Samples: 783, Modality: RGB-Thermal videos, RGB-Depth videos, 2D/3D human poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Dynamic Video Frame Interpolation with integrated Difficulty Pre-Assessment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.12664"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Samsung Electronics (China) R&D Center<br>
‚Ä¢ Dataset: VFI Difficulty Assessment dataset, Samples: 13030, Modality: RGB video frames + difficulty scores<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.12281"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/showlab/HOSNeRF"><img src="https://img.shields.io/github/stars/showlab/HOSNeRF.svg?style=social&label=Star"></a><br><a href="https://showlab.github.io/HOSNeRF"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Show Lab, National University of Singapore<br>
‚Ä¢ Dataset: HOSNeRF dataset, Samples: 6, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>MAMAF-Net: Motion-Aware and Multi-Attention Fusion Network for Stroke Diagnosis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.09466"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: VTT Technical Research Centre of Finland<br>
‚Ä¢ Dataset: Stroke-data, Samples: 148, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Text2Performer: Text-Driven Human Video Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.08483"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://yumingj.github.io/projects/Text2Performer.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: Fashion-Text2Video Dataset, Samples: 600, Modality: RGB videos + text annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Learning How To Robustly Estimate Camera Pose in Endoscopic Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.08023"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/aimi-lab/robust-pose-estimator"><img src="https://img.shields.io/github/stars/aimi-lab/robust-pose-estimator.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.7727692"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ARTORG Center, University of Bern, Switzerland<br>
‚Ä¢ Dataset: StereoMIS, Samples: 16, Modality: Stereo videos + camera pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Text-Conditional Contextualized Avatars For Zero-Shot Personalization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.07410"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta AI<br>
‚Ä¢ Dataset: Image Text Pseudo-Pose (ITPP), Samples: 35000000, Modality: text + 3D SMPL poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Self-Supervised Scene Dynamic Recovery from Rolling Shutter Images and Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.06930"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/w3un/SelfUnroll"><img src="https://img.shields.io/github/stars/w3un/SelfUnroll.svg?style=social&label=Star"></a><br><a href="https://w3un.github.io/selfunroll/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University<br>
‚Ä¢ Dataset: DAVIS-RS-Event (DRE), Samples: 100, Modality: Rolling Shutter (RS) images + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>FollowMe: Vehicle Behaviour Prediction in Autonomous Vehicle Settings</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.06121"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Texas at Austin<br>
‚Ä¢ Dataset: FollowMe, Samples: 384 sequences (32 participants x 12 scenarios), Modality: Vehicle trajectories (positions p=(x,y) over time)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.05684"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Tr3e/InterGen"><img src="https://img.shields.io/github/stars/Tr3e/InterGen.svg?style=social&label=Star"></a><br><a href="https://tr3e.github.io/intergen-page/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: InterHuman, Samples: 7779, Modality: skeletal motions + text descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>FIR-based Future Trajectory Prediction in Nighttime Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.05345"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/FordCVResearch/FIR-Trajectory-Prediction"><img src="https://img.shields.io/github/stars/FordCVResearch/FIR-Trajectory-Prediction.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ford Motor Company, GreenÔ¨Åeld Labs, Palo Alto, CA, USA<br>
‚Ä¢ Dataset: FIR-based Large Animal Detection and Trajectory Prediction Dataset, Samples: 26127, Modality: FIR videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.05170"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://deeperaction.github.io/datasets/sportsmot.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Laboratory for Novel Software Technology, Nanjing University, China<br>
‚Ä¢ Dataset: SportsMOT, Samples: 240, Modality: RGB videos + bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Multi-Object Tracking by Iteratively Associating Detections with Uniform Appearance for Trawl-Based Fishing Bycatch Monitoring</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.04816"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical & Computer Engineering, University of Washington, United States<br>
‚Ä¢ Dataset: NIWA underwater fish dataset, Samples: 8, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>WOMD-LiDAR: Raw Sensor Dataset Benchmark for Motion Forecasting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.03834"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://waymo.com/open/data/motion/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Waymo LLC<br>
‚Ä¢ Dataset: WOMD-LiDAR, Samples: 104000, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Motion Capture Benchmark of Real Industrial Tasks and Traditional Crafts for Human Movement Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.03771"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/olivas-bre/GOM.git"><img src="https://img.shields.io/github/stars/olivas-bre/GOM.git.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.5356992"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The Centre for Robotics, Mines Paris, Universit√© PSL, 75006 Paris, France<br>
‚Ä¢ Dataset: TV assembly (TVA), Samples: 479, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
‚Ä¢ Dataset: TV packaging (TVP), Samples: 54, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
‚Ä¢ Dataset: Airplane floater assembly (APA), Samples: 19, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
‚Ä¢ Dataset: Postures according to EAWS protocol (ERGD), Samples: 840, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
‚Ä¢ Dataset: Silk weaving (SLW), Samples: 308, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
‚Ä¢ Dataset: Glass blowing (GLB), Samples: 152, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
‚Ä¢ Dataset: Mastic cultivation (MSC), Samples: 83, Modality: MoCap joints (BVH format, from 52 IMUs)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Automatic Detection of Reactions to Music via Earable Sensing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.03295"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KOREATECH, Republic of Korea<br>
‚Ä¢ Dataset: MusicReactionSet, Samples: 240, Modality: IMU and audio<br>
‚Ä¢ Dataset: controlled dataset, Samples: 30, Modality: IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>DEFLOW: Self-supervised 3D Motion Estimation of Debris Flow</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.02569"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/liyzhu/DEFLOW"><img src="https://img.shields.io/github/stars/liyzhu/DEFLOW.svg?style=social&label=Star"></a><br><a href="https://liyzhu.github.io/DEFLOW/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Geodesy and Photogrammetry, ETH Zurich<br>
‚Ä¢ Dataset: Debris flow dataset, Samples: 6000, Modality: RGB images + LiDAR point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Re-Evaluating LiDAR Scene Flow for Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.02150"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/n-chodosh/re-evaluating-lidar-flow"><img src="https://img.shields.io/github/stars/n-chodosh/re-evaluating-lidar-flow.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: Argoverse 2.0 flow labels, Samples: , Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.01186"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://follow-your-pose.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China<br>
‚Ä¢ Dataset: LAION-Pose, Samples: , Modality: image-text-pose pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Apr 2023</td>
  <td style="width:70%;"><strong>DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2304.01168"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: DeepAccident, Samples: 285000 samples, Modality: multi-view cameras, LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>CIMI4D: A Large Multimodal Climbing Motion Dataset under Human-scene Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.17948"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.lidarhumanmotion.net/cimi4d/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University<br>
‚Ä¢ Dataset: CIMI4D, Samples: 42, Modality: RGB videos, LiDAR point clouds, IMU measurements, static point cloud scenes, reconstructed scene meshes, annotated human poses, global trajectories, contact annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>CIRCLE: Capture In Rich Contextual Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.17912"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jp-araujo/CIRCLE"><img src="https://img.shields.io/github/stars/jp-araujo/CIRCLE.svg?style=social&label=Star"></a><br><a href="https://jp-araujo.github.io/CIRCLE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: CIRCLE, Samples: 7000, Modality: ['SMPL-X parameters', 'VR headset trajectory', 'Egocentric RGB-D video', 'MoCap joints']<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>CryoFormer: Continuous Heterogeneous Cryo-EM Reconstruction using Transformer-based Neural Representations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.16254"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cryoformer.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University, Cellverse, HKUST<br>
‚Ä¢ Dataset: PEDV Spike Protein Dataset, Samples: 50000, Modality: Atomic models (PDB), density maps (MRC), simulated 2D projection images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Multimodal video and IMU kinematic dataset on daily life activities using affordable devices (VIDIMU)</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.16150"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/twyncoder/vidimu-tools"><img src="https://img.shields.io/github/stars/twyncoder/vidimu-tools.svg?style=social&label=Star"></a><br><a href="https://doi.org/10.5281/zenodo.7681316"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Valladolid, Valladolid, Spain<br>
‚Ä¢ Dataset: VIDIMU, Samples: 702, Modality: RGB video + IMU data + 3D body pose + 3D joint angles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Recovering 3D Hand Mesh Sequence from a Single Blurry Image: A New Dataset and Temporal Unfolding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.15417"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JaehaKim97/BlurHand_RELEASE"><img src="https://img.shields.io/github/stars/JaehaKim97/BlurHand_RELEASE.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of ECE&ASRI, Seoul National University, Korea<br>
‚Ä¢ Dataset: BlurHand, Samples: 155896, Modality: RGB images + 3D hand mesh sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>NeuralPCI: Spatio-temporal Neural Field for 3D Point Cloud Multi-frame Non-linear Interpolation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.15126"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ispc-lab/NeuralPCI"><img src="https://img.shields.io/github/stars/ispc-lab/NeuralPCI.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tongji University<br>
‚Ä¢ Dataset: NL-Drive, Samples: None, Modality: LiDAR point cloud sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.14840"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Junggy/HAMMER-dataset"><img src="https://img.shields.io/github/stars/Junggy/HAMMER-dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Technical University of Munich<br>
‚Ä¢ Dataset: HAMMER, Samples: 26 trajectories (~13k frames), Modality: Robot pose trajectories, RGB+Polarization, D-ToF, I-ToF, Stereo (passive/active), Dense ground truth depth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.14435"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JokerYan/NeRF-DS"><img src="https://img.shields.io/github/stars/JokerYan/NeRF-DS.svg?style=social&label=Star"></a><br><a href="https://github.com/JokerYan/NeRF-DS"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, National University of Singapore<br>
‚Ä¢ Dataset: Dynamic Specular Dataset, Samples: 8, Modality: monocular RGB videos + camera poses + object masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>OPDMulti: Openable Part Detection for Multiple Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.14087"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/3dlg-hcvc/OPDMulti"><img src="https://img.shields.io/github/stars/3dlg-hcvc/OPDMulti.svg?style=social&label=Star"></a><br><a href="https://3dlg-hcvc.github.io/OPDMulti/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Simon Fraser University<br>
‚Ä¢ Dataset: OPDMulti, Samples: 64213, Modality: RGB-D images + part masks + motion parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>MusicFace: Music-driven Expressive Singing Face Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.14044"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://vcg.xmu.edu.cn/datasets/singingface/index.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Informatics, Xiamen University, Xiamen, 361000, China<br>
‚Ä¢ Dataset: SingingFace, Samples: 600, Modality: RGB videos with extracted 3D face pose and expression parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Progressively Optimized Local Radiance Fields for Robust View Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.13791"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/facebookresearch/localrf"><img src="https://img.shields.io/github/stars/facebookresearch/localrf.svg?style=social&label=Star"></a><br><a href="https://localrf.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST<br>
‚Ä¢ Dataset: Static Hikes, Samples: 12, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.13767"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/flyLu/STIR-EG-VSR"><img src="https://img.shields.io/github/stars/flyLu/STIR-EG-VSR.svg?style=social&label=Star"></a><br><a href="https://vlis2022.github.io/cvpr23/egvsr"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AI Thrust, HKUST(GZ)<br>
‚Ä¢ Dataset: ALPIX-VSR, Samples: 26 video sequences, Modality: RGB videos + Event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>3D-POP -- An automated annotation approach to facilitate markerless 2D-3D tracking of freely moving birds with marker-based motion capture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.13174"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/alexhang212/Dataset-3DPOP"><img src="https://img.shields.io/github/stars/alexhang212/Dataset-3DPOP.svg?style=social&label=Star"></a><br><a href="https://tinyurl.com/4ckbjcpx"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Collective Behavior and Dept. of Ecology of Animal Societies, Max Planck Institute of Animal Behavior<br>
‚Ä¢ Dataset: 3D-POP, Samples: 57, Modality: MoCap (6-DOF pose, 3D coordinates), RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Music-Driven Group Choreography</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.12337"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://aioz-ai.github.io/AIOZ-GDANCE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AIOZ, Singapore<br>
‚Ä¢ Dataset: AIOZ-GDANCE, Samples: 1808000, Modality: 3D Mesh + Music Audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Motion Matters: Neural Motion Transfer for Better Camera Physiological Measurement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.12059"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://motion-matters.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UNC Chapel Hill<br>
‚Ä¢ Dataset: CDVS, Samples: 90, Modality: RGB videos<br>
‚Ä¢ Dataset: MAUBFC-rPPG, Samples: 42, Modality: RGB videos<br>
‚Ä¢ Dataset: MAPURE, Samples: 59, Modality: RGB videos<br>
‚Ä¢ Dataset: MASCAMPS-200, Samples: 200, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Propagate And Calibrate: Real-time Passive Non-line-of-sight Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.11791"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/againstentropy/NLOS-Track/"><img src="https://img.shields.io/github/stars/NLOS-Track/.svg?style=social&label=Star"></a><br><a href="https://againstentropy.github.io/NLOS-Track/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shanghai AI Laboratory<br>
‚Ä¢ Dataset: NLOS-Track, Samples: 1500, Modality: RGB videos + 2D trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Learning Optical Flow from Event Camera with Rendered Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.11011"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: MDR, Samples: 80000, Modality: event streams + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>EarCough: Enabling Continuous Subject Cough Event Detection on Hearables</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.10445"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Technology, Tsinghua University<br>
‚Ä¢ Dataset: Synchronous Audio and Motion Dataset for Subject Cough Detection (name not explicitly given), Samples: None, Modality: 6-axis IMU (3-axis accelerometer, 3-axis gyroscope) and dual-channel audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Highly Efficient 3D Human Pose Tracking from Events with Spiking Spatiotemporal Transformer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.09681"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JimmyZou/HumanPoseTrackingSNN"><img src="https://img.shields.io/github/stars/JimmyZou/HumanPoseTrackingSNN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences<br>
‚Ä¢ Dataset: SynEventHPD, Samples: 9197, Modality: Synthetic event streams + SMPL annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Classification of Primitive Manufacturing Tasks from Filtered Event Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.09558"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Centre for Mechanical Engineering, Materials and Processes (CEMMPRE), University of Coimbra, 3030-788, Coimbra, Portugal<br>
‚Ä¢ Dataset: Dataset of Manufacturing Tasks (DMT22), Samples: 72, Modality: event camera data, RGB-D videos, electromagnetic pose data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.09095"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://www.lidarhumanmotion.net/sloper4d/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Xiamen University, China<br>
‚Ä¢ Dataset: SLOPER4D, Samples: 15, Modality: LiDAR point clouds, RGB videos, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Evaluating gesture generation in a large-scale open challenge: The GENEA Challenge 2022</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.08737"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/GENEA-workshop/genea_challenge_2022"><img src="https://img.shields.io/github/stars/GENEA-workshop/genea_challenge_2022.svg?style=social&label=Star"></a><br><a href="https://youngwoo-yoon.github.io/GENEAchallenge2022"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SEED ‚Äì Electronic Arts (EA), Sweden<br>
‚Ä¢ Dataset: GENEA Challenge 2022 Dataset, Samples: 18 hours (training set), Modality: 3D full-body MoCap joints (including fingers), speech audio, text transcriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Unsupervised Contour Tracking of Live Cells by Mechanical and Cycle Consistency Losses</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.08364"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JunbongJang/contour-tracking/"><img src="https://img.shields.io/github/stars/contour-tracking/.svg?style=social&label=Star"></a><br><a href="https://junbongjang.github.io/projects/contour-tracking/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: KAIST<br>
‚Ä¢ Dataset: Live Cell Sparse Contour Tracking Labels, Samples: 13, Modality: Phase contrast & confocal fluorescence microscopy videos + sparse point trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>A large-scale multimodal dataset of human speech recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.08295"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/G-Bob/Multimodal-dataset-for-human-speech-recognition"><img src="https://img.shields.io/github/stars/G-Bob/Multimodal-dataset-for-human-speech-recognition.svg?style=social&label=Star"></a><br><a href="https://nextcloud.gla.ac.uk/s/LJHKyBxLHXdk4xZ"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: James Watt School of Engineering, University of Glasgow, Glasgow, G12 8QQ,UK<br>
‚Ä¢ Dataset: A large-scale multimodal dataset of human speech recognition, Samples: Approx. 400 minutes from 20 participants, performing tasks including speaking 5 vowels, 15 words, and 16 sentences., Modality: UWB radar, mmWave radar, laser speckle patterns, audio, mouth video, mouth skeleton points<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>ForDigitStress: A multi-modal stress dataset employing a digital job interview scenario</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.07742"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://hcai.eu/fordigitstress"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Lab for Human-Centered AI, Augsburg University, Augsburg, Germany<br>
‚Ä¢ Dataset: ForDigitStress, Samples: 40, Modality: 3D Skeleton data (Kinect), 2D pose trajectories (OpenPose), facial landmarks, action units, head pose, RGB video, audio, PPG, EDA, eye-tracking video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>BlinkFlow: A Dataset to Push the Limits of Event-based Optical Flow Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.07716"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://zju3dv.github.io/blinkflow/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: State Key Lab of CAD&CG, Zhejiang University<br>
‚Ä¢ Dataset: BlinkFlow, Samples: 3587, Modality: Event stream + optical flow ground truth + RGB images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>DisCoHead: Audio-and-Video-Driven Talking Head Generation by Disentangled Control of Head Pose and Facial Expressions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.07697"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/deepbrainai-research/koeba"><img src="https://img.shields.io/github/stars/deepbrainai-research/koeba.svg?style=social&label=Star"></a><br><a href="https://deepbrainai-research.github.io/discohead/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DeepBrain AI Inc., Seoul, Korea<br>
‚Ä¢ Dataset: Korean election broadcast addresses dataset (KoEBA), Samples: None, Modality: audio-video<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>InsMOS: Instance-Aware Moving Object Segmentation in LiDAR Data</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.03909"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/nubot-nudt/InsMOS"><img src="https://img.shields.io/github/stars/nubot-nudt/InsMOS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China<br>
‚Ä¢ Dataset: SemanticKITTI with 3D Bounding Box Instances, Samples: None, Modality: LiDAR<br>
‚Ä¢ Dataset: KITTI-road with 3D Bounding Box Instances, Samples: None, Modality: LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Event Voxel Set Transformer for Spatiotemporal Representation Learning on Event Streams</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.03856"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/bochenxie/NeuroHAR"><img src="https://img.shields.io/github/stars/bochenxie/NeuroHAR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering, City University of Hong Kong, Hong Kong SAR, China<br>
‚Ä¢ Dataset: Neuromorphic Human Action Recognition (NeuroHAR), Samples: 1584, Modality: event, RGB, and depth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>EvHandPose: Event-based 3D Hand Pose Estimation with Sparse Supervision</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.02862"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Jianping-Jiang/EvHandPose"><img src="https://img.shields.io/github/stars/Jianping-Jiang/EvHandPose.svg?style=social&label=Star"></a><br><a href="https://www.pku-vcl.com/project/EvHandPose/main.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory for Multimedia Information Processing, National Engineering Research Center of Visual Technology, and AI Innovation Center, School of Computer Science, Peking University, Beijing 100871, China<br>
‚Ä¢ Dataset: EvRealHands, Samples: 102, Modality: Event streams, RGB images, 3D hand pose and shape annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.01943"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://spring-benchmark.org"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Visualization and Interactive Systems, University of Stuttgart<br>
‚Ä¢ Dataset: Spring, Samples: 23812, Modality: RGB videos + scene flow + optical flow + stereo disparity<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.01765"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/XingqunQi/Diverse-3D-Hand-Gesture-Prediction"><img src="https://img.shields.io/github/stars/XingqunQi/Diverse-3D-Hand-Gesture-Prediction.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AAII, University of Technology Sydney; Netease Fuxi AI Lab<br>
‚Ä¢ Dataset: TED Hands, Samples: 134456, Modality: 3D axis-angle joint representations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Mar 2023</td>
  <td style="width:70%;"><strong>UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2303.00938"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://pku-epic.github.io/UniDexGrasp/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Peking University<br>
‚Ä¢ Dataset: UniDexGrasp's synthesized dexterous grasp dataset, Samples: 1120000, Modality: Robotic hand kinematics (root rotation, root translation, joint angles) for static grasps on 5519 object instances.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Tracking Fast by Learning Slow: An Event-based Speed Adaptive Hand Tracker Leveraging Knowledge in RGB Domain</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.14430"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ChuanlinLan/ESAHT"><img src="https://img.shields.io/github/stars/ChuanlinLan/ESAHT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: City University of Hong Kong<br>
‚Ä¢ Dataset: Event-based Speed Adaptive Hand Tracker (ESAHT) Dataset, Samples: 44, Modality: event streams + RGB videos + 3D hand pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Learning to Super-Resolve Blurry Images with Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.13766"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ShinyWang33/eSL-Net-Plusplus"><img src="https://img.shields.io/github/stars/ShinyWang33/eSL-Net-Plusplus.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University<br>
‚Ä¢ Dataset: synthetic GoPro dataset, Samples: 270 video sequences, Modality: HR clear images, LR blurry images, Event streams<br>
‚Ä¢ Dataset: RWS (Real-World Scenes), Samples: None, Modality: real-world events, real-world blurry APS frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>FLSea: Underwater Visual-Inertial and Stereo-Vision Forward-Looking Datasets</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.12772"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.kaggle.com/datasets/viseaonlab/flsea-vi"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ViSEAon Marine Imaging Lab, Department of Marine Technologies, University of Haifa, Haifa, Israel<br>
‚Ä¢ Dataset: FLSea, Samples: 17, Modality: underwater stereo videos, monocular videos + IMU data, ground truth depth maps and camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Causal Explanations for Sequential Decision-Making in Multi-Agent Systems</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.10809"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/uoe-agents/cema"><img src="https://img.shields.io/github/stars/uoe-agents/cema.svg?style=social&label=Star"></a><br><a href="https://datashare.ed.ac.uk/handle/10283/8714"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Edinburgh<br>
‚Ä¢ Dataset: HEADD, Samples: 1308, Modality: RGB videos + text explanations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Stable Motion Primitives via Imitation and Contrastive Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.10017"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rperezdattari/Stable-Motion-Primitives-via-Imitation-and-Contrastive-Learning"><img src="https://img.shields.io/github/stars/rperezdattari/Stable-Motion-Primitives-via-Imitation-and-Contrastive-Learning.svg?style=social&label=Star"></a><br><a href="https://youtu.be/OM-2edHBRfc"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Delft University of Technology<br>
‚Ä¢ Dataset: LAIR handwriting dataset, Samples: 10, Modality: mouse interface (2D position and velocity)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Anticipating Next Active Objects for Egocentric Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.06358"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sanket-thakur/ANACTO"><img src="https://img.shields.io/github/stars/sanket-thakur/ANACTO.svg?style=social&label=Star"></a><br><a href="https://sanket-thakur.github.io/ANACTO/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Pattern Analysis and Computer Vision (PA VIS), Istituto Italiano di Tecnologia (IIT), Department of Electrical, Electronics and Telecommunication Engineering and Naval Architecture (DITEN), University of Genoa, Italy<br>
‚Ä¢ Dataset: EpicKitchens-100 ANACTO Annotations, Samples: None, Modality: RGB videos + bounding box annotations<br>
‚Ä¢ Dataset: EGTEA+ ANACTO Annotations, Samples: None, Modality: RGB videos + bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>A Neuromorphic Dataset for Object Segmentation in Indoor Cluttered Environment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.06301"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/yellow07200/ESD_labeling_tool"><img src="https://img.shields.io/github/stars/yellow07200/ESD_labeling_tool.svg?style=social&label=Star"></a><br><a href="https://figshare.com/s/7cf0e84fe8e7b9f7ae42"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Advanced Research and Innovation Center (ARIC), Khalifa University, Abu Dhabi, UAE<br>
‚Ä¢ Dataset: Event-based Segmentation Dataset (ESD), Samples: 145, Modality: Event streams, RGBD frames, Robot end-effector pose and velocity<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.05991"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/augcog/DTTDv1"><img src="https://img.shields.io/github/stars/augcog/DTTDv1.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California, Berkeley<br>
‚Ä¢ Dataset: Digital Twin Tracking Dataset (DTTD), Samples: 103, Modality: RGB-D videos + MoCap camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for Indoor Localization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.05309"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ilaydayaman/LuViRA Dataset"><img src="https://img.shields.io/github/stars/ilaydayaman/LuViRA Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Lund University, Department of Electrical and Information Technology<br>
‚Ä¢ Dataset: LuViRA, Samples: 89, Modality: 6DOF pose ground truth, IMU, RGB-D images, 5G radio channel estimates, audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>MMPD: Multi-Domain Mobile Video Physiology Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.03840"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/THU-CS-PI/MMPD_rPPG_dataset"><img src="https://img.shields.io/github/stars/THU-CS-PI/MMPD_rPPG_dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua University<br>
‚Ä¢ Dataset: MMPD, Samples: 660, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Feb 2023</td>
  <td style="width:70%;"><strong>Motion ID: Human Authentication Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2302.01751"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SamsungLabs/MotionID"><img src="https://img.shields.io/github/stars/SamsungLabs/MotionID.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Samsung R&D Institute Rus<br>
‚Ä¢ Dataset: MotionID: IMU all motions, Samples: None, Modality: IMU (accelerometer, magnetometer, gyroscope, rotation sensor)<br>
‚Ä¢ Dataset: MotionID: IMU specific motion, Samples: 30300, Modality: IMU (accelerometer, magnetometer, gyroscope, rotation sensor)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>GyroFlow+: Gyroscope-Guided Unsupervised Deep Homography and Optical Flow Learning</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.10018"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Information and Communication Engineering, University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: GHOF, Samples: 10000+ pairs, Modality: RGB videos + gyroscope readings + optical flow + homography<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Learning to View: Decision Transformers for Active Object Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.09544"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Amazon Lab126, Sunnyvale, CA 94098, USA; Carnegie Mellon University, Pittsburgh, PA 15213, USA<br>
‚Ä¢ Dataset: Unnamed Interactive Dataset (from AI2THOR), Samples: approx. 1000 initial poses in each of the 4 scene categories, Modality: Robot pose trajectories with RGB and depth images<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.09489"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Sapienza University of Rome, Italy<br>
‚Ä¢ Dataset: HR-UBnormal, Samples: 234751, Modality: kinematic skeletons<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Dance2MIDI: Dance-driven multi-instruments music generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.09080"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ZJUKG/Dance2MIDI"><img src="https://img.shields.io/github/stars/ZJUKG/Dance2MIDI.svg?style=social&label=Star"></a><br><a href="https://dance2midi.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Computer Science and Technology, Zhejiang University<br>
‚Ä¢ Dataset: D2MIDI, Samples: 71754, Modality: RGB videos + 3D keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>HMDO: Markerless Multi-view Hand Manipulation Capture with Deformable Objects</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.07652"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southeast University, China<br>
‚Ä¢ Dataset: HMDO (Hand Manipulation with Deformable Objects), Samples: 12, Modality: multi-view synchronized images, 3D meshes, contact deformation maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic Environment</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.06648"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="bit.ly/yelan-research"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of California San Diego, USA<br>
‚Ä¢ Dataset: YeLan (Synthetic), Samples: 3958169, Modality: Event Camera + 3D joint coordinates<br>
‚Ä¢ Dataset: YeLan (Real-world), Samples: 446158, Modality: Event Camera + Motion Capture<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Deep learning-based approaches for human motion decoding in smart walkers for rehabilitation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.05575"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Microelectromechanical Systems (CMEMS), University of Minho, Guimar√£es, Portugal<br>
‚Ä¢ Dataset: Custom Smart Walker Gait Dataset (not explicitly named), Samples: 360, Modality: RGB-D videos, IMU<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>A Unified Framework for Event-based Frame Interpolation with Ad-hoc Deblurring in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.05191"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AHupuJR/REFID"><img src="https://img.shields.io/github/stars/AHupuJR/REFID.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Research Center for Optical Instrumentation, Zhejiang University, 310027 Hangzhou, China, the Robotics and Perception Group, University of Zurich, 8050 Zurich, Switzerland<br>
‚Ä¢ Dataset: HighREV, Samples: 30, Modality: RGB videos + events<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.03213"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/EGO4D/episodic-memory/tree/main/EgoTracks"><img src="https://img.shields.io/github/stars/main/EgoTracks.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta AI<br>
‚Ä¢ Dataset: EgoTracks, Samples: 22028, Modality: RGB videos + bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Augmenting Ego-Vehicle for Traffic Near-Miss and Accident Classification Dataset using Manipulating Conditional Style Translation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.02726"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jampang41/CST-S3D"><img src="https://img.shields.io/github/stars/jampang41/CST-S3D.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Big Data Integration Research Center, NICT, Tokyo, Japan<br>
‚Ä¢ Dataset: re-annotation DADA-2000 dataset, Samples: None, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.00493"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/argoverse/av2-api"><img src="https://img.shields.io/github/stars/argoverse/av2-api.svg?style=social&label=Star"></a><br><a href="https://www.argoverse.org/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Georgia Tech<br>
‚Ä¢ Dataset: Argoverse 2 Sensor Dataset, Samples: 1000, Modality: LiDAR point clouds, RGB cameras, stereo cameras, 6-DOF pose<br>
‚Ä¢ Dataset: Argoverse 2 Lidar Dataset, Samples: 20000, Modality: LiDAR point clouds, 6-DOF pose<br>
‚Ä¢ Dataset: Argoverse 2 Motion Forecasting Dataset, Samples: 250000, Modality: Object trajectories (2D position, velocity, heading), HD vector maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jan 2023</td>
  <td style="width:70%;"><strong>Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2301.00411"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Luciferbobo/D4NeRF"><img src="https://img.shields.io/github/stars/Luciferbobo/D4NeRF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: PhiGent Robotics<br>
‚Ä¢ Dataset: urban driving scenes, Samples: None, Modality: monocular RGB videos, depth, optical flow, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>X-MAS: Extremely Large-Scale Multi-Modal Sensor Dataset for Outdoor Surveillance in Real Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.14574"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lge-robot-navi"><img src="https://img.shields.io/github/stars/github.com/lge-robot-navi.svg?style=social&label=Star"></a><br><a href="http://gofile.me/6GfMG/eYjbJSjvF"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical Engineering at Korea Advanced Institute of Science and Technology (KAIST); Advanced Robotics Lab. at LG Electronics<br>
‚Ä¢ Dataset: X-MAS, Samples: 2624, Modality: RGB, thermal, IR, night vision, depth, LiDAR<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>NeMo: 3D Neural Motion Fields from Multiple Video Instances of the Same Action</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.13660"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/nemo-neural-motion-field"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: Our MoCap Dataset, Samples: 40, Modality: MoCap + RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>ESVIO: Event-based Stereo Visual Inertial Odometry</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.13184"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arclab-hku/ESVIO"><img src="https://img.shields.io/github/stars/arclab-hku/ESVIO.svg?style=social&label=Star"></a><br><a href="https://github.com/arclab-hku/Event based VO-VIO-SLAM"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Hong Kong<br>
‚Ä¢ Dataset: HKU Dataset, Samples: 9, Modality: Stereo event streams, stereo images, IMU, VICON ground truth poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Full-Body Articulated Human-Object Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.10621"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jnnan.github.io/project/chairs/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Intelligence Science and Technology, Peking University; Beijing Institute of General Artificial Intelligence (BIGAI)<br>
‚Ä¢ Dataset: CHAIRS, Samples: 1390, Modality: MoCap, multi-view RGB-D sequences, 3D meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP Benchmark</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.08914"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/JeffWang987/ASAP"><img src="https://img.shields.io/github/stars/JeffWang987/ASAP.svg?style=social&label=Star"></a><br><a href="https://github.com/JeffWang987/ASAP"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CASIA<br>
‚Ä¢ Dataset: nuScenes-H, Samples: 1,200,000 annotated images (1M training, 0.2M validation), Modality: Surround-view RGB images + 3D bounding box annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Automatic vehicle trajectory data reconstruction at scale</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.07907"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="not available"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Civil and Environmental Engineering, Vanderbilt University, United States; Institute for Software Integrated Systems, Vanderbilt University, United States<br>
‚Ä¢ Dataset: Trajectory Reconciliation Benchmark Datasets, Samples: 5, Modality: vehicle trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object Interactions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.07626"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ShanghaiTech University<br>
‚Ä¢ Dataset: HODome, Samples: 274, Modality: multi-view RGB videos + MoCap data (markers, SMPL-X parameters, keypoints)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Accidental Turntables: Learning 3D Pose by Watching Objects Turn</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.06300"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://people.cs.umass.edu/~zezhoucheng/acci-turn/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Massachusetts, Amherst<br>
‚Ä¢ Dataset: Accidental Turntables Dataset, Samples: 313, Modality: RGB videos + 3D pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Ego-Body Pose Estimation via Ego-Head Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.04636"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: AMASS-Replica-Ego-Syn (ARES), Samples: 1664616, Modality: RGB videos + 3D human motions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>MIME: Human-Aware 3D Scene Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.04360"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://mime.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T ¬®ubingen, Germany<br>
‚Ä¢ Dataset: 3D FRONT HUMAN, Samples: None, Modality: 3D scenes populated with SMPL-X human models<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.03741"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Tsinghua Shenzhen International Graduate School, Tsinghua University<br>
‚Ä¢ Dataset: FineDance, Samples: 346, Modality: MoCap joints (52 joints), SMPL parameters, fbx files, multi-view videos, music<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Privacy-Preserving Visual Localization with Event Cameras</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.03177"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/82magnolia/event_localization"><img src="https://img.shields.io/github/stars/82magnolia/event_localization.svg?style=social&label=Star"></a><br><a href="https://82magnolia.github.io/event_localization/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Seoul National University<br>
‚Ä¢ Dataset: EvRooms, Samples: 23345, Modality: Event streams + 6DoF poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Muscles in Action</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.02978"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Columbia University<br>
‚Ä¢ Dataset: Muscles in Action (MIA), Samples: 15000, Modality: synchronized RGB video and surface electromyography (sEMG) signals<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Dec 2022</td>
  <td style="width:70%;"><strong>Minimum Latency Deep Online Video Stabilization</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2212.02073"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/liuzhen03/NNDVS"><img src="https://img.shields.io/github/stars/liuzhen03/NNDVS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: MotionStab, Samples: 65238, Modality: paired unstable/stable camera motion (meshflow)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>Reconstructing Hand-Held Objects from Monocular Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.16835"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://dihuangdh.github.io/hhor"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Sydney<br>
‚Ä¢ Dataset: Hand-held Object Dataset (HOD), Samples: 35, Modality: Monocular 4K RGB videos + 3D ground truth meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>Comparison of Motion Encoding Frameworks on Human Manipulation Actions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.13024"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://doi.org/10.5281/zenodo.7351664"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Third Institute of Physics, Dept. Computational Neuroscience, University of G ¬®ottingen, 37073 G ¬®ottingen, Germany<br>
‚Ä¢ Dataset: Human Manipulation Actions Dataset, Samples: 7652, Modality: 3D hand trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>Tensor4D : Efficient Neural 4D Decomposition for High-fidelity Dynamic Reconstruction and Rendering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.11610"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/DSaurus/Tensor4D"><img src="https://img.shields.io/github/stars/DSaurus/Tensor4D.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Automation, Tsinghua University<br>
‚Ä¢ Dataset: Tensor4D multiview human motion dataset, Samples: 9, Modality: multi-view synchronized RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>Blur Interpolation Transformer for Real-World Motion from Blur</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.11423"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zzh-tech/BiT"><img src="https://img.shields.io/github/stars/zzh-tech/BiT.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo, Japan; National Institute of Informatics, Japan<br>
‚Ä¢ Dataset: RBI, Samples: 55, Modality: low-frame-rate blurred videos and high-frame-rate sharp videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>H-VFI: Hierarchical Frame Interpolation for Videos with Large Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.11309"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Kuaishou Technology<br>
‚Ä¢ Dataset: YouTube200K, Samples: 200000, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>Leveraging Multi-stream Information Fusion for Trajectory Prediction in Low-illumination Scenarios: A Multi-channel Graph Convolutional Approach</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.10226"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/TommyGong08/MSIF"><img src="https://img.shields.io/github/stars/TommyGong08/MSIF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Mechanical Engineering, Beijing Institute of Technology<br>
‚Ä¢ Dataset: Dark-HEV-I, Samples: 230, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>3d human motion generation from the text via gesture action classification and the autoregressive model</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.10003"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/GT-KIM/motion_generation_from_text"><img src="https://img.shields.io/github/stars/GT-KIM/motion_generation_from_text.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical Engineering, Korea University, Seoul, South Korea<br>
‚Ä¢ Dataset: action-based gesture dataset, Samples: 1200, Modality: MoCap joints<br>
‚Ä¢ Dataset: Gesture Action Classification dataset, Samples: 1309, Modality: text sentences with labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>HARDVS: Revisiting Human Activity Recognition with Dynamic Vision Sensors</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.09648"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Event-AHU/HARDVS"><img src="https://img.shields.io/github/stars/Event-AHU/HARDVS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science and Technology, Anhui University, Hefei, China<br>
‚Ä¢ Dataset: HARDVS, Samples: 107646, Modality: DVS event streams + RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>Detecting Line Segments in Motion-blurred Images with Events</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.07365"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/levenberg/FE-LSD"><img src="https://img.shields.io/github/stars/levenberg/FE-LSD.svg?style=social&label=Star"></a><br><a href="https://levenberg.github.io/FE-LSD"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electronic Information, Wuhan University<br>
‚Ä¢ Dataset: FE-Wireframe, Samples: 5462, Modality: motion-blurred RGB images + event data + line annotations<br>
‚Ä¢ Dataset: FE-Blurframe, Samples: 800, Modality: motion-blurred RGB images + event streams + line annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.05709"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/lisiyao21/AnimeRun"><img src="https://img.shields.io/github/stars/lisiyao21/AnimeRun.svg?style=social&label=Star"></a><br><a href="https://lisiyao21.github.io/projects/AnimeRun"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: S-Lab, Nanyang Technological University<br>
‚Ä¢ Dataset: AnimeRun, Samples: 30, Modality: RGB videos + optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>CRIPP-VQA: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.03779"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Maitreyapatel/CRIPP-VQA/"><img src="https://img.shields.io/github/stars/CRIPP-VQA/.svg?style=social&label=Star"></a><br><a href="https://maitreyapatel.com/CRIPP-VQA/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Arizona State University<br>
‚Ä¢ Dataset: CRIPP-VQA, Samples: 7000, Modality: Synthetic RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>TAP-Vid: A Benchmark for Tracking Any Point in a Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.03726"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/deepmind/tapnet"><img src="https://img.shields.io/github/stars/deepmind/tapnet.svg?style=social&label=Star"></a><br><a href="https://github.com/deepmind/tapnet"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: DeepMind<br>
‚Ä¢ Dataset: TAP-Vid-Kinetics, Samples: 1189, Modality: RGB videos + point tracks<br>
‚Ä¢ Dataset: TAP-Vid-DAVIS, Samples: 30, Modality: RGB videos + point tracks<br>
‚Ä¢ Dataset: TAP-Vid-Kubric, Samples: 799, Modality: Synthetic RGB videos + ground-truth point tracks<br>
‚Ä¢ Dataset: TAP-Vid-RGB-Stacking, Samples: 50, Modality: Synthetic RGB videos + ground-truth point tracks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Nov 2022</td>
  <td style="width:70%;"><strong>UmeTrack: Unified multi-view end-to-end hand tracking for VR</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2211.00099"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Meta Reality Labs, USA<br>
‚Ä¢ Dataset: Large-scale egocentric hand tracking dataset, Samples: 1397 real sequences and 1397 synthetic sequences, Modality: Multi-view monochrome video from headset-mounted cameras, 3D hand poses (from motion capture)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>InGVIO: A Consistent Invariant Filter for Fast and High-Accuracy GNSS-Visual-Inertial Odometry</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.15145"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ChangwuLiu/InGVIO"><img src="https://img.shields.io/github/stars/ChangwuLiu/InGVIO.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Aerospace Engineering, Tsinghua University<br>
‚Ä¢ Dataset: fixed-wing datasets (fwgvieasy, fwgvimedium, fwgvihard), Samples: 3, Modality: Stereo images + IMU + raw GNSS<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>Monocular Dynamic View Synthesis: A Reality Check</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.13445"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/hangg7/dycheck"><img src="https://img.shields.io/github/stars/hangg7/dycheck.svg?style=social&label=Star"></a><br><a href="https://hangg7.com/dycheck"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: UC Berkeley<br>
‚Ä¢ Dataset: iPhone dataset, Samples: 14, Modality: RGB videos + Lidar depth + annotated keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>BlanketGen - A synthetic blanket occlusion augmentation pipeline for MoCap datasets</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.12035"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://gitlab.inesctec.pt/brain-lab/brain-lab-public/blanket-gen-releases"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Biomedical Engineering Research, INESC TEC, Porto, Portugal<br>
‚Ä¢ Dataset: BlanketGen-3DPW, Samples: 1037, Modality: RGB videos + 3D pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>JRDB-Pose: A Large-scale Dataset for Multi-Person Pose Estimation and Tracking</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.11940"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://jrdb.erc.monash.edu/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: JRDB-Pose, Samples: 5022 pose tracks, Modality: RGB videos, LiDAR point clouds, human pose annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>A Clinical Dataset for the Evaluation of Motion Planners in Medical Applications</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.10834"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UNC-Robotics/Med-MPD"><img src="https://img.shields.io/github/stars/UNC-Robotics/Med-MPD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, University of North Carolina at Chapel Hill<br>
‚Ä¢ Dataset: Medical Motion Planning Dataset (Med-MPD), Samples: 15, Modality: 3D binary maps (from medical images) + start/target poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.09729"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://silverster98.github.io/HUMANISE/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer Science & Technology, Beijing Institute of Technology<br>
‚Ä¢ Dataset: HUMANISE, Samples: 19600, Modality: SMPL-X motion sequences in 3D scene point clouds with language descriptions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>Neural Contact Fields: Tracking Extrinsic Contact with Tactile Sensing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.09297"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/carolinahiguera/NCF"><img src="https://img.shields.io/github/stars/carolinahiguera/NCF.svg?style=social&label=Star"></a><br><a href="https://github.com/carolinahiguera/NCF"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Washington<br>
‚Ä¢ Dataset: YCB-Extrinsic-Contact, Samples: 4500, Modality: Tactile images (DIGIT), end-effector poses, object point clouds<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>INSANE: Cross-Domain UAV Data Sets with Increased Number of Sensors for developing Advanced and Novel Estimators</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.09114"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sst.aau.at/cns/datasets"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Control of Networked Systems Group of the University of Klagenfurt, Austria<br>
‚Ä¢ Dataset: INSANE, Samples: 27, Modality: 6-DoF trajectories, multiple IMUs, mono and stereo cameras, RTK GNSS, UWB, Laser Range Finder, Magnetometer, Motion Capture<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>Self-Improving SLAM in Dynamic Environments: Learning When to Mask</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.08350"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universit√© Paris-Saclay, CEA, List F-91120, Palaiseau, France<br>
‚Ä¢ Dataset: ConsInv, Samples: None, Modality: Monocular and Stereo RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>Weakly-Supervised Optical Flow Estimation for Time-of-Flight</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.05298"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/schellmi42/WFlowToF"><img src="https://img.shields.io/github/stars/schellmi42/WFlowToF.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ulm University<br>
‚Ä¢ Dataset: CB-dataset extension, Samples: 14, Modality: Simulated raw iToF measurements<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>Robustness Certification of Visual Perception Models via Camera Motion Smoothing</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.04625"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HanjiangHu/camera-motion-smoothing"><img src="https://img.shields.io/github/stars/HanjiangHu/camera-motion-smoothing.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Carnegie Mellon University<br>
‚Ä¢ Dataset: MetaRoom, Samples: 500 training camera poses and 120 testing camera poses per object (20 objects total), Modality: RGB images, dense point clouds, camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>OGC: Unsupervised 3D Object Segmentation from Rigid Dynamics of Point Clouds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.04458"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/vLAR-group/OGC"><img src="https://img.shields.io/github/stars/vLAR-group/OGC.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: vLAR Group, The Hong Kong Polytechnic University<br>
‚Ä¢ Dataset: OGC-DR, Samples: 5000, Modality: Point Cloud<br>
‚Ä¢ Dataset: OGC-DRSV, Samples: 5000, Modality: Point Cloud<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>BlanketSet -- A clinical real-world in-bed action recognition and qualitative semi-synchronised MoCap dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.03600"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://rdm.inesctec.pt/dataset/nis-2022-004"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Biomedical Engineering Research, INESC TEC, Porto, Portugal; Faculty of Engineering (FEUP), University of Porto, Porto, Portugal<br>
‚Ä¢ Dataset: BlanketSet, Samples: 405, Modality: RGB-IR-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>GLAD: Grounded Layered Autonomous Driving for Complex Service Tasks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.02302"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, SUNY Binghamton<br>
‚Ä¢ Dataset: IMDataset, Samples: 13800, Modality: Sequences of four RGB images from four simulated cameras (front, back, left, right) with safety labels for lane merging behaviors.<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>A Benchmark for Multi-Modal Lidar SLAM with Ground Truth in GNSS-Denied Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.00812"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/TIERS/tiers-lidars-dataset-enhanced"><img src="https://img.shields.io/github/stars/TIERS/tiers-lidars-dataset-enhanced.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Turku Intelligent Embedded and Robotic Systems (TIERS) Lab, University of Turku, Finland.<br>
‚Ä¢ Dataset: TIERS Lidar Dataset Enhanced, Samples: 17, Modality: Multi-modal LiDAR, IMU, cameras, with 6-DOF pose trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Oct 2022</td>
  <td style="width:70%;"><strong>WorldGen: A Large Scale Generative Simulator</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2210.00715"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://prg.cs.umd.edu/WorldGen"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Perception and Robotics Group, University of Maryland Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, USA<br>
‚Ä¢ Dataset: WorldGen, Samples: None, Modality: RGB videos + optical flow, depth, surface normals, semantic segmentation, event data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Motion and Appearance Adaptation for Cross-Domain Motion Transfer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.14529"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Electronic Science and Technology of China<br>
‚Ä¢ Dataset: Mixamo-Video Dataset, Samples: 690, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>NEURAL MARIONETTE: A Transformer-based Multi-action Human Motion Synthesis System</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.13204"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/wjohnnyw/NeuralMarionette"><img src="https://img.shields.io/github/stars/wjohnnyw/NeuralMarionette.svg?style=social&label=Star"></a><br><a href="https://wjohnnyw.github.io/blog/tag2motion/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Faculty of Information Technology, Monash University, Melbourne 3168, Australia<br>
‚Ä¢ Dataset: BABEL-MAG (BABEL for Multi-Action Generation), Samples: 7643, Modality: MoCap joint rotations and root translation<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Assessing the Role of Datasets in the Generalization of Motion Deblurring Methods to Real Images</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.12675"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/GuillermoCarbajal/SBDD"><img src="https://img.shields.io/github/stars/GuillermoCarbajal/SBDD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: IIE, Facultad de Ingenier¬¥ ƒ±a, Universidad de la Rep¬¥ ublica, Herrera y Reissig 565, Montevideo, 11500, Uruguay.<br>
‚Ä¢ Dataset: SBDD (Segmentation-Based Deblurring Dataset), Samples: None, Modality: RGB images (sharp/blurred pairs)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Real-RawVSR: Real-World Raw Video Super-Resolution with a Benchmark Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.12475"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zmzhang1998/Real-RawVSR"><img src="https://img.shields.io/github/stars/zmzhang1998/Real-RawVSR.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical and Information Engineering, Tianjin University, Tianjin, China<br>
‚Ä¢ Dataset: Real-RawVSR, Samples: 450, Modality: raw videos + sRGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.12354"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://intercap.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T ¬®ubingen, Germany<br>
‚Ä¢ Dataset: InterCap, Samples: 223, Modality: multi-view RGB-D videos + 3D meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>PL-EVIO: Robust Monocular Event-based Visual Inertial Odometry with Point and Line Features</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.12160"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/arclab-hku/Event_based_VO-VIO-SLAM_dataset"><img src="https://img.shields.io/github/stars/arclab-hku/Event_based_VO-VIO-SLAM_dataset.svg?style=social&label=Star"></a><br><a href="https://youtu.be/KnWZ4anBMK4"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Adaptive Robotic Controls Lab (ArcLab), Department of Mechanical Engineering, Faculty of Engineering, The University of Hong Kong, Hong Kong SAR, China<br>
‚Ä¢ Dataset: Event based VO-VIO-SLAM dataset, Samples: 11, Modality: event camera data, standard camera images, IMU measurements, VICON ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.12009"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: CFCS, Peking University; Beijing Institute for General AI<br>
‚Ä¢ Dataset: SimGrasp, Samples: 1810, Modality: depth point cloud sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>T3VIP: Transformation-based 3D Video Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.11693"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://t3vip.cs.uni-freiburg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Freiburg<br>
‚Ä¢ Dataset: DexHand, Samples: 10000, Modality: RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Learning Interpretable Dynamics from Images of a Freely Rotating 3D Rigid Body</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.11355"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/jjmason687/LearningSO3fromImages"><img src="https://img.shields.io/github/stars/jjmason687/LearningSO3fromImages.svg?style=social&label=Star"></a><br><a href="https://www.dropbox.com/sh/menv3lu9mquu1wh/AABovQ53udtryDC24xPLGw17a?dl=0"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical and Aerospace Engineering, Princeton University<br>
‚Ä¢ Dataset: Uniform density cube, Samples: 1000, Modality: Rendered RGB videos<br>
‚Ä¢ Dataset: Uniform density prism, Samples: 1000, Modality: Rendered RGB videos<br>
‚Ä¢ Dataset: Non-uniform density cube, Samples: 1000, Modality: Rendered RGB videos<br>
‚Ä¢ Dataset: Non-uniform density prism, Samples: 1000, Modality: Rendered RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>T2FPV: Dataset and Method for Correcting First-Person View Errors in Pedestrian Trajectory Prediction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.11294"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/cmubig/T2FPV"><img src="https://img.shields.io/github/stars/cmubig/T2FPV.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Computer Science Dept., Carnegie Mellon University<br>
‚Ä¢ Dataset: T2FPV-ETH, Samples: 49115, Modality: Synthetic first-person view RGB videos, instance segmentation masks, 2D ground-plane trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Visual Localization and Mapping in Dynamic and Changing Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.10710"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical Engineering, University of Illinois at Urbana-Champaign<br>
‚Ä¢ Dataset: PUC-USP dataset, Samples: 6, Modality: RGB-D videos + MoCap trajectory<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>FT-HID: A Large Scale RGB-D Dataset for First and Third Person Human Interaction Analysis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.10155"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/guozih/FT-HID-Dataset"><img src="https://img.shields.io/github/stars/guozih/FT-HID-Dataset.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Electrical and Automation Engineering, Tianjin University, Tianjin, China<br>
‚Ä¢ Dataset: FT-HID dataset, Samples: 38364, Modality: RGB videos, depth maps, 3D skeleton sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Decentralized Vehicle Coordination: The Berkeley DeepDrive Drone Dataset and Consensus-Based Models</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.08763"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/b3d-project/b3d"><img src="https://img.shields.io/github/stars/b3d-project/b3d.svg?style=social&label=Star"></a><br><a href="https://github.com/b3d-project/b3d"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering at Cornell University, Department of Electrical Engineering and Computer Sciences at the University of California, Berkeley<br>
‚Ä¢ Dataset: Berkeley DeepDrive Drone (B3D) dataset, Samples: 20, Modality: aerial drone videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Imitrob: Imitation Learning Dataset for Training and Evaluating 6D Object Pose Estimators</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.07976"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/imitrob/imitrob_dataset_code"><img src="https://img.shields.io/github/stars/imitrob/imitrob_dataset_code.svg?style=social&label=Star"></a><br><a href="http://imitrob.ciirc.cvut.cz/imitrobdataset.php"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague, Czech Republic<br>
‚Ä¢ Dataset: Imitrob, Samples: 352, Modality: RGB-D videos + 6D object poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.07556"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS"><img src="https://img.shields.io/github/stars/ubisoft/ubisoft-laforge-ZeroEGGS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Ubisoft, Canada; York University, Canada<br>
‚Ä¢ Dataset: unnamed, Samples: 67, Modality: MoCap joints, Audio<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>A Temporal Densely Connected Recurrent Network for Event-based Human Pose Estimation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.07034"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/xavier-zw/tDenseRNN pose"><img src="https://img.shields.io/github/stars/xavier-zw/tDenseRNN pose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: College of Information Science and Engineering, Hunan Normal University, 36 Lushan Road, Changsha, China<br>
‚Ä¢ Dataset: CDEHP, Samples: 500, Modality: event streams, RGB color frames, depth frames<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>TEAM: a parameter-free algorithm to teach collaborative robots motions from user demonstrations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.06940"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SchindlerReGIS/team"><img src="https://img.shields.io/github/stars/SchindlerReGIS/team.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ¬¥Ecole polytechnique f¬¥ ed¬¥ erale de Lausanne (EPFL), Lausanne, Switzerland<br>
‚Ä¢ Dataset: Door maintenance dataset, Samples: 21, Modality: robot joint angles<br>
‚Ä¢ Dataset: Factory worker dataset, Samples: , Modality: robot joint angles<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>COMPASS: A Formal Framework and Aggregate Dataset for Generalized Surgical Procedure Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.06424"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/UVA-DSA/COMPASS"><img src="https://img.shields.io/github/stars/UVA-DSA/COMPASS.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, 22903, VA, USA<br>
‚Ä¢ Dataset: COMPASS, Samples: 286, Modality: Robotic kinematics + Videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Picking Up Speed: Continuous-Time Lidar-Only Odometry using Doppler Velocity Measurements</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.03304"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/utiasASRL/steam_icp"><img src="https://img.shields.io/github/stars/utiasASRL/steam_icp.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Toronto Institute for Aerospace Studies (UTIAS), University of Toronto<br>
‚Ä¢ Dataset: Aeva dataset, Samples: 8, Modality: FMCW LiDAR with Doppler velocity<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>SIND: A Drone Dataset at Signalized Intersection in China</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.02297"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SOTIF-AVLab/SinD"><img src="https://img.shields.io/github/stars/SOTIF-AVLab/SinD.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China<br>
‚Ä¢ Dataset: SIND, Samples: 13248, Modality: Drone-captured videos, trajectory data (position, velocity, acceleration), HD map, traffic light states<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>Visual Odometry with Neuromorphic Resonator Networks</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.02000"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute of Neuroinformatics, University of Zurich and ETH Zurich, Switzerland<br>
‚Ä¢ Dataset: Robotics Arm Dataset, Samples: None, Modality: Event camera data + robot arm trajectory<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Sep 2022</td>
  <td style="width:70%;"><strong>A Benchmark for Unsupervised Anomaly Detection in Multi-Agent Trajectories</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2209.01838"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/againerju/r_u_maad"><img src="https://img.shields.io/github/stars/againerju/r_u_maad.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Mercedes-Benz Group AG, Institute of Measurement, Control and Microtechnology, University Ulm<br>
‚Ä¢ Dataset: R-U-MAAD, Samples: 160, Modality: 2D multi-agent trajectories, HD-maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>The Magni Human Motion Dataset: Accurate, Complex, Multi-Modal, Natural, Semantically-Rich and Contextualized</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.14925"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ¬®Orebro University, Sweden<br>
‚Ä¢ Dataset: Magni, Samples: None, Modality: Motion capture, eye-gaze, LiDAR, RGB videos, RGB-D videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Stabilize, Decompose, and Denoise: Self-Supervised Fluoroscopy Denoising</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.14022"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Z2Sky Technologies Inc.<br>
‚Ä¢ Dataset: Fluoroscopy dataset, Samples: 27, Modality: Fluoroscopy videos<br>
‚Ä¢ Dataset: Clinical dataset, Samples: 60, Modality: Low-dose X-ray videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Robust and Efficient Depth-based Obstacle Avoidance for Autonomous Miniaturized UAVs</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.12624"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ETH-PBL/Matrix-ToF-Drones"><img src="https://img.shields.io/github/stars/ETH-PBL/Matrix-ToF-Drones.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETH Zurich<br>
‚Ä¢ Dataset: Robust and Efficient Depth-based Obstacle Avoidance for Autonomous Miniaturized UAVs Dataset, Samples: 43, Modality: Grayscale video, 8x8 ToF depth matrix, UAV internal state (attitude, velocity, position), MoCap (attitude, position)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Unsupervised Spike Depth Estimation via Cross-modality Cross-domain Knowledge Transfer</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.12527"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Theia-4869/BiCross"><img src="https://img.shields.io/github/stars/Theia-4869/BiCross.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: National Key Laboratory for Multimedia Information Processing, School of CS, Peking University<br>
‚Ä¢ Dataset: Virtual KITTI spike, Samples: None, Modality: spike streams<br>
‚Ä¢ Dataset: KITTI spike, Samples: None, Modality: spike streams<br>
‚Ä¢ Dataset: Driving Stereo spike, Samples: None, Modality: spike streams<br>
‚Ä¢ Dataset: NYUv2 spike, Samples: None, Modality: spike streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Event-based Image Deblurring with Dynamic Motion Awareness</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.11398"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huawei Technologies, Zurich Research Center<br>
‚Ä¢ Dataset: RGBlur+E, Samples: 61, Modality: RGB images + event streams<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>The GENEA Challenge 2022: A large evaluation of data-driven co-speech gesture generation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.10441"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/genea-workshop/genea_challenge_2022"><img src="https://img.shields.io/github/stars/genea-workshop/genea_challenge_2022.svg?style=social&label=Star"></a><br><a href="https://youngwoo-yoon.github.io/GENEAchallenge2022/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: ETRI<br>
‚Ä¢ Dataset: GENEA Challenge 2022 Dataset, Samples: 18 hours, Modality: MoCap joints (BVH format) + speech<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Temporal View Synthesis of Dynamic Scenes through 3D Object Motion Estimation with Multi-Plane Images</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.09463"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://nagabhushansn95.github.io/publications/2022/DeCOMPnet.html"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Communication Engineering, Indian Institute of Science<br>
‚Ä¢ Dataset: Indian Institute of Science Virtual Environment Exploration Dataset - Dynamic Scenes (IISc VEED-Dynamic), Samples: 800, Modality: RGB videos + depth + camera pose<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>MoCapDeform: Monocular 3D Human Motion Capture in Deformable Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.08439"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Malefikus/MoCapDeform"><img src="https://img.shields.io/github/stars/Malefikus/MoCapDeform.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Saarland University, SIC<br>
‚Ä¢ Dataset: MoCapDeform (MCD) dataset, Samples: 4 video sequences, Modality: RGB videos + ground-truth 3D human meshes<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>WatchPed: Pedestrian Crossing Intention Prediction Using Embedded Sensors of Smartwatch</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.07441"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://tinyurl.com/pedestrian2023"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: AÔ¨Åniti, Virginia, United States<br>
‚Ä¢ Dataset: WatchPed Pedestrian Crossing Intention Dataset, Samples: 255, Modality: RGB videos, smartwatch IMU (accelerometer, gyroscope), bounding boxes, pose information, vehicle speed<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.07363"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Institute for Robotics and Intelligent Machines, Georgia Institute of Technology<br>
‚Ä¢ Dataset: MoCapAct, Samples: 517800, Modality: Simulated humanoid rollouts (proprioceptive observations + actions)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Heart rate estimation in intense exercise videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.02509"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ynapolean/IBIS-CNN"><img src="https://img.shields.io/github/stars/ynapolean/IBIS-CNN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Delft<br>
‚Ä¢ Dataset: IntensePhysio, Samples: 15, Modality: RGB videos + heart rate<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Learning Modal-Invariant and Temporal-Memory for Video-based Visible-Infrared Person Re-Identification</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.02450"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VCM-project233/MITML"><img src="https://img.shields.io/github/stars/VCM-project233/MITML.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Harbin Institute of Technology, Shenzhen<br>
‚Ä¢ Dataset: HITSZ-VCM, Samples: 21863, Modality: RGB videos + IR videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>AutoLaparo: A New Dataset of Integrated Multi-tasks for Image-guided Surgical Automation in Laparoscopic Hysterectomy</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.02049"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/WANG-Ziyi/AutoLaparo"><img src="https://img.shields.io/github/stars/WANG-Ziyi/AutoLaparo.svg?style=social&label=Star"></a><br><a href="https://autolaparo.github.io"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong; T Stone Robotics Institute, The Chinese University of Hong Kong<br>
‚Ä¢ Dataset: AutoLaparo, Samples: 300, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Mates2Motion: Learning How Mechanical CAD Assemblies Work</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.01779"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Washington<br>
‚Ä¢ Dataset: Mates2Motion Processed Dataset, Samples: 7328, Modality: CAD B-Reps with mate annotations<br>
‚Ä¢ Dataset: Mates2Motion User-Annotated Validation Set, Samples: 100, Modality: CAD B-Reps with expert-annotated mates<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.01633"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://4dqv.mpi-inf.mpg.de/UnrealEgo/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, SIC, Keio University<br>
‚Ä¢ Dataset: UnrealEgo, Samples: 45520, Modality: Synthetic stereo fisheye images, depth maps, and 3D joint positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Interaction Mix and Match: Synthesizing Close Interaction using Conditional Hierarchical GAN with Multi-Hot Class Embedding</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.00774"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Aman-Goel1/IMM"><img src="https://img.shields.io/github/stars/Aman-Goel1/IMM.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: International Institute of Information Technology Hyderabad, India<br>
‚Ä¢ Dataset: New synthetic 2-character close interactions dataset, Samples: 300, Modality: 3D skeletal motions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Aug 2022</td>
  <td style="width:70%;"><strong>Learning Pseudo Front Depth for 2D Forward-Looking Sonar-based Multi-view Stereo</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2208.00233"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sollynoay/EPSSN"><img src="https://img.shields.io/github/stars/sollynoay/EPSSN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Precision Engineering, Graduate School of Engineering, The University of Tokyo, Japan<br>
‚Ä¢ Dataset: EPSSN Synthetic Datasets, Samples: 8000, Modality: Acoustic images + relative poses<br>
‚Ä¢ Dataset: EPSSN Real Dataset, Samples: 1493, Modality: Acoustic images + relative poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Efficient Video Deblurring Guided by Motion Magnitude</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.13374"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/sollynoay/MMP-RNN"><img src="https://img.shields.io/github/stars/sollynoay/MMP-RNN.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo<br>
‚Ä¢ Dataset: MMP dataset, Samples: 22,499 training samples, Modality: RGB videos + motion magnitude maps<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>CelebV-HQ: A Large-Scale Video Facial Attributes Dataset</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.12393"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/CelebV-HQ/CelebV-HQ"><img src="https://img.shields.io/github/stars/CelebV-HQ/CelebV-HQ.svg?style=social&label=Star"></a><br><a href="https://celebv-hq.github.io/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: SenseTime Research<br>
‚Ä¢ Dataset: CelebV-HQ, Samples: 35666, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Hybrid Classifiers for Spatio-temporal Real-time Abnormal Behaviors Detection, Tracking, and Recognition in Massive Hajj Crowds</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.11931"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Jamoum University College, Umm Al-Qura University<br>
‚Ä¢ Dataset: HAJJv2, Samples: 18, Modality: RGB videos + annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Combining Internal and External Constraints for Unrolling Shutter in Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.11725"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="www.wisdom.weizmann.ac.il/~vision/VideoRS"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Dept. of Computer Science and Applied Math, The Weizmann Institute of Science<br>
‚Ä¢ Dataset: In-the-wild-RS, Samples: 15, Modality: RS/GS video pairs<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>My View is the Best View: Procedure Learning from Egocentric Videos</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.10883"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Sid2697/EgoProceL"><img src="https://img.shields.io/github/stars/Sid2697/EgoProceL.svg?style=social&label=Star"></a><br><a href="https://sid2697.github.io/egoprocel/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Center for Visual Information Technology, IIIT, Hyderabad<br>
‚Ä¢ Dataset: EgoProceL, Samples: 349, Modality: egocentric videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>D2-TPred: Discontinuous Dependency for Trajectory Prediction under Traffic Lights</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.10398"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/VTP-TL/D2-TPred"><img src="https://img.shields.io/github/stars/VTP-TL/D2-TPred.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: School of Computer and Artificial Intelligence, Zhengzhou University, Zhengzhou, China.<br>
‚Ä¢ Dataset: VTP-TL, Samples: 5073, Modality: 2D vehicle trajectories and traffic light states from drone videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Region Aware Video Object Segmentation with Deep Motion Modeling</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.10258"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://ieee-dataport.org/9608"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science and Software Engineering, The University of Western Australia<br>
‚Ä¢ Dataset: OVOS, Samples: 607, Modality: RGB videos + segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Spotting Temporally Precise, Fine-Grained Events in Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.10213"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/SoccerNet/sn-spotting"><img src="https://img.shields.io/github/stars/SoccerNet/sn-spotting.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Stanford University<br>
‚Ä¢ Dataset: Tennis, Samples: 3345, Modality: RGB videos<br>
‚Ä¢ Dataset: Figure Skating, Samples: 371, Modality: RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.10123"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/zzh-tech/Animation-from-Blur"><img src="https://img.shields.io/github/stars/zzh-tech/Animation-from-Blur.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: The University of Tokyo<br>
‚Ä¢ Dataset: GenBlur, Samples: 192, Modality: Synthesized blurry RGB images + sharp RGB video sequences + motion guidance<br>
‚Ä¢ Dataset: B-Aist++, Samples: 105, Modality: Synthesized blurry RGB images + sharp RGB video sequences + motion guidance<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.10120"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/dmoltisanti/brace/"><img src="https://img.shields.io/github/stars/brace/.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Edinburgh<br>
‚Ä¢ Dataset: BRACE, Samples: 465, Modality: 2D keypoints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Human keypoint detection for close proximity human-robot interaction</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.07742"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://osf.io/qfkvt/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic<br>
‚Ä¢ Dataset: Human in Close Proximity (HiCP) dataset, Samples: 3232, Modality: RGB images + 2D keypoint annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>A Dual-Masked Auto-Encoder for Robust Motion Capture with Spatial-Temporal Skeletal Token Completion</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.07381"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/HKBU-VSComputing/2022_MM_DMAE-Mocap"><img src="https://img.shields.io/github/stars/HKBU-VSComputing/2022_MM_DMAE-Mocap.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China<br>
‚Ä¢ Dataset: BU-Mocap, Samples: 5, Modality: RGB videos, depth maps, IMU sensor data, point cloud data, 3D skeletal trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>SHREC 2022 Track on Online Detection of Heterogeneous Gestures</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.06706"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Verona, Department of Computer Science<br>
‚Ä¢ Dataset: SHREC 2022 Heterogeneous Gestures, Samples: 288, Modality: 3D hand joint positions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Is Appearance Free Action Recognition Possible?</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.06261"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://f-ilic.github.io/AppearanceFreeActionRecognition"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: TU Graz, Austria<br>
‚Ä¢ Dataset: Appearance Free Dataset (AFD101), Samples: 13320, Modality: Synthetic noise videos animated by optical flow<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Entry-Flipped Transformer for Inference and Prediction of Participant Behavior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.06235"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Singtel Cognitive and Artificial Intelligence Lab (SCALE@NTU), Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore<br>
‚Ä¢ Dataset: Tennis Doubles Dataset, Samples: 4905, Modality: RGB videos with bounding box, action, and trajectory annotations<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Learning to Estimate External Forces of Human Motion in Video</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.05845"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/MichiganCOG/ForcePose"><img src="https://img.shields.io/github/stars/MichiganCOG/ForcePose.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Michigan<br>
‚Ä¢ Dataset: ForcePose, Samples: 1344, Modality: RGB videos + MoCap markers + force plate data<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Occluded Human Body Capture with Self-Supervised Spatial-Temporal Motion Prior</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.05375"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/boycehbz/CHOMP"><img src="https://img.shields.io/github/stars/boycehbz/CHOMP.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Southeast University, China<br>
‚Ä¢ Dataset: OcMotion, Samples: 43 sequences, Modality: RGB videos + 3D motion annotations (SMPL), 2D poses, camera parameters<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Fine-grained Activities of People Worldwide</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.05182"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/visym/heyvi"><img src="https://img.shields.io/github/stars/visym/heyvi.svg?style=social&label=Star"></a><br><a href="https://visym.github.io/cap"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Visym Labs, Cambridge MA, USA<br>
‚Ä¢ Dataset: Consented Activities of People (CAP), Samples: 1450000, Modality: RGB videos with annotations (bounding box tracks, temporal localization)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>CausalAgents: A Robustness Benchmark for Motion Forecasting using Causal Relationships</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.03586"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/google-research/causal-agents"><img src="https://img.shields.io/github/stars/google-research/causal-agents.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Google Research, Brain Team<br>
‚Ä¢ Dataset: CausalAgents, Samples: None, Modality: Causal agent labels for agent trajectories from the Waymo Open Motion Dataset (WOMD)<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>MoRPI: Mobile Robot Pure Inertial Navigation</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.02982"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/ansfl/MoRPI"><img src="https://img.shields.io/github/stars/ansfl/MoRPI.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: the Hatter Department of Marine Technologies, University of Haifa, Israel<br>
‚Ä¢ Dataset: MoRPI, Samples: 143, Modality: IMU data (accelerometer, gyroscope) from a mobile robot<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Robustness Analysis of Video-Language Models Against Visual and Language Perturbations</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.02159"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Madeline-Schiappa/Robustness-of-Video-Language-Models"><img src="https://img.shields.io/github/stars/Madeline-Schiappa/Robustness-of-Video-Language-Models.svg?style=social&label=Star"></a><br><a href="https://bit.ly/3CNOly4"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Central Florida<br>
‚Ä¢ Dataset: MSRVTT-P, Samples: 90000, Modality: RGB videos + text captions<br>
‚Ä¢ Dataset: YouCook2-P, Samples: 301500, Modality: RGB videos + text captions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>VECtor: A Versatile Event-Centric Benchmark for Multi-Sensor SLAM</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.01404"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://star-datasets.github.io/vector/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Mobile Perception Lab of the School of Information Science and Technology, ShanghaiTech University<br>
‚Ä¢ Dataset: VECtor, Samples: 18, Modality: event stereo camera, regular stereo camera, RGB-D, LiDAR, IMU, Motion Capture (MoCap) ground truth<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jul 2022</td>
  <td style="width:70%;"><strong>Segmentation Guided Deep HDR Deghosting</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2207.01229"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="http://val.serc.iisc.ernet.in/HDR/shdr/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computational and Data Sciences, Indian Institute of Science, Bengaluru, KA, 560012, INDIA<br>
‚Ä¢ Dataset: MEDS (Multi-Exposure Dynamic motion Segmentation dataset), Samples: 3683, Modality: Multi-exposure RGB images with human-annotated motion segmentation masks<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>Spatial Transformer Network with Transfer Learning for Small-scale Fine-grained Skeleton-based Tai Chi Action Recognition</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.15002"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://cloud.hit605.org/s/taichi"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Control Science and Engineering Harbin Institute of Technology, Harbin, China<br>
‚Ä¢ Dataset: Tai Chi dataset, Samples: 200, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.13673"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/Tobias-Fischer/sparse-event-vpr"><img src="https://img.shields.io/github/stars/Tobias-Fischer/sparse-event-vpr.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: QUT Centre for Robotics, Queensland University of Technology<br>
‚Ä¢ Dataset: QCR-Event-VPR dataset, Samples: 16, Modality: Event camera stream, Robot kinematics/trajectories<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>IBISCape: A Simulated Benchmark for multi-modal SLAM Systems Evaluation in Large-scale Dynamic Environments</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.13455"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/AbanobSoliman/IBISCape.git"><img src="https://img.shields.io/github/stars/AbanobSoliman/IBISCape.git.svg?style=social&label=Star"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Universit√© Paris-Saclay, Univ Evry, IBISC Laboratory, 34 Rue du Pelvoux, Evry, 91020, Essonne, France.<br>
‚Ä¢ Dataset: IBISCape, Samples: 34, Modality: stereo-RGB, stereo-DVS, Depth, IMU, GPS, ground truth scene segmentation, vehicle ego-motion<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>Learn to Predict How Humans Manipulate Large-sized Objects from Interactive Motions</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.12612"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Computer Science, The University of Hong Kong, Hong Kong SAR<br>
‚Ä¢ Dataset: None, Samples: 18343, Modality: MoCap joints<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>EventNeRF: Neural Radiance Fields from a Single Colour Event Camera</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.11896"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/4DV-MPII/EventNeRF"><img src="https://img.shields.io/github/stars/4DV-MPII/EventNeRF.svg?style=social&label=Star"></a><br><a href="https://4dqv.mpi-inf.mpg.de/EventNeRF"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Informatics, SIC<br>
‚Ä¢ Dataset: EventNeRF Dataset, Samples: 17, Modality: Colour event streams + camera poses<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>A High Resolution Multi-exposure Stereoscopic Image & Video Database of Natural Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.11095"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://sites.google.com/view/multi-exposure-stereo-data/"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Department of Electrical Engineering, Indian Institute of Technology Madras, Tamil Nadu, 600036, India<br>
‚Ä¢ Dataset: A High Resolution Multi-exposure Stereoscopic Image & Video Database of Natural Scenes, Samples: 18, Modality: Stereoscopic RGB videos<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>SJ-HD^2R: Selective Joint High Dynamic Range and Denoising Imaging for Dynamic Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.09611"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Huawei Noah‚Äôs Ark Lab<br>
‚Ä¢ Dataset: SJ-HD2R RAW-HDR dataset, Samples: 207, Modality: RAW image sequences<br>
‚Ä¢ Dataset: SJ-HD2R RGB-HDR dataset, Samples: 207, Modality: RGB image sequences<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>Capturing and Inferring Dense Full-Body Human-Scene Contact</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.09553"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/rich-dataset/RICH"><img src="https://img.shields.io/github/stars/rich-dataset/RICH.svg?style=social&label=Star"></a><br><a href="https://rich.is.tue.mpg.de"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Max Planck Institute for Intelligent Systems, T ¬®ubingen, Germany<br>
‚Ä¢ Dataset: RICH, Samples: 142, Modality: Multiview 4K RGB videos, 3D body meshes (SMPL-X), 3D laser scene scans, human-scene contact labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>Recurrent Transformer Variational Autoencoders for Multi-Action Motion Synthesis</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.06741"><img src="https://img.shields.io/badge/Paper-red"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: University of Bonn<br>
‚Ä¢ Dataset: PROX (augmented), Samples: 100000 frames, Modality: SMPL fittings + GT action labels<br>
‚Ä¢ Dataset: Charades (augmented), Samples: 1918 sequences, Modality: SMPL fittings + action labels<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>SCAMPS: Synthetics for Camera Measurement of Physiological Signals</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.04197"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://github.com/danmcduff/scampsdataset"><img src="https://img.shields.io/github/stars/danmcduff/scampsdataset.svg?style=social&label=Star"></a><br><a href="https://github.com/danmcduff/scampsdataset"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: Microsoft<br>
‚Ä¢ Dataset: SCAMPS, Samples: 2800, Modality: RGB videos + head pose + facial actions<br>
</td></tr>

<tr>
  <td rowspan="2" style="width:15%;">Jun 2022</td>
  <td style="width:70%;"><strong>Generating Long Videos of Dynamic Scenes</strong></td>
  <td style="width:15%;"><img src="https://img.shields.io/badge/Dataset-blue"></td>
  <td style="width:15%;"><a href="http://arxiv.org/pdf/2206.03429"><img src="https://img.shields.io/badge/Paper-red"></a><br><a href="https://www.timothybrooks.com/tech/long-videos"><img src="https://img.shields.io/badge/Website-9cf"></a><br></td></tr>
<tr><td colspan="4">‚Ä¢ Affiliation: NVIDIA, UC Berkeley<br>
‚Ä¢ Dataset: mountain biking dataset, Samples: 1202, Modality: RGB videos<br>
‚Ä¢ Dataset: horseback riding dataset, Samples: 66, Modality: RGB videos<br>
</td></tr>
</table>

## üí™ How to Contribute

If you have a paper or are aware of relevant research that should be incorporated, please contribute via pull requests, issues, email, or other suitable methods.
